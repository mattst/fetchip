#!/bin/bash

#
# Name:             getip
# Description:      Script to retrieve an external IPv4 address from IP providing web sites.
# Requirements:     UNIX/Linux, Bash, either curl or wget.
# Usage:            getip [-g | -c | -u url] [-n num_sources] [-t timeout_secs] [-b] [-v] [-h]
# Exit Status:      0 on success, and non-zero on failure.
# Author:           mattst@i-dig.info
# Version:
# Homepage:         https://github.com/mattst/getip
# License:          GNU General Public License v3 - http://www.gnu.org/copyleft/gpl.html
#
#
# The complete list of all utilities and Bash shell commands used by the getip script:
#
# awk, bc, cat, curl, date, disown, echo, exit, getopts, grep, hash, kill, man, mktemp, printf, ps,
# $RANDOM, read, rm, sed, shift, sort, tr, uniq, unset, wc, wget. [Either wget or curl required.]
#
# With the exception of wget and curl all of the above utilities and commands are expected to exist
# on all modern UNIX like operating systems. If neither wget nor curl are installed the script will
# advise the user to install one of them.
#
#
# Description:
#
# getip is a script used to retrieve a wide area network (WAN) IPv4 address from behind a router
# using web pages which provide the IP address in plain text or HTML. Note: IPv4 only, not IPv6.
#
# getip echoes the IP address so that it is displayed in the shell or so that it can be stored in a
# variable if getip is called from within another script.
#
# Command line options (all optional, in typical use none are necessary.):
#
# -g           GET; get the IP from multiple sources and verify they are all the same (default).
# -c           CHECK; display the IP address returned by all the web sources and show stats.
# -u url       TEST; test an URL for possible inclusion in the source urls and show info.
# -n number    Set the NUMBER of sources to use for verification. Range >=2 and <=5 (default 2).
# -t seconds   Set the TIMEOUT for each source. Real numbers are permitted (e.g. '1.5', '0.75').
# -b           BARE output; display only the IP address and not the "IPv4 address:" prefix.
# -v           VERBOSE; output information while the script is running.
# -h, --help   HELP; output program usage and help.
# --version    Display the VERSION of the script.
#
# GET is the default operation so -g is not needed on the command line, is uses the default number
# of sources which is 2 unless '-n number' is used to set the number of sources.
#
#
# The getip script uses a file containing a long list of web pages, each of which provide the IP
# address in either plain text or HTML, this is known as the sources list. Each web page url is on a
# separate line in the file. The sources list is stored online on the script's github page. Each
# time the script is run it downloads the sources list - by doing this the script always uses the
# most up-to-date version of the list, rather than a local copy which would soon contain sites that
# no longer exist, have become slow to respond, or which are otherwise no longer reliable.
#
# From time to time a web page used as a source will stop providing IP addresses, either temporarily
# or permanently, but an example IP address might remain on the page, or an IP-like software version
# number might be present on the page (embedded within the HTML) and which could be mistaken for a
# real IP address (although the script tries hard to spot and ignore these). In such circumstances
# an IP address fetching script could provide a user with an incorrect IP address. The getip script
# makes sure this does not happen.
#
# The getip script retrieves the IP address from multiple sources and checks that they have supplied
# the same IP address. The default number of sources used is 2 but this can be set on the command
# line using the -n option, the range of values is >=2 and <=5. The default is considered to be
# sufficient due to the unlikelihood of 2 sources both providing the same incorrect IP address. Very
# cautious users might want to use 3 sources, while 5 would indicate paranoia. Clearly the more
# sources used the longer it takes [guesstimate about 1 second per source on average].
#
#
# Description of Options:
# FAST                   - Command line switch: -f
#
# In fast mode getip will retrieve the IP address from just one source with no verification. It is
# generally faster, though not guaranteed to be. It is suggested that fast mode only be used when a
# user already knows their IP address and is seeking confirmation of it, for instance if checking
# whether a connection to a VPN has finished or succeeded. Fast mode should never be used when
# calling getip from within another script.
#
#
# CHECK                  - Command line switch: -c
#
# In check mode getip will display a table in the shell of all the web sources, the IP address which
# was retrieved from the source, or a message of failure, and the time taken to retrieve the IP
# address. In addition it displays some statistics at the end. It can be used by users to check that
# all the sources are functioning correctly. getip will continue to function perfectly adequately
# with even 25% of the sources down. If more than 15% are down you may wish to download the latest
# version of the script or manually delete sources which aren't working from the list below.
#
#
# TESTURL                - Command line switch: -u url
#
# In test url mode getip will test the url to see if it is suitable for adding to the source list.
# getip will display a small table showing the url's status; whether the Url successfully returned a
# single valid IP address, or failed by supplying more than one valid IP address, no IP address at
# all, or if the timeout was exceeded. Only experienced users should use this feature to add an url
# to the source list. If you do use this to find a new url that is suitable for inclusion please
# contact the developer with the url.
#
#
# The other command line options:
#
# NUMBER OF SOURCES      - Command line switch: -n num_sources [Default 2, range: >=2 and <=5.]
#
# Sets the number of sources to use in the multi operation mode.
#
#
# TIMEOUT, FOR EACH SOURCE   - Command line switch: -t seconds
#                            - Range >=0.2 and <=30. [Real numbers are permitted.]
#                            - Default timeout in multi mode:    1.0
#                            - Default timeout in fast mode:     0.75
#                            - Default timeout in check mode:    10.0
#                            - Default timeout in test url mode: 10.0
#
# Sets the timeout in seconds for each source, not an overall timeout for the script. When a source
# gives no response or is slow to respond, getip simply moves on to the next source in the sources
# list. Each of the operation modes has its own default timeout (shown above). The allowed range is
# large, >=0.2 and <=30, as large variations in speed of web access and the user's physical location
# both mean a flexible timeout is sensible, however values less than 0.75 or greater than 3 are not
# usually advisable in multi or fast mode. In check and test url modes a high timeout facilitates
# checking whether sources are functioning at all or not.
#
# There is no 'global' timeout feature for getip, however the timeout multiplied by the number of
# sources means there is a de facto 'global' timeout.
#
#
# BARE                   - Command line switch: -b
#
# Turn bare output on - the script will display only the IP address and not the "IPv4 address:"
# prefix.
#
#
# VERBOSE                - Command line switch: -v
#
# Turns verbose on - the script will output diagnostic information while it runs.
#
#
# HELP                   - Command line switch: -h or --help
#
# Outputs program usage and help then exits.
#
#
# Note on Bash: If 'local' is on the same line as a command then $? will contain local's exit status
# instead of the command's, effectively $? will always be zero. Define the local first, e.g.
#
# local web_sources_temp_file
# web_sources_temp_file=$(mktemp -q -t "getip.tmp.XXXXXX")
#
#
# http://stackoverflow.com/questions/4082966/what-are-the-alternatives-now-that-the-google-web-search-api-has-been-deprecated

##
## START OF SCRIPT.
##


#
# The Principal Variables.
#


# Holds the script version number.
version="0.9.1"


# Constants to hold the 3 operation modes.
GET_IP_MODE="1"
TEST_URL_MODE="2"
CHECK_MODE="3"

# Holds the operation mode, default is $GET_IP_MODE.
operation_mode="$GET_IP_MODE"

# Holds the default url of the list of web sources.
web_sources_list_default="https://crius.feralhosting.com/gencon/SourceUrlsList"

# Holds the url or local file if the user set the location of the web sources list (the -s option).
web_sources_list_user_set="FALSE"

# Array which holds the urls from the web sources list (once it's been read).
web_sources=()

# Holds the number of urls in $web_sources.
web_sources_len="0"

# Holds the minimum number of web sources that are required. Note: Consider removing this.
web_sources_min="15"

# Holds the default number of web sources to use for verification. Range: >=2 and <=5.
web_sources_verify_num="2"

# Holds the timeout value in seconds (of each web source, no overall script timeout).
timeout="0"

# Holds the GETIP mode default timeout. [Set it low so that if a source is offline or slow to
# respond the script can move on to the next source. Recommended: 0.75 to 3.0.]
timeout_get_ip_mode_default="1.5"

# Holds the CHECK mode default timeout. [Set it high to give the url a chance to succeed.]
timeout_check_mode_default="10"

# Holds the TESTURL mode default timeout. [Set it high to give the url a chance to succeed.]
timeout_test_url_mode_default="10"

# Holds the user set timeout (the -t option).
timeout_user_set="FALSE"

# Holds the url of the web source to test in test url mode.
test_url_address=""

# Constants to hold the 2 download program possibilities, either wget or curl can be used.
DOWNLOAD_PROGRAM_WGET="1"
DOWNLOAD_PROGRAM_CURL="2"

# Holds the download program.
download_program=""

# Holds the default download program (in tests wget seems to be a little quicker than curl).
download_program_default="$DOWNLOAD_PROGRAM_WGET"

# Holds the user set download program (the -d option).
download_program_user_set="FALSE"

# Holds if the user has turned on verbose reporting (the -v option).
verbose="FALSE"

# Holds if the user has turned on bare output (the -b option).
bare_output="FALSE"

# Holds the user agent to use for web page downloads. The script does not try to hide that it
# is a script. The user agent header is modelled on the one used by Google's 'googlebot':
# Googlebot 2.1: "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
user_agent="Mozilla/5.0 (compatible; getipscript/$version; +https://github.com/mattst/getip)"

# Variables used to hold statistical information in the check operation mode.
stats_ip_address_list=""
stats_ip_valid_total="0"
stats_ip_invalid_total="0"
stats_ip_retrieval_times_list=""

# Variables used only for neatly displaying information.
web_sources_list_location_display_text=""
download_program_display_text=""

# Constants to hold the values returned by the Get_IP_Address_From_Web_Source() function.
SINGLE_VALID_IP="0"
NO_VALID_IP="1"
MULTIPLE_VALID_IP="2"
SERVER_TIMED_OUT="3"
TEMP_FILE_CREATION_FAILED="4"

# Constants to hold the values returned by the Verify_IP_List() function.
IP_ADDRESS_LIST_VERIFICATION_SUCCESS="0"
IP_ADDRESS_LIST_VERIFICATION_FAILURE="1"

# Constant to hold the successful exit value.
EXIT_SUCCESS="0"

# Constants to hold the various error exit values, they are also used for error messages.
EXIT_ERR_ARG_INVALID="100"
EXIT_ERR_OPTION_INVALID="101"
EXIT_ERR_ARG_MISSING="102"
EXIT_ERR_MORE_THAN_ONE_MODE_SET="103"
EXIT_ERR_VERBOSE_AND_BARE_SET="104"
EXIT_ERR_TEMP_FILE_CREATION_FAILED="105"
EXIT_ERR_READ_WEB_SOURCE_LIST_FAILED="106"
EXIT_ERR_NOT_ENOUGH_SOURCE_URLS="107"
EXIT_ERR_FAILED_TO_GET_IP_ADDRESS="108"
EXIT_ERR_WGET_NOT_INSTALLED="109"
EXIT_ERR_CURL_NOT_INSTALLED="110"
EXIT_ERR_WGET_AND_CURL_NOT_INSTALLED="111"
EXIT_ERR_TEST_URL_NO_IP_ADDRESS="112"
EXIT_ERR_TEST_URL_MULTIPLE_IP_ADDRESS="113"
EXIT_ERR_TEST_URL_TIMED_OUT="114"


#
# The Functions.
#


# Function to display help.
Display_Usage_Message()
{
    # Create a temp file to store the man page here document.
    local getip_man_page_temp_file
    getip_man_page_temp_file=$(mktemp -q -t "getip.tmp.XXXXXX")
    local make_temp_file_ret_val=$?

    if [ "$make_temp_file_ret_val" -ne "0" ]; then
        Exit_With_Error_Message "$EXIT_ERR_TEMP_FILE_CREATION_FAILED"
    fi

# Output a here document of the getip man page to $getip_man_page_temp_file.
# Here documents don't work well with indentation, keep to the hard left.
cat >>$getip_man_page_temp_file <<EOF

.TH GETIP 1 "29 Sep 2015" "1.0" "GETIP VERSION 1.0"

.SH NAME

getip \- retrieves the WAN IP address

.SH SYNOPSIS

getip [-m | -f | -c | -u url] [-n num_sources] [-t timeout_secs] [-v] [-h]

.SH REQUIREMENTS

The BASH shell and either wget or curl must be installed. Most modern UNIX like systems come with BASH and wget pre-installed. wget and curl are both programs which download pages from the web. getip works equally well with them both. wget is used by default if both are installed on the system.

.SH DESCRIPTION

getip is a BASH script used to retrieve the WAN IP address from behind a router using web sites (sources) which provide the IP address in plain text or HTML.

From time to time a web page used as a source will stop providing IP addresses, either temporarily or permanently, but an example IP address might remain on the page, or an IP-like software version number might be present, embedded in the HTML, and which could be mistaken for a real IP address (although the script tries hard to spot version numbers so they can be ignored). In such cases an IP address fetching script could provide a user with an incorrect IP address. getip uses multi mode by default to prevent this from happening.

In multi mode the IP address is retrieved from multiple sources (default: 2) and compared, so that the correct IP address can be provided. Users are advised to use multi mode in preference to fast mode.

The getip script retrieves IPv4 addresses only and not IPv6 addresses.

This script currently contains over 100 web sources.

The list of web sources is randomized every time the script is run. This is so that undue pressure is not placed on servers near the top of the list. The randomizing process is done very quickly and does not have a significant impact on the running time of the script.

.SH OPTIONS
.BR
In typical usage no options are necessary.
.BR

.TP
\f_b\-m\f_r
Multi mode (default); fetch the IP address from multiple sources and check that they are all the same. In multi mode the default number of sources to use is 2. This is the default mode so \f_b\-m\f_r is optional. See: \f_b\-n\f_r \f_bnum_sources\f_r

.TP
\f_b\-f\f_r
Fast mode; fetch the IP address from just one source in order to retrieve it more quickly than in multi mode. Fast mode is not guaranteed to be faster than multi mode but on average it will be slightly faster. It is advisable that users only use fast mode to confirm their IP address when it is already known, e.g. checking a VPN connection. In some circumstances an incorrect IP address can be returned in fast mode, see \f_bDescription\f_r section above.

.TP
\f_b\-c\f_r
Check mode; displays a table containing the url of each source, the IP address that it returned (or a failure message), the time taken to retrieve the IP address, and various statistics. Used to test the sources list.

.TP
\f_b\-u\f_r \f_burl\f_r
Test Url mode; tests an url for inclusion in the sources list. It will display whether the url returned a single IP address (which is required for inclusion in the sources list), multiple IP addresses, or no IP addresses at all. To aid debugging it will list all IP addresses returned.

.TP
\f_b\-n\f_r \f_bnum_sources\f_r
Sets the Number of sources to use in multi mode. The allowed range of values is >=2 and <=5. The default value is 2.

.TP
\f_b\-t\f_r \f_bnum_seconds\f_r
Sets the Timeout for each source. This is not an overall timeout for the script. When a source gives no response or is slow to respond, getip moves on to the next source and this is the timeout in seconds for each one. Note: Real numbers are permitted, e.g. 0.75, 1.5, .85, etc. The allowed range of values is >=0.2 and <=30 (but these extremes are overkill).

The default timeouts are dependant on mode, in seconds they are; multi mode 1, fast mode 0.75, check mode 10, test url mode 10.

.TP
\f_b\-b\f_r
Bare output; display only the IP address and not the "IPv4 address:" prefix.

.TP
\f_b\-v\f_r
Verbose; display detailed information while the script runs.

.TP
\f_b\-h\f_r, \f_b\-\-help\f_r
Help; display usage and help.

.TP
\f_b\-\-version\f_r
Display version number.

.SH EXIT STATUS
getip returns 0 on success, and non-zero on failure.

.SH BUGS
No known bugs.

.SH HOMEPAGE
https://github.com/mattst/getip
.LP
Feel free to submit new IP address providing web pages to be added to the sources list.

.SH AUTHOR
<mattst@i-dig.info>
EOF

    # Display the man page.
    man "$getip_man_page_temp_file"

    # Delete the temp file.
    if [ -f "$getip_man_page_temp_file" ]; then rm "$getip_man_page_temp_file"; fi
}


# Function to send the various error messages to STDERR.
Exit_With_Error_Message()
{
    # Assign $err_num the value of the 1st function arg (NOT script arg).
    local err_num="$1"

    # Assign $additional_info the value of the 2nd function arg (NOT script arg).
    # Some errors display a configurable piece of information, e.g. an option.
    local additional_info="$2"

    # If bare output is set, just exit with no error message. Not sure about this?!
    if [ "$bare_output" = "TRUE" ]; then
        exit "$err_num"
    fi

    # Output the appropriate error message to STDERR.

    echo "" >&2

    if [ "$err_num" = "$EXIT_ERR_ARG_INVALID" ]; then
        echo "The argument given to the $additional_info option is invalid." >&2

    elif [ "$err_num" = "$EXIT_ERR_OPTION_INVALID" ]; then
        echo "An invalid option has been entered." >&2

    elif [ "$err_num" = "$EXIT_ERR_ARG_MISSING" ]; then
        echo "The argument is missing for the option: -$additional_info" >&2

    elif [ "$err_num" = "$EXIT_ERR_MORE_THAN_ONE_MODE_SET" ]; then
        echo "Only one operation mode can be used at a time." >&2

    elif [ "$err_num" = "$EXIT_ERR_VERBOSE_AND_BARE_SET" ]; then
        echo "Verbose and bare output can not be used at the same time." >&2

    elif [ "$err_num" = "$EXIT_ERR_TEMP_FILE_CREATION_FAILED" ]; then
        echo "Unable to create a temp file using 'mktemp'." >&2

    elif [ "$err_num" = "$EXIT_ERR_READ_WEB_SOURCE_LIST_FAILED" ]; then
        echo "Unable to read the web sources file located at:" >&2
        echo "$web_sources_list_location_display_text" >&2

    elif [ "$err_num" = "$EXIT_ERR_NOT_ENOUGH_SOURCE_URLS" ]; then
        echo "Not enough web sources urls, the minimum required is $web_sources_min." >&2

    elif [ "$err_num" = "$EXIT_ERR_FAILED_TO_GET_IP_ADDRESS" ]; then
        echo "The script failed to get the IP address." >&2

    elif [ "$err_num" = "$EXIT_ERR_WGET_NOT_INSTALLED" ]; then
        echo "The program wget is not installed on your system." >&2

    elif [ "$err_num" = "$EXIT_ERR_CURL_NOT_INSTALLED" ]; then
        echo "The program curl is not installed on your system." >&2

    elif [ "$err_num" = "$EXIT_ERR_WGET_AND_CURL_NOT_INSTALLED" ]; then
        echo "Neither wget nor curl are installed on your system." >&2
        echo "" >&2
        echo "The getip script requires either 'wget' or 'curl' to access the web, please" >&2
        echo "install one of them in order to use this script. wget and curl are both" >&2
        echo "programs that can retrieve web pages, and they are widely available for all" >&2
        echo "UNIX/Linux based systems. The getip script works equally well with either," >&2
        echo "but in tests on Debian and Linux Mint wget was consistently just a little" >&2
        echo "faster. If both wget and curl are installed getip will use wget by default." >&2
    fi

    echo "" >&2
    echo "Use \"getip -h\" for usage and help or \"man getip\" (if installed)." >&2

    exit "$err_num"
}


# Function to store the urls from the appropriate web sources list in $web_sources.
Read_Source_Url_List()
{
    # Constants to specify whether the web sources list is an url or local file.
    local WEB_SOURCES_TYPE_URL="1"
    local WEB_SOURCES_TYPE_FILE="2"

    # Variables to hold the type (url or local file) and address/location.
    local web_sources_type
    local web_sources_list_url
    local web_sources_list_file

    # Use the default url if the user has not set the web sources list on the command line.
    if [ "$web_sources_list_user_set" = "FALSE" ]; then

        web_sources_type="$WEB_SOURCES_TYPE_URL"
        web_sources_list_url="$web_sources_list_default"
        web_sources_list_location_display_text="$web_sources_list_default"

    # If the user has set the web sources list on the command line then check if
    # $web_sources_list_user_set is a local file, if it's not then it's an url.
    else
        if [ -f "$web_sources_list_user_set" ]; then
            web_sources_type="$WEB_SOURCES_TYPE_FILE"
            web_sources_list_file="$web_sources_list_user_set"
            web_sources_list_location_display_text="$web_sources_list_user_set"
        else
            web_sources_type="$WEB_SOURCES_TYPE_URL"
            web_sources_list_url="$web_sources_list_user_set"
            web_sources_list_location_display_text="$web_sources_list_user_set"
        fi
    fi

    # If the web sources list is an url, download it into a temp file.
    if [ "$web_sources_type" = "$WEB_SOURCES_TYPE_URL" ]; then

        # Create a temp file to store the web sources list in.
        local web_sources_temp_file
        web_sources_temp_file=$(mktemp -q -t "getip.tmp.XXXXXX")
        local make_temp_file_ret_val=$?

        if [ "$make_temp_file_ret_val" -ne "0" ]; then
            Exit_With_Error_Message "$EXIT_ERR_TEMP_FILE_CREATION_FAILED"
        fi

        # Use wget to download the web sources list to the temp file.
        if [ "$download_program" = "$DOWNLOAD_PROGRAM_WGET" ]; then

            wget --quiet --tries=1 --user-agent="$user_agent" --output-document=- \
                                        "$web_sources_list_url" > "$web_sources_temp_file"
            local download_web_sources_list_ret_val=$?

            if [ "$download_web_sources_list_ret_val" -ne "0" ]; then
                Exit_With_Error_Message "$EXIT_ERR_READ_WEB_SOURCE_LIST_FAILED"
            fi

        # Use curl to download the web sources list to the temp file.
        elif [ "$download_program" = "$DOWNLOAD_PROGRAM_CURL" ]; then

            curl --silent --fail --user-agent "$user_agent" "$web_sources_list_url" \
                                                                > "$web_sources_temp_file"
            local download_web_sources_list_ret_val=$?

            if [ "$download_web_sources_list_ret_val" -ne "0" ]; then
                Exit_With_Error_Message "$EXIT_ERR_READ_WEB_SOURCE_LIST_FAILED"
            fi
        fi

        # Set the filename for the array reading to the temp filename.
        web_sources_list_file="$web_sources_temp_file"
    fi

    # Read the urls from $web_sources_list_file into the $web_sources array.

    # The $web_sources array must be INDEXED IN NUMERICAL SEQUENCE FROM 0.
    local index="0"

    while read -r line ; do

        # Store only lines which are urls by checking if the first 4 chars are 'http'.
        local line_first_4_chars=${line:0:4}
        local line_first_4_chars_lc=$(echo "$line_first_4_chars" | tr '[:upper:]' '[:lower:]')

        if [ "$line_first_4_chars_lc" = "http" ]; then
            web_sources[$index]="$line"
            let 'index = index + 1'
        fi

    done < "$web_sources_list_file"

    # Set the length of the web_sources array.
    web_sources_len="$index"

    # Delete the temp file.
    if [ "$web_sources_type" = "$WEB_SOURCES_TYPE_URL" ]; then
        if [ -f "$web_sources_temp_file" ]; then rm "$web_sources_temp_file"; fi
    fi
}


# Function that randomizes the $web_sources array. Since this script is being publicly released it
# is sensible to randomize the sources so as not to place undue pressure on the servers at the top
# of the list. This is an implementation of the Knuth shuffle algorithm (aka Fisher-Yates shuffle),
# which is a very efficient randomizer, needing just one pass through an array. IMPORTANT: THE
# $web_sources ARRAY MUST BE INDEXED IN NUMERICAL SEQUENCE FROM 0 FOR THIS FUNCTION.
Randomize_Url_List()
{
    # Bash $RANDOM provides a random number in the range 0..32767 so 32768 possible values.
    local rand_max="32768"

    # Start at the number of items in the array (i.e. final element's index + 1).
    local index="$web_sources_len"

    # Loop down through the array randomly swapping the current element with another randomly
    # chosen element (possibly itself). The use of >1 needs a moment to understand... if instead
    # it was >0 then the final iteration would always swap index 0 with itself.

    while [ "$index" -gt "1" ]; do

        # Modulo bias avoidance as per Knuth shuffle algorithm description. Calculate the highest
        # usable random number so that the modulo by index calculation will not result in any modulo
        # bias. Note: Not really necessary but the function only takes ~0.003 seconds with 100 urls
        # (on my PC) so why not do it right!? That's just ~0.15% of the approx. average run time.

        # Integer arithmetic, the division will be rounded down.
        local rand_num_highest_usable
        let "rand_num_highest_usable = (($rand_max / $index) * $index) - 1"

        # Get a random number within the modulo bias avoidance range.
        local rand_num="$RANDOM"

        while [ "$rand_num" -gt "$rand_num_highest_usable" ]; do
            rand_num="$RANDOM"
        done

        # Get a random index in the range 0..index-1.
        local rand_index
        let "rand_index = rand_num % index"

        # Decrement index to reference the current 'last' element in the array. For the first loop
        # iteration index will be $web_sources_len - 1 (the final element of the array) and for the
        # last loop iteration index will be 1. By doing so the current index will be swapped with
        # a randomly chosen index ($rand_index) in the range 0..index, so elements will sometimes
        # be 'swapped' with themselves (if elements could not stay put it wouldn't be random).

        let "index = index - 1"
        local swap_val=${web_sources[$index]}
        web_sources[$index]=${web_sources[$rand_index]}
        web_sources[$rand_index]=$swap_val

    done
}


# Function to extract valid IP addresses from the file given in the 1st function arg.
Extract_IP_Addresses_From_File()
{
    # Assign $web_page_source_file the value of the 1st function arg (NOT script arg).
    local web_page_source_file="$1"

    # The following Awk program will search for and print all valid IP addresses in the file
    # which is given to it as input.

    # The regex ip_like_sequence deliberately matches IP address like sequences which are too long
    # to be a valid IP so that the regex does not examine something like '11.22.33.44.55' and
    # match just the '11.22.33.44' section of it, and in so doing cause something that is
    # definitely not an IP address to get mistaken for a valid one. IP address like sequences
    # are checked in the END section to make sure they have exactly 4 segments, separated by
    # dots (".") and that each of the 4 numbers are within the 0..255 range of a valid IP.

    # The regex digit_sequence_too_long_not_ip will match sequences of numbers which are longer than
    # 3 digits. They are all removed as they can not possibly be part of an IP address.

    # The regex versioning_not_ip attempts to spot version numbers which happen to be in the same
    # format as an IP address (which they are often are). e.g. Version 1.2.0.12
    # Any ip_like_sequence which follows words like "Version", "Ver", or "V", will be removed.

    # Version numbers are also often embedded in an url like "1.2.3.4" in these examples:
    # e.g. <script language="Java_script" src="http://web.com/libs/1.2.3.4/file.js"></script>
    # e.g. <script language="Java_script" src="http://web.com/libs/v.1.2.3.4/file.js"></script>
    # The regexes begins_with_fwd_slash_not_ip and ends_with_fwd_slash_not_ip match an
    # ip_like_sequence which begins or ends with a forward slash so that those sequences can be
    # removed.

    local awk_extract_ip_addresses='
    BEGIN {
        # Regex to match an IP address like sequence (even if too long to be an IP). This is
        # deliberately a loose match, the END section will check for IP address validity.
        ip_like_sequence = "[0-9]+[.][0-9]+[.][0-9]+[.][0-9]+[0-9.]*";

        # Regex to match a number sequence longer than 3 digits.
        digit_sequence_too_long_not_ip = "[0-9][0-9][0-9][0-9]+";

        # Regex to match an IP address like sequence which is a version number.
        # Equivalent to "(version|ver|v)[ .:]*" in conjunction with: line = tolower($0);
        versioning_not_ip = "[Vv]([Ee][Rr]([Ss][Ii][Oo][Nn])?)?[ .:]*" ip_like_sequence;

        # Regexes to match IP address like sequences next to forward slashes, to avoid version
        # numbers in urls: e.g. http://web.com/libs/1.2.3.4/file.js
        begins_with_fwd_slash_not_ip = "[/]" ip_like_sequence;
        ends_with_fwd_slash_not_ip = ip_like_sequence "[/]";
    }
    {
        # Set line to the current line (more efficient than using $0 below).
        line = $0;

        # Replace sequences on line which will interfere with extracting genuine IPs. Use a
        # replacement char and not the empty string to avoid accidentally creating a valid IP
        # address from digits on either side of the removed sections. Use "/" as the replacement
        # char for the 2 "Fwd_slash" regexes so that multiple number dot slash sequences all get
        # removed, as using "x" could result in inadvertently leaving such a sequence in place.
        # e.g. "/lib1.2.3.4/5.6.7.8/9.10.11.12/file.js" would leave "/lib1.2.3.4xx/file.js".

        gsub(digit_sequence_too_long_not_ip, "x", line);
        gsub(versioning_not_ip, "x", line);
        gsub(begins_with_fwd_slash_not_ip, "/", line);
        gsub(ends_with_fwd_slash_not_ip, "/", line);

        # Loop through the current line matching IP address like sequences and storing them in
        # the INDEX of the array ip_unique_matches. By using ip_match as the array index duplicates
        # are avoided and the values can be easily retrieved by the for loop in the END section.
        # match() automatically sets the built in variables RSTART and RLENGTH.

        while (match(line, ip_like_sequence))
        {
            ip_match = substr(line, RSTART, RLENGTH);
            ip_unique_matches[ip_match];
            line = substr(line, RSTART + RLENGTH + 1);
        }
    }
    END {
        # Define some IP address related constants.
        ip_range_min = 0;
        ip_range_max = 255;
        ip_num_segments = 4;
        ip_delimiter = ".";

        # Loop through the ip_unique_matches array and print any valid IP addresses. The awk "for
        # each" type of loop is different from the norm. It provides the indexes of the array
        # and NOT the values of the array elements which is more usual in this type of loop.

        for (ip_match in ip_unique_matches)
        {
            num_segments = split(ip_match, ip_segments, ip_delimiter);
            if (num_segments == ip_num_segments &&
                ip_segments[1] >= ip_range_min && ip_segments[1] <= ip_range_max &&
                ip_segments[2] >= ip_range_min && ip_segments[2] <= ip_range_max &&
                ip_segments[3] >= ip_range_min && ip_segments[3] <= ip_range_max &&
                ip_segments[4] >= ip_range_min && ip_segments[4] <= ip_range_max)
            {
                print ip_match;
            }
        }
    }'

    # Extract valid IP addresses, each will be separated by a new line.
    local valid_ip_addresses=$(awk "$awk_extract_ip_addresses" < "$web_page_source_file")

    # Echo for capture in a variable at the point of function invocation.
    echo "$valid_ip_addresses"
}


# Function that retrieves the web source's content and returns all unique IP addresses in it. The
# function provides an accurate timeout facility; neither curl nor wget reliably honour any timeout
# value which has been set nor do they allow timeouts in fractions of a second. Since getip uses so
# many web sources a low timeout is used, if one source fails to return quickly then the next source
# is tried. With this methodology a reliable and accurate timeout is required, this function
# provides it.
Get_IP_Address_From_Web_Source()
{
    # Assign $web_source_url the value of the 1st function arg (NOT script arg).
    local web_source_url="$1"

    # Create a temp file to store the page source of $web_source_url.
    local web_source_temp_file
    web_source_temp_file=$(mktemp -q -t "getip.tmp.XXXXXX")
    local make_temp_file_ret_val=$?

    # Temp file creation errors must be handled differently here. The function is called using the
    # $() construct which means a subshell will be created for it and any exit call inside the
    # function will exit only the subshell and not the script. To get around this the return value
    # of Get_IP_Address_From_Web_Source() is checked to handle exiting the script if mktemp failed.
    if [ "$make_temp_file_ret_val" -ne "0" ]; then
        return "$TEMP_FILE_CREATION_FAILED"
    fi

    # Download the web page's source and store its contents in the temp file. The process is forked
    # (the trailing '&') to facilitate the timeout procedure.

    # Store the start time for the timeout. Note: %s seconds, %N nanoseconds.
    local timeout_start=$(date +%s.%N)

    # Download the web page's source placing it in the temp file and store the process's id.

    if [ "$download_program" = "$DOWNLOAD_PROGRAM_WGET" ]; then

        wget --quiet --tries=1 --user-agent="$user_agent" --output-document=- \
                                    "$web_source_url" > "$web_source_temp_file" &
        local download_process_id=$!

    elif [ "$download_program" = "$DOWNLOAD_PROGRAM_CURL" ]; then

        curl --silent --user-agent "$user_agent" "$web_source_url" > "$web_source_temp_file" &
        local download_process_id=$!
    fi

    # Even if kill has both stdout and stderr redirected to /dev/null the death of the process is
    # still displayed. Using disown to remove the job from the shell's active jobs table stops this
    # from happening so that there are no unsightly terminal messages.
    disown "$download_process_id"

    # Handle the timeout facility for the web page source download.

    local is_download_done="FALSE"
    local is_download_timed_out="FALSE"

    while [ "$is_download_done" = "FALSE" -a "$is_download_timed_out" = "FALSE" ]; do

        # Store whether the wget/curl download process is still listed by ps.
        ps -e | grep --quiet "$download_process_id"
        local process_grep_ret_val=$?

        # If the url download process has finished.
        if [ "$process_grep_ret_val" -ne "0" ]; then
            is_download_done="TRUE"

        # If the url download process has not finished.
        else
            # Store the current time for the timeout. Note: %s seconds, %N nanoseconds.
            local timeout_now=$(date +%s.%N)

            # Use bc to check if the timeout has been exceeded.
            local bc_exp_timeout="if (($timeout_now - $timeout_start) > $timeout) 1 else 0"
            local bc_exp_timeout_ret_val=$(echo "$bc_exp_timeout" | bc -l)

            # The timeout has been exceeded.
            if [ "$bc_exp_timeout_ret_val" -eq "1" ]; then
                is_download_timed_out="TRUE"
            fi
        fi
    done

    # If the download process completed successfully.
    if [ "$is_download_done" = "TRUE" ]; then

        # Extract IP addresses from the temp file. Each IP address will be separated by a new line
        # but if the source is working as expected then there will be exactly one IP address.
        local ip_address_list=$(Extract_IP_Addresses_From_File "$web_source_temp_file")

        # Delete the temp file.
        if [ -f "$web_source_temp_file" ]; then rm "$web_source_temp_file"; fi

        # Iterate through $ip_address_list counting the number of IP addresses.
        local ip_address_list_len="0"

        for valid_ip_address in $(echo "$ip_address_list" | tr "\n" " "); do
            let "ip_address_list_len = ip_address_list_len + 1"
        done

        # Success: if exactly 1, then that is considered to be the user's IP address, it is echoed
        # so that it can be placed in a variable at the point of function invocation.
        if [ "$ip_address_list_len" -eq "1" ]; then
            echo "$ip_address_list"
            return "$SINGLE_VALID_IP"

        # Failure: if less than 1, then no valid IP addresses were found.
        elif [ "$ip_address_list_len" -lt "1" ]; then
            return "$NO_VALID_IP"

        # Failure: if more than 1 there is no way to tell which of them is the user's IP address.
        # Echo the list so that the IP addresses can be displayed if being verbose.
        elif [ "$ip_address_list_len" -gt "1" ]; then
            echo "$ip_address_list"
            return "$MULTIPLE_VALID_IP"
        fi
    fi

    # If the url download process timed out (which it must have done or this section of the
    # function would not have been reached) then kill the process (and make doubly sure).

    # Since the process being killed has been disowned (removed from the shell's active jobs
    # table) the redirect of stdout and stderr to /dev/null may be unnecessary to hide the kill
    # messages, it is not necessary on the 3 Linux systems used to test the script, other OSes?

    if [ "$is_download_timed_out" = "TRUE" ]; then

        kill -INT "$download_process_id" > /dev/null 2>&1
        local kill_ret_val=$?

        if [ "$kill_ret_val" -ne "0" ]; then
            kill -KILL "$download_process_id" > /dev/null 2>&1
        fi
    fi

    # Delete the temp file.
    if [ -f "$web_source_temp_file" ]; then rm "$web_source_temp_file"; fi

    # Failure: the download timed out.
    return "$SERVER_TIMED_OUT"
}


# Function to check that all the IP addresses passed to it are exactly the same.
Verify_IP_List()
{
    # Assign the IP addresses passed to this function to an array.
    local ip_addresses=("$@")

    # Iterate through the array checking that all the IP addresses are the same.
    local ip
    for ip in "${ip_addresses[@]}"; do

        # If $ip is not the same as the first IP address in the array.
        if [ "$ip" != "${ip_addresses[0]}" ]; then
            return "$IP_ADDRESS_LIST_VERIFICATION_FAILURE"
        fi
    done

    # All the IP addresses in the array must be the same.
    return "$IP_ADDRESS_LIST_VERIFICATION_SUCCESS"
}


# Function that adds information to the statistics variables (check mode only).
Add_To_Stats()
{
    local status="$1"
    local ip_address_for_stats="$2"
    local ip_retrieval_time_for_stats="$3"

    # If the IP address is valid.
    if [ "$status" = "$SINGLE_VALID_IP" ]; then

        # Add the IP address to the list of valid IP addresses. [The trailing space is essential.]
        stats_ip_address_list+="$ip_address_for_stats "

        # Add the time taken to the list of times taken. [The trailing space is essential.]
        stats_ip_retrieval_times_list+="$ip_retrieval_time_for_stats "

        # Increment the valid IP address counter.
        let "stats_ip_valid_total = stats_ip_valid_total + 1"

    # If the IP address is not valid increment the invalid IP address counter.
    else
        let "stats_ip_invalid_total = stats_ip_invalid_total + 1"
    fi
}


# Function to find the fastest time in $stats_ip_retrieval_times_list (check mode only).
Find_Fastest_Time()
{
    # Awk expression to find the fastest time.
    local awk_fastest='{ if (NR == 1) fastest = $1; if ($1 < fastest) fastest = $1; } \
                   END { printf("%.3f", fastest) }'

    # Pipe the $stats_ip_retrieval_times_list to awk to get the fastest time.
    local fastest=$(echo "$stats_ip_retrieval_times_list" | tr " " "\n" | awk "$awk_fastest" )

    # Echo $fastest for capture at point of function invocation.
    echo "$fastest"
}


# Function to find the slowest time in $stats_ip_retrieval_times_list (check mode only).
Find_Slowest_Time()
{
    # Awk expression to find the slowest time.
    local awk_slowest='{ if (NR == 1) slowest = $1; if ($1 > slowest) slowest = $1; } \
                   END { printf("%.3f", slowest) }'

    # Pipe the $stats_ip_retrieval_times_list to awk to get the slowest time.
    local slowest=$(echo "$stats_ip_retrieval_times_list" | tr " " "\n" | awk "$awk_slowest" )

    # Echo $slowest for capture at point of function invocation.
    echo "$slowest"
}


# Function to calculate the mean of the $stats_ip_retrieval_times_list (check mode only).
Find_Mean_Average_Time()
{
    # Awk expression to calculate the mean average.
    local awk_mean='{ total += $1; num += 1; } \
                END { mean = total / num; printf("%.3f", mean) }'

    # Pipe the $stats_ip_retrieval_times_list to awk to get the mean.
    local mean=$(echo "$stats_ip_retrieval_times_list" | tr " " "\n" | awk "$awk_mean")

    # Echo $mean for capture at point of function invocation.
    echo "$mean"
}


# Function to calculate the median of the $stats_ip_retrieval_times_list (check mode only).
Find_Median_Average_Time()
{
    # Establish if there are an even or odd number of items stored in $stats_ip_retrieval_times_list.
    local num_items_parity
    let 'num_items_parity = stats_ip_valid_total % 2'

    # If an even number of items.
    if [ "$num_items_parity" -eq "0" ]; then

        # Work out which are the 2 middle items.
        local mid_item_1
        local mid_item_2
        let "mid_item_1 = stats_ip_valid_total / 2"
        let "mid_item_2 = mid_item_1 + 1"

        # Awk expression to print the median value (the mean of the 2 middle rows).
        local awk_median='{ if (NR == mid_row_1) med_1 = $1; if (NR == mid_row_2) med_2 = $1; } \
                      END { median = (med_1 + med_2) / 2; printf("%.3f", median) }'

        # Pipe the $stats_ip_retrieval_times_list to awk as a sorted list to get the median.
        local median=$(echo "$stats_ip_retrieval_times_list" | tr " " "\n" | sort -n |
                       awk "$awk_median" mid_row_1="$mid_item_1" mid_row_2="$mid_item_2")

    # If an odd number of items.
    else

        # Work out which is the middle item (integer arithmetic so division will be rounded down).
        local mid_item
        let "mid_item = (stats_ip_valid_total / 2) + 1"

        # Awk expression to print the median value (the middle row).
        local awk_median='{ if (NR == mid_row) printf("%.3f", $1) }'

        # Pipe the $stats_ip_retrieval_times_list to awk as a sorted list to get the median.
        local median=$(echo "$stats_ip_retrieval_times_list" | tr " " "\n" | sort -n |
                       awk "$awk_median" mid_row="$mid_item")
    fi

    # Echo $median for capture at point of function invocation.
    echo "$median"
}


# Function to display the statistical information (check mode only).
Display_Stats()
{
    # Trim whitespace from $stats_ip_retrieval_times_list and $stats_ip_address_list.
    local sed_exp_trim='s/^[ \t]*//g;s/[ \t]*$//g'
    stats_ip_retrieval_times_list=$(echo "$stats_ip_retrieval_times_list" | sed "$sed_exp_trim")
    stats_ip_address_list=$(echo "$stats_ip_address_list" | sed "$sed_exp_trim")

    # Find the fastest, slowest, mean, and median of the successful retrieval times.
    local fastest_time=$(Find_Fastest_Time)
    local slowest_time=$(Find_Slowest_Time)
    local mean_time=$(Find_Mean_Average_Time)
    local median_time=$(Find_Median_Average_Time)

    # Calculate the percentage of successful and failed retrievals.
    local bc_exp_success_percent="($stats_ip_valid_total / $web_sources_len) * 100"
    local success_percent=$(printf "%.2f" $(echo "$bc_exp_success_percent" | bc -l))
    local bc_exp_failed_percent="100 - $success_percent"
    local failed_percent=$(printf "%.2f" $(echo "$bc_exp_failed_percent" | bc -l))

    # Calculate the number of unique IP addresses held in $stats_ip_address_list.
    # Important: Use printf to avoid the new line that echo would add.
    local ip_num_unique=$(printf "$stats_ip_address_list" | tr " " "\n" | sort -n | uniq | wc -l)

    # Output the statistics.

    printf "%s\n" "------------------------------------------------------------------------------"

    printf "%-62s %s\n" "Web Sources Checked Total:" "$web_sources_len"
    printf "%-62s %-3s (%s%%)\n" "IP Addresses - Successfully Retrieved:" \
                                 "$stats_ip_valid_total" "$success_percent"
    printf "%-62s %-3s (%s%%)\n" "IP Addresses - Failed (Timed Out / Invalid):" \
                                 "$stats_ip_invalid_total" "$failed_percent"
    printf "%-62s %s\n" "Fastest (Successfully Retrieved):" "$fastest_time secs"
    printf "%-62s %s\n" "Slowest (Successfully Retrieved):" "$slowest_time secs"
    printf "%-62s %s\n" "Mean Average   (Successfully Retrieved):" "$mean_time secs"
    printf "%-62s %s\n" "Median Average (Successfully Retrieved):" "$median_time secs"

    # The number of unique IP addresses should be 1, if not issue a warning.
    if [ "$ip_num_unique" = "1" ]; then
        printf "%-62s %s\n" "Unique IP Addresses (Should Be 1):" "$ip_num_unique"
    else
        printf "%-62s %s *WARNING*\n" "Unique IP Addresses (Should Be 1):" "$ip_num_unique"
    fi

    printf "%s\n" "------------------------------------------------------------------------------"

    # Echo a warning if there were no valid IP addresses.
    if [ "$stats_ip_valid_total" = "0" ]; then
        printf "\nWARNING: The web sources did not supply any valid IP addresses at all. The\n"
        printf "most likely cause is that you have lost your internet connection or possibly\n"
        printf "the web source list file used is extremely out-of-date or corrupted.\n\n"
    fi

    # Echo a warning if the number of unique IP addresses is > 1, and list them.
    if [ "$ip_num_unique" -gt "1" ]; then
        printf "\nWARNING: The number of unique IP addresses retrieved exceeds one, it should be\n"
        printf "exactly one, your IP address. One or more of the sources is no longer suitable\n"
        printf "for use by this script. If you have the time please contact the developer with\n"
        printf "the url of the problematic source, use the -h option for contact details.\n\n"

        # List each unique IP address in a table along with its number of occurrences.

        # This awk code uses a useful aspect of awk where any number or string in awk may be used
        # as an array index. In this case the IP addresses piped to awk will serve as the array
        # indexes for the count_ip[] array.
        local awk_ip_count='{ count_ip[$1]++ } END { for (ip in count_ip) \
                              printf("%-25s %d\n", ip, count_ip[ip]); }'

        printf "Unique IP Addresses List:\n\n"
        printf "IP Address         Num Occurrences\n"
        printf "%s\n" "----------------------------------"
        echo "$stats_ip_address_list" | tr " " "\n" | awk "$awk_ip_count"
        printf "%s\n" "----------------------------------"
    fi
}


# Function which controls IP address retrieval and verification.
Handle_Get_IP_Mode()
{
    if [ "$verbose" = "TRUE" ]; then
        printf "\n%-25s %s\n" "Script Version:" "$version"
        printf "%-25s %s\n" "Operation Mode:" "Get IP Address"
        printf "%-25s %s\n" "Num Web Sources:" "$web_sources_verify_num (used for verification)"
        printf "%-25s %s\n" "Timeout (Seconds):" "$timeout"
        printf "%-25s %s\n" "Download Program:" "$download_program_display_text"
    fi

    # Read the web sources list into $web_sources.
    Read_Source_Url_List

    if [ "$verbose" = "TRUE" ]; then
        printf "%-25s %s\n" "Web Sources Location:" "$web_sources_list_location_display_text"
        printf "%-25s %s\n" "Load Web Sources:" "Succeeded ($web_sources_len urls)"
    fi

    # Check if the number of source urls is fewer than the required minimum.
    if [ "$web_sources_len" -lt "$web_sources_min" ]; then
        Exit_With_Error_Message "$EXIT_ERR_NOT_ENOUGH_SOURCE_URLS"
    fi

    # Shuffle the web sources array.
    Randomize_Url_List

    if [ "$verbose" = "TRUE" ]; then
        printf "%-25s %s\n" "Randomize Web Sources:" "Done"
    fi

    # Array to hold the IP addresses retrieved.
    local ip_list=()

    # Counter to hold how many IP addresses have so far been retrieved.
    local ip_counter="0"

    # Test error handling with 'fake' IP address.
    # 1 = has 1 IP address, 2 = has 2 IP addresses, 3 = no IP addresses, 4 = does not exist.
    # web_sources[1]="https://crius.feralhosting.com/gencon/fakeip1.html"
    # web_sources[1]="https://crius.feralhosting.com/gencon/fakeip2.html"
    # web_sources[2]="https://crius.feralhosting.com/gencon/fakeip3.html"
    # web_sources[2]="https://crius.feralhosting.com/gencon/fakeip4.html"

    # Loop through the urls in $web_sources. The script will exit from within this loop if the IP
    # address has been successfully retrieved from the required number of sources and verified.
    local url

    for url in "${web_sources[@]}"; do

        if [ "$verbose" = "TRUE" ]; then
            local time_start=$(date +%s.%N);
            printf "\n%-25s %s\n" "Trying Web Source:" "$url"
        fi

        # Store any IP addresses in the page source of $url in $ip_address.
        local ip_address
        ip_address=$(Get_IP_Address_From_Web_Source "$url")
        local get_ip_ret_val=$?

        # Failed due to a temp file creation error.
        if [ "$get_ip_ret_val" -eq "$TEMP_FILE_CREATION_FAILED" ]; then
            Exit_With_Error_Message "$EXIT_ERR_TEMP_FILE_CREATION_FAILED"
        fi

        if [ "$verbose" = "TRUE" ]; then

            # Calculate the time taken (reduce to 3 decimal places).
            local time_stop=$(date +%s.%N)
            local time_taken=$(printf "%.3F" $(echo "$time_stop - $time_start" | bc -l))

            if [ "$get_ip_ret_val" -eq "$SINGLE_VALID_IP" ]; then
                printf "%-25s %s\n" "Status:" "Success: A single valid IP address"
                printf "%-25s %s\n" "IP Address:" "$ip_address"

            elif [ "$get_ip_ret_val" -eq "$NO_VALID_IP" ]; then
                printf "%-25s %s\n" "Status:" "Failure: No IP address"

            elif [ "$get_ip_ret_val" -eq "$MULTIPLE_VALID_IP" ]; then
                printf "%-25s %s\n" "Status:" "Failure: More than one IP address"
                # Create neat text for displaying the list of IP addresses.
                local ip_address_ssv
                ip_address_ssv=$(echo "$ip_address" | tr '\n' ' ' | sed "s/ /, /g" | sed "s/, $//g")
                printf "%-25s %s\n" "IP Addresses:" "$ip_address_ssv"

            elif [ "$get_ip_ret_val" -eq "$SERVER_TIMED_OUT" ]; then
                printf "%-25s %s\n" "Status:" "Failure: Server timed out"
            fi

            printf "%-25s %s\n" "Duration (Seconds):" "$time_taken"
        fi

        # If a single valid IP addresses was retrieved.
        if [ "$get_ip_ret_val" -eq "$SINGLE_VALID_IP" ]; then

            # Add the IP address to the array, and increment the counter.
            ip_list[$ip_counter]="$ip_address"
            let "ip_counter = ip_counter + 1"

            if [ "$verbose" = "TRUE" ]; then
                printf "%-25s %s\n" "Add IP To List:" "$ip_counter of $web_sources_verify_num"
            fi

            # Check if all the IP addresses in the array are the same.
            Verify_IP_List "${ip_list[@]}"
            local verify_ip_list_ret_val=$?

            # If the IP addresses in the array are NOT all the same then re-start.
            if [ "$verify_ip_list_ret_val" = "$IP_ADDRESS_LIST_VERIFICATION_FAILURE" ]; then

                if [ "$verbose" = "TRUE" ]; then
                    printf "\n%-25s %s\n" "IP List Verification:" "Failed [IP addresses differed]"
                    printf "%-25s %s\n" "Status:" "Restart using different sources"
                fi

                # Reset the array and counter variables.
                unset ip_list
                ip_counter="0"
            fi

            # If the array now contains the required number of IP addresses to use for verification
            # then output the IP address and exit. Note: The if statement below will only evaluate
            # to true if the IP addresses in the array are all the same, if they weren't then
            # $ip_counter would have been set to 0 in the conditional above.
            if [ "$ip_counter" -eq "$web_sources_verify_num" ]; then

                if [ "$verbose" = "TRUE" ]; then
                    printf "\n%-25s %s" "IP List Verification:" "Succeeded "
                    printf "%s\n" "[$ip_counter identical IP addresses]"
                    printf "\n%-25s %s\n" "IPv4 address:" "$ip_address"
                else
                    if [ "$bare_output" = "TRUE" ]; then
                        printf "%s\n" "$ip_address"
                    else
                        printf "%s\n" "IPv4 address: $ip_address"
                    fi
                fi

                # The exit point of the script in getip mode.
                exit "$EXIT_SUCCESS"
            fi
        fi

    done  # End of loop through $web_sources loop.

    # The exit point of the script in getip mode if the script failed to supply a valid IP address
    # from the required number of sources.
    Exit_With_Error_Message "$EXIT_ERR_FAILED_TO_GET_IP_ADDRESS"
}


# Function which controls testing a web page to see if suitable for inclusion in the source list.
Handle_Test_Url_Mode()
{
    local time_start=$(date +%s.%N)

    # Store any IP addresses in the page source of $test_url_address in $ip_address.
    local ip_address
    ip_address=$(Get_IP_Address_From_Web_Source "$test_url_address")
    local get_ip_ret_val=$?

    # Failed due to temp file creation error.
    if [ "$get_ip_ret_val" -eq "$TEMP_FILE_CREATION_FAILED" ]; then
        Exit_With_Error_Message "$EXIT_ERR_TEMP_FILE_CREATION_FAILED"
    fi

    local time_stop=$(date +%s.%N)
    local time_taken=$(printf "%.3F" $(echo "$time_stop - $time_start" | bc -l))

    # 'Pretty print' the test results.

    printf "\n%-22s %s\n" "Script Version:" "$version"
    printf "%-22s %s\n" "Operation Mode:" "Test Url"
    printf "%-22s %s\n" "Test Url:" "$test_url_address"
    printf "%-22s %s\n" "Download Program:" "$download_program_display_text"
    printf "%-22s %s\n" "Timeout  (Seconds):" "$timeout"
    printf "%-22s %s\n" "Duration (Seconds):" "$time_taken"

    if [ "$get_ip_ret_val" -eq "$SINGLE_VALID_IP" ]; then
        printf "%-22s %s\n" "Status:" "Success: A single valid IP address"
        printf "%-22s %s\n" "IP Address:" "$ip_address"
        printf "%-22s %s\n" "Conclusion:" "Url is suitable for using as a source."
        printf "%-22s %s\n" "" "[As long as the IP address is correct.]"
        exit "$EXIT_SUCCESS"

    elif [ "$get_ip_ret_val" -eq "$MULTIPLE_VALID_IP" ]; then
        printf "%-22s %s\n" "Status:" "Failure: More than one IP address"
        # Create neat text for displaying the list of IP addresses.
        local ip_address_ssv=$(echo "$ip_address" | tr '\n' ' ' | sed "s/ /, /g" | sed "s/, $//g")
        printf "%-22s %s\n" "IP Addresses:" "$ip_address_ssv"
        printf "%-22s %s\n" "Conclusion:" "Url is NOT suitable for using as a source."
        exit "$EXIT_ERR_TEST_URL_MULTIPLE_IP_ADDRESS"

    elif [ "$get_ip_ret_val" -eq "$NO_VALID_IP" ]; then
        printf "%-22s %s\n" "Status:" "Failure: No IP address"
        printf "%-22s %s\n" "Conclusion:" "Url is NOT suitable for using as a source."
        exit "$EXIT_ERR_TEST_URL_NO_IP_ADDRESS"

    elif [ "$get_ip_ret_val" -eq "$SERVER_TIMED_OUT" ]; then
        printf "%-22s %s\n" "Status:" "Failure: Server timed out"
        printf "%-22s %s\n" "Conclusion:" "Url is NOT suitable for using as a source unless the"
        printf "%-22s %s\n" "" "timeout was set very low or its timing out was unusual."
        exit "$EXIT_ERR_TEST_URL_TIMED_OUT"
    fi
}


# Function which controls checking if the web pages in the source list are working or not.
Handle_Check_Mode()
{
    printf "\n%-24s %s\n" "Script Version:" "$version"
    printf "%-24s %s\n" "Operation Mode:" "Check Source Urls"
    printf "%-24s %s\n" "Timeout (Seconds):" "$timeout"
    printf "%-24s %s\n" "Download Program:" "$download_program_display_text"

    # Read the web sources list into $web_sources.
    Read_Source_Url_List

    printf "%-24s %s\n" "Web Sources Location:" "$web_sources_list_location_display_text"
    printf "%-24s %s\n\n" "Load Web Sources:" "Succeeded ($web_sources_len Urls)"

    # Table header.
    printf "%s\n" "------------------------------------------------------------------------------"
    printf "%-23s %-40s %s\n" "Seconds" "Url" "IP/Status"
    printf "%s\n" "------------------------------------------------------------------------------"

    # Loop through the urls in $web_sources providing information about the status of each one.
    local url

    for url in "${web_sources[@]}"; do

        local time_start=$(date +%s.%N);

        # Store any IP addresses in the page source of $url in $ip_address.
        local ip_address
        ip_address=$(Get_IP_Address_From_Web_Source "$url")
        local get_ip_ret_val=$?

        # Failed due to temp file creation error.
        if [ "$get_ip_ret_val" -eq "$TEMP_FILE_CREATION_FAILED" ]; then
            Exit_With_Error_Message "$EXIT_ERR_TEMP_FILE_CREATION_FAILED"
        fi

        # Calculate the time taken (reduce to 3 decimal places).
        local time_stop=$(date +%s.%N)
        local time_taken=$(printf "%.3F" $(echo "$time_stop - $time_start" | bc -l))

        # Add info. to the stored statistics for displaying at the end.
        Add_To_Stats "$get_ip_ret_val" "$ip_address" "$time_taken"

        # Set the failure text for display.
        local web_source_failure_text

        if [ "$get_ip_ret_val" -eq "$NO_VALID_IP" ]; then
            web_source_failure_text="NO IP ADDRESS"

        elif [ "$get_ip_ret_val" -eq "$MULTIPLE_VALID_IP" ]; then
            web_source_failure_text="MULTIPLE IP"

        elif [ "$get_ip_ret_val" -eq "$SERVER_TIMED_OUT" ]; then
            web_source_failure_text="URL TIMED OUT"
        fi

        # Neatly display the duration, source url, and the IP address or failure text.

        # Truncate the $url if its length will muck up the table columns.
        local url_length=${#url}
        if [ "$url_length" -gt "52" ]; then
            local truncate_url=${url:0:48}
            url="$truncate_url..."
        fi

        if [ "$get_ip_ret_val" -eq "$SINGLE_VALID_IP" ]; then
            printf "%-8s %-53s %s\n" "$time_taken" "$url" "$ip_address"
        else
            printf "%-8s %-53s %s\n" "$time_taken" "$url" "$web_source_failure_text"
        fi

    done  # End of loop through $web_sources loop.

    # Display the stats info.
    Display_Stats

    # The exit point of the script in check mode.
    exit "$EXIT_SUCCESS"
}


#
# Process The Command Line.
#


# Before processing the arguments with getopts, check for long options:

# Check if the first arg is "--help", if so display help and exit (-h is handled by getopts).
if [ "$1" = "--help" -o "$1" = "-?" ]; then
    Display_Usage_Message
    exit "$EXIT_SUCCESS"

# Check if the first arg is "--version", etc., if so display version number and exit.
elif [ "$1" = "--version" -o "$1" = "-version" -o "$1" = "-ver" ]; then
    echo "getip v. $version"
    exit "$EXIT_SUCCESS"
fi

# Holds how many modes the user set on the command line. Only 1 mode can be used at a time.
modes_used_counter="0"

# Use getopts to process the command line arguments.
while getopts ":gcu:bvhn:t:d:s:" option; do

    case $option in

        g)
            # -g has been used, set the operation mode.
            operation_mode="$GET_IP_MODE"
            let "modes_used_counter = modes_used_counter + 1"
            ;;

        c)
            # -c has been used, set the operation mode.
            operation_mode="$CHECK_MODE"
            let "modes_used_counter = modes_used_counter + 1"
            ;;

        u)
            # -u url has been used, set the operation mode and set the test url address.
            operation_mode="$TEST_URL_MODE"
            test_url_address="$OPTARG"
            let "modes_used_counter = modes_used_counter + 1"
            ;;

        b)
            # -b has been used, turn on bare output.
            bare_output="TRUE"
            ;;

        v)
            # -v has been used, turn on verbose.
            verbose="TRUE"
            ;;

        h)
            # -h has been used, display help and then exit.
            Display_Usage_Message
            exit "$EXIT_SUCCESS"
            ;;

        n)
            # -n num has been used, set the number of sources to use.
            web_sources_verify_num="$OPTARG"

            # Use sed to check that $web_sources_verify_num is a single digit >=2 and <=5.
            num_min="2"
            num_max="5"

            sed_exp_is_num_in_range="s/^[$num_min-$num_max]\{1\}$/SINGLE_DIGIT_IN_RANGE/g"
            is_num_in_range=$(echo "$web_sources_verify_num" | sed "$sed_exp_is_num_in_range")

            # A single digit in range has not been entered.
            if [ "$is_num_in_range" != "SINGLE_DIGIT_IN_RANGE" ]; then
                Exit_With_Error_Message "$EXIT_ERR_ARG_INVALID" "-n"
            fi
            ;;

        t)
            # -t seconds has been used, set the user timeout variable.
            timeout_user_set="$OPTARG"

            # Check that $timeout_user_set is a number, int or real. This is a Bash hack to use
            # awk to check whether a variable is any kind of number (int or real).
            awk_is_num='{ if ($1 + 0 == $1) print "Num"; else print "Not_num" }'
            awk_is_num_ret_val=$(echo "$timeout_user_set" | awk "$awk_is_num")

            if [ "$awk_is_num_ret_val" = "Not_num" ]; then
                Exit_With_Error_Message "$EXIT_ERR_ARG_INVALID" "-t"
            fi

            # Use bc to check that $timeout_user_set is a number in range, >=0.2 and <=30.
            # The awk check has to be done first as bc doesn't like being given a non-number.

            timeout_min="0.2"
            timeout_max="30"

            bc_exp_is_num_in_range="if ($timeout_user_set < $timeout_min || \
                                        $timeout_user_set > $timeout_max) 1 else 0"
            bc_exp_is_num_in_range_ret_val=$(echo "$bc_exp_is_num_in_range" | bc -l)

            # A positive number in range has not been entered.
            if [ "$bc_exp_is_num_in_range_ret_val" = "1" ]; then
                Exit_With_Error_Message "$EXIT_ERR_ARG_INVALID" "-t"
            fi
            ;;

        d)
            # -d has been used to set the download program, set which program to use.
            download_program_user_set_lc=$(echo "$OPTARG" | tr '[:upper:]' '[:lower:]')

            if [ "$download_program_user_set_lc" = "wget" ]; then
                download_program_user_set="$DOWNLOAD_PROGRAM_WGET"

            elif [ "$download_program_user_set_lc" = "curl" ]; then
                download_program_user_set="$DOWNLOAD_PROGRAM_CURL"

            else
                Exit_With_Error_Message "$EXIT_ERR_ARG_INVALID" "-d"
            fi
            ;;

        s)
            # -s has been used to set the location of the source urls list.
            web_sources_list_user_set="$OPTARG"
            ;;

        \?)
            # User entered an invalid option, e.g. '-x'. Note: Invalid args with no '-' prefix,
            # are handled immediately after this while loop.
            Exit_With_Error_Message "$EXIT_ERR_OPTION_INVALID"
            ;;

        :)
            # User omitted the required additional arg of an option (e.g. -t <BLANK>).
            Exit_With_Error_Message "$EXIT_ERR_ARG_MISSING" "$OPTARG"
            ;;
    esac
done


# getopts stops processing args if it encounters an arg without a '-' prefix (unless it is an
# $OPTARG in which case processing continues). Check if any invalid args have been entered by
# working out how many args getopts has actually processed, shifting them off, and if more
# than 0 args remain then an invalid arg must have been entered.

let "num_args_processed_by_getopts = OPTIND - 1"
shift "$num_args_processed_by_getopts"
num_args_remaining="$#"

if [ "$num_args_remaining" -gt "0" ]; then
    Exit_With_Error_Message "$EXIT_ERR_OPTION_INVALID"
fi

# If the user set more than 1 mode on the command line, display error and exit.
if [ "$modes_used_counter" -gt "1" ]; then
    Exit_With_Error_Message "$EXIT_ERR_MORE_THAN_ONE_MODE_SET"
fi

# Set the appropriate timeout.

# If the user set the timeout use that otherwise use the default.
if [ "$timeout_user_set" != "FALSE" ]; then
    timeout="$timeout_user_set"
else
    if [ "$operation_mode" = "$GET_IP_MODE" ]; then
        timeout="$timeout_get_ip_mode_default"

    elif [ "$operation_mode" = "$CHECK_MODE" ]; then
        timeout="$timeout_check_mode_default"

    elif [ "$operation_mode" = "$TEST_URL_MODE" ]; then
        timeout="$timeout_test_url_mode_default"
    fi
fi

# If the user has set both verbose and bare output, display error and exit.
if [ "$verbose" = "TRUE" -a "$bare_output" = "TRUE" ]; then
    Exit_With_Error_Message "$EXIT_ERR_VERBOSE_AND_BARE_SET"
fi

# Determine whether to use wget or curl as the download program.

# Check whether wget and curl are installed, at least one is required.
hash wget > /dev/null 2>&1
is_wget_installed=$?

hash curl > /dev/null 2>&1
is_curl_installed=$?

# If the user has not set which one to use on the command line.
if [ "$download_program_user_set" = "FALSE" ]; then

    # If both wget and curl are installed, use the default.
    if [ "$is_wget_installed" -eq "0" -a "$is_curl_installed" -eq "0" ]; then
        download_program="$download_program_default"

    # If only wget is installed, use that.
    elif [ "$is_wget_installed" -eq "0" -a "$is_curl_installed" -ne "0" ]; then
        download_program="$DOWNLOAD_PROGRAM_WGET"

    # If only curl is installed, use that.
    elif [ "$is_curl_installed" -eq "0" -a "$is_wget_installed" -ne "0" ]; then
        download_program="$DOWNLOAD_PROGRAM_CURL"

    # If neither wget nor curl are installed, display error and exit.
    else
        Exit_With_Error_Message "$EXIT_ERR_WGET_AND_CURL_NOT_INSTALLED"
    fi

# If the user has set which one to use on the command line.
else

    # If the user set wget, use that if it is installed.
    if [ "$download_program_user_set" = "$DOWNLOAD_PROGRAM_WGET" ]; then

        if [ "$is_wget_installed" -eq "0" ]; then
            download_program="$DOWNLOAD_PROGRAM_WGET"
        else
            Exit_With_Error_Message "$EXIT_ERR_WGET_NOT_INSTALLED"
        fi

    # If the user set curl, use that if it is installed.
    elif [ "$download_program_user_set" = "$DOWNLOAD_PROGRAM_CURL" ]; then

        if [ "$is_curl_installed" -eq "0" ]; then
            download_program="$DOWNLOAD_PROGRAM_CURL"
        else
            Exit_With_Error_Message "$EXIT_ERR_CURL_NOT_INSTALLED"
        fi
    fi
fi

# Set the url download program display text (used if being verbose).
if [ "$download_program" = "$DOWNLOAD_PROGRAM_WGET" ]; then
    download_program_display_text="wget"
elif [ "$download_program" = "$DOWNLOAD_PROGRAM_CURL" ]; then
    download_program_display_text="curl"
fi

# Call the appropriate function for the operation mode, the script will exit from within these
# functions so the conditional below will never complete.

if [ "$operation_mode" = "$GET_IP_MODE" ]; then
    Handle_Get_IP_Mode

elif [ "$operation_mode" = "$TEST_URL_MODE" ]; then
    Handle_Test_Url_Mode

elif [ "$operation_mode" = "$CHECK_MODE" ]; then
    Handle_Check_Mode
fi


##
## END OF SCRIPT.
##
