#!/bin/bash

#
# Name:            getip
# Description:     Script to retrieve a WAN IPv4 address from IP providing web sites.
# Requirements:    UNIX/Linux, Bash, either wget or curl (wget is a little faster).
# Exit Status:     0 on success, and non-zero on failure.
# Author:          mattst@i-dig.info
# Homepage:        https://github.com/mattst/getip
# License:         GNU General Public License v3 - http://www.gnu.org/copyleft/gpl.html
#
# getip retrieves the wide area network (WAN) IP address; this is sometimes referred to as the
# external IP address or the public IP address. It gets IPv4 addresses only and not IPv6 addresses.
# getip always gets the IP address from multiple sources so that it can be verified. See the
# documentation for detailed information; a man page is built-in to this script and can be viewed
# using the --help option, while -h will display a help summary (cheat sheet).
#
#
# Developer's Notes:
#
# 1. The list of all utilities and Bash shell commands used by the getip script. With the exception
#    of wget and curl all of the utilities below are expected to exist on all modern UNIX like
#    operating systems (wget is fairly standard these days). If neither wget nor curl are installed
#    the script will inform the user that one of them needs to be installed.
#
#    awk, bc, case (etc.), cat, curl, date, disown, echo, exit, for (etc.), getopts, grep, hash,
#    if (etc.), kill, let, local, man, mktemp, printf, ps, $RANDOM, read, return, rm, sed, shift,
#    sort, tr, uniq, unset, wc, wget, while (etc.). [Either wget or curl is required.]
#
# 2. In Bash if 'local' is on the same line as a command and '$?' is used immediately afterwards,
#    then '$?' will return local's exit status and not the command's, effectively '$?' will always
#    return zero. In such cases define the local first. e.g.
#
# Wrong:
#    local example=$(program ...)
#    local program_ret_val=$?
#
# Right:
#    local example
#    example=$(program ...)
#    local program_ret_val=$?
#
# 3. If a function is called using the $() construct then a subshell will be created for it. This
#    means that any exit call from inside the function will exit only the subshell and not the
#    script. Watch out for this if the intention is to exit the script. This is seen in the function
#    Get_IP_Address_From_Web_Source() which returns an error constant if its call to mktemp fails,
#    rather than exiting the script which is what happens if mktemp were to fail elsewhere.
#
# 4. Note that a regex pattern used with the =~ regular expression matching operator is handled
#    differently by different versions of Bash. The safe way of formatting these is to always place
#    the pattern in a variable first and then to avoid quoting the pattern in the conditional. e.g.
#
# Wrong:
#    regex_pattern="^regex$"
#    if [[ "$variable" =~ "$regex_pattern" ]]; then
#
# Right:
#    regex_pattern="^regex$"
#    if [[ "$variable" =~ $regex_pattern ]]; then
#


#
# CONSTANTS
#


VERSION="1.0.1"

# The 3 operation modes.
GET_IP_MODE="1"
TEST_URL_MODE="2"
CHECK_MODE="3"

# Holds the default url to the list of web sources.
WEB_SOURCES_LIST_DEFAULT="https://crius.feralhosting.com/gencon/SourceUrlsList"

# The default, min, and max values of the number of web sources to use for verification.
WEB_SOURCES_VERIFY_NUM_DEFAULT="2"
WEB_SOURCES_VERIFY_NUM_MIN="2"
WEB_SOURCES_VERIFY_NUM_MAX="5"

# The default, min, and max timeouts.
TIMEOUT_MIN="0.2"
TIMEOUT_MAX="15"
TIMEOUT_DEFAULT_GET_IP_MODE="1.5"
TIMEOUT_DEFAULT_CHECK_MODE="10"
TIMEOUT_DEFAULT_TEST_URL_MODE="10"

# The minimum number of web sources that are required (in a web sources list).
WEB_SOURCES_MIN="10"

# The 2 download program possibilities.
DOWNLOAD_PROGRAM_WGET="1"
DOWNLOAD_PROGRAM_CURL="2"

# Holds the default download program (wget is slightly faster on tested systems).
DOWNLOAD_PROGRAM_DEFAULT="$DOWNLOAD_PROGRAM_WGET"

# Holds the user agent to use for web page downloads. The script does not try to hide that it
# is a script. The user agent header is modelled on the one used by Google's 'googlebot':
# Googlebot 2.1: "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
USER_AGENT="Mozilla/5.0 (compatible; getipscript/$VERSION; +https://github.com/mattst/getip)"

# Holds a regex to match a http url (just a rudimentary case insensitive match).
REGEX_HTTP_URL="^[Hh][Tt][Tt][Pp][Ss]?://.+\..+$"

# The values returned by the Get_IP_Address_From_Web_Source() function.
GET_IP_SINGLE_VALID_IP="0"
GET_IP_NO_VALID_IP="1"
GET_IP_MULTIPLE_VALID_IP="2"
GET_IP_SERVER_TIMED_OUT="3"
GET_IP_TEMP_FILE_CREATION_FAILED="4"

# The values returned by the Verify_IP_Address_List() function.
VERIFY_IP_ADDRESS_LIST_SUCCESS="0"
VERIFY_IP_ADDRESS_LIST_FAILURE="1"

# The various exit values.
EXIT_SUCCESS="0"
EXIT_ERR_ARG_INVALID="101"
EXIT_ERR_OPTION_INVALID="102"
EXIT_ERR_ARG_MISSING="103"
EXIT_ERR_DOWNLOAD_PROGRAM_NOT_INSTALLED="104"
EXIT_ERR_TEMP_FILE_CREATION_FAILED="105"
EXIT_ERR_WEB_SOURCE_LIST_INVALID="106"
EXIT_ERR_READ_WEB_SOURCE_LIST_FAILED="107"
EXIT_ERR_NOT_ENOUGH_SOURCE_URLS="108"
EXIT_ERR_FAILED_TO_GET_IP_ADDRESS="109"
EXIT_ERR_TEST_URL_NO_IP_ADDRESS="110"
EXIT_ERR_TEST_URL_MULTIPLE_IP_ADDRESS="111"
EXIT_ERR_TEST_URL_TIMED_OUT="112"


#
# VARIABLES
#


# Holds the operation mode, default is $GET_IP_MODE.
operation_mode="$GET_IP_MODE"

# Array which holds the urls from the web sources list.
web_sources=()

# Holds the number of urls in $web_sources.
web_sources_len="0"

# Holds the number of web sources to use for verification.
web_sources_verify_num="$WEB_SOURCES_VERIFY_NUM_DEFAULT"

# Holds the url or local file if the user set the location of the web sources list.
web_sources_list_user_set="FALSE"

# Holds the timeout value in seconds (of each web source, no overall script timeout).
timeout=""

# Holds the user set timeout.
timeout_user_set="FALSE"

# Holds the download program.
download_program="$DOWNLOAD_PROGRAM_DEFAULT"

# Holds the user set download program.
download_program_user_set="FALSE"

# Holds if the user has turned on verbose reporting.
verbose="FALSE"

# Holds if the user has turned on bare output.
bare_output="FALSE"

# Holds the url of the web source to test in test url mode.
test_url_address=""

# Variables used to hold statistical data in check mode.
stats_ip_address_list=""
stats_ip_valid_total="0"
stats_ip_invalid_total="0"
stats_ip_retrieval_times_list=""

# Variables used only for neatly displaying information if being verbose.
download_program_display_text=""
web_sources_list_location_display_text=""


#
# FUNCTIONS
#


# Function to display the help summary (cheat sheet). [-h]
Display_Help_Summary()
{
cat <<EOF
getip v. $VERSION - script usage and help summary (cheat sheet).

--help displays the full manual, a built-in man page will be shown.

A Bash script used to get the WAN IPv4 address (aka external/public IP).
The IP address will be retrieved from multiple web sources and verified.

getip [OPTION...]
getip [ --help | --version ]

In typical usage no options are needed; -g is the default.

-g         Get the IP address from multiple sources and verify it. [Default]
-n num     Set the num sources used to verify IP address. [Default: 2; >=2 <=5]
-t secs    Set the timeout for each web source. [Default 1.5 secs; >=0.2 <=15]
-s file    Set a custom web sources list file. [Use: file path or http url.]
-p prog    Set the program to download each source. [Use: 'wget' or 'curl'.]
           [Default: wget; unless only curl is installed. wget is faster.]
-v         Verbose; print detailed information while the script runs.
-b         Bare; print only the IP address.
-h         Help; print this usage and help summary (cheat sheet).
--help     Help; display the full manual in the form of a built-in man page.
--version  Print the version number.
-c         Diagnostic information about all sources.  [List maintenance only.]
-u url     Tests an url for possible use as a source. [List maintenance only.]
EOF
}


# Function to display the manual in the form of an inbuilt man page. [--help]
Display_Help_Man_Page()
{
    # Create a temp file to store the man page here document.
    local getip_man_page_temp_file
    getip_man_page_temp_file=$(mktemp -q -t "getip.tmp.XXXXXX")
    local make_temp_file_ret_val=$?

    if [ "$make_temp_file_ret_val" -ne "0" ]; then
        Exit_With_Error_Message "$EXIT_ERR_TEMP_FILE_CREATION_FAILED"
    fi

# Output a here document of the getip man page to the temp file.
# Here documents don't like indentation, keep to the hard left.

cat >>$getip_man_page_temp_file <<EOF
.\" Manpage for getip.

.TH GETIP 1 "04 Nov 2015" "1.0.1" "GETIP VERSION 1.0.1"

.SH NAME

getip retrieves the wide area network (WAN) IP address; this is sometimes referred to as the external IP address or the public IP address. It gets IPv4 addresses only and not IPv6 addresses.

.SH SYNOPSIS
\fBgetip\fR [OPTION...]
.br
\fBgetip\fR [ --help | --version ]

In typical usage no options are necessary.


.SH DESCRIPTION

\fBgetip\fR is a \fBBASH\fR script used to retrieve the WAN IPv4 address from behind a router using web pages (sources) which provide the IP address of the 'visitor' in HTML or plain text. Once retrieved and confirmed, the IP address is printed to the terminal (\fBSTDOUT\fR). \fBgetip\fR can use either \fBwget\fR or \fBcurl\fR as the download program to retrieve the web sources. See: \fBREQUIREMENTS\fR

From time to time a web page used as a source will stop providing IP addresses, either temporarily or permanently, but an example IP address might remain on the web page, or an IP-address-like software version number might be present, embedded in the HTML, and which could be mistaken for a real IP address (although the script tries hard to spot these so they can be ignored). In such cases, or in other similar ones, an IP address fetching script could provide an incorrect IP address.

To prevent an incorrect IP address being provided \fBgetip\fR always retrieves the IP address from multiple sources (2 by default) and compares them to make sure they are the same. In the unusual cases that they differ \fBgetip\fR will restart the retrieval procedure and use different web sources. The number of sources used for verification can be set on the command line using the \fB-n\fR option, the range of values is >=2 and <=5 but the default of 2 is considered to be sufficient due to the unlikelihood of 2 web sources both providing the same incorrect IP address. Very cautious users might want to use 3 sources, while 5 would indicate a state of paranoia. Clearly if more sources are used then the IP address retrieval will take longer (on average), 1 second per source on a low latency network is not an unreasonable guesstimate.

\fBgetip\fR uses a list of web sources held in a file on the script's \fBGithub\fR page. Each web source is a web page that would display the IP address if viewed in a web browser. \fBgetip\fR downloads the web sources list file each time the script is run so that users are always using the most up-to-date list, even if they are not using the most recent version of \fBgetip\fR. An alternative web sources list can be set on the command line using the \fB-s\fR option which can take a local file path or http url. The urls in a web sources list file must each be on a separate line and any line which does not begin with 'http' will be ignored ('https' is fine), so avoid leading whitespace. The minimum number of urls needed for a valid list is 10. See the description of the \fB-u\fR option (Test Url mode), if you wish to create a custom web sources list file.

The \fB-s\fR option can also be used to specify a copy of the web sources list file which has been downloaded and saved locally but clearly the most up-to-date list will not be used each time the script is run, and the quality of the list will degrade over time.

The web sources list is randomized every time the script is run. This is so that undue pressure is not placed on servers at the top of the list. The randomizing process is done very quickly and does not have a significant impact on the running time of the script.

\fBgetip\fR implements its own timeout facility; it does not rely on the timeout options of \fBwget\fR or \fBcurl\fR, neither of which can accept timeouts in the fractions of a second. Since \fBgetip\fR uses many web sources a relatively low timeout is used for each web source, this can be set with the \fB-t\fR option. If one web source fails to respond quickly then \fBgetip\fR simply moves on to the next one. In Get IP mode the default timeout for each web source is 1.5 seconds. If modifying this a sensible range would be 0.75 to 3 seconds but tweaking the timeout for your internet connection's latency (speed of response) can result in improved performance (on average).

New users are advised to try the \fB-v\fR verbose option to see the current settings, the number of urls in the web sources list, etc., and (more importantly) to see a comprehensive representation of what the script is doing while it runs. This will help new users gain confidence in \fBgetip's\fR reliability. Running it repeatedly with different timeout values can help users fine tune the best timeout for them to use. e.g. \fB-v\fR \fB-t\fR \fB0.75\fR ... \fB-v\fR \fB-t\fR \fB3\fR. Once a value has been decided on an alias can be created which uses that timeout so it is always used, advanced users can change the value of the variable 'TIMEOUT_DEFAULT_GET_IP_MODE' in the script itself if they prefer.

The \fBgetip\fR script retrieves \fBIPv4\fR addresses only and not \fBIPv6\fR addresses.

All output is printed to \fBSTDOUT\fR except for error messages which are printed to \fBSTDERR\fR.

.SH REQUIREMENTS

\fBgetip\fR is a \fBBASH\fR script which requires either \fBwget\fR or \fBcurl\fR to be installed. Many modern \fBUNIX\fR like systems come with \fBwget\fR pre-installed. \fBwget\fR and \fBcurl\fR are both command line programs which can download web pages from the internet. \fBgetip\fR works very well with them both but, in lots of tests on \fBDebian\fR and \fBLinux\fR \fBMint\fR, \fBwget\fR was consistently just a little faster (approx. 200 milliseconds with the default options and using the same web sources). If both \fBwget\fR and \fBcurl\fR are installed \fBgetip\fR will use \fBwget\fR by default, if only one of them is installed then that one will be automatically used. See: \fB\-p\fR \fBdownload_program\fR

.SH OPTIONS
.BR
In typical usage no options are necessary.
.BR

.TP
\fB\-g\fR
Get IP mode (default); print the WAN IP address. This will retrieve the IP address from multiple web sources and check that they are all the same. The default number of sources to use for IP address verification is 2. This is the default mode so \fB\-g\fR is optional. See: \fB\-n\fR \fBnum_sources\fR

.TP
\fB\-c\fR
Check mode; print a table containing the url of each web source in the list, the IP address that it returned or a failure message, and the time taken to retrieve the IP address. Various statistics and, if necessary, warnings will be printed on completion. [This option is used during web sources list maintenance and is not needed by the average user.]

.TP
\fB\-u\fR \fBurl\fR
Test Url mode; tests an url for possible inclusion in the web sources list. It will print a table of helpful information, including the IP address and whether the url returned a single IP address (which is required for inclusion in the web sources list file), multiple IP addresses, or no IP addresses at all. The "http://" protocol part of the url is required. [This option is used during web sources list maintenance and is not needed by the average user.]

.TP
\fB\-n\fR \fBnum_web_sources\fR
Sets the Number of web sources to use for IP address verification in Get IP mode; the allowed range of values is >=2 and <=5. The default value is 2. See: \fBDESCRIPTION\fR

.TP
\fB\-t\fR \fBnum_seconds\fR
Sets the Timeout for each web source; when a web source gives no response or is slow to respond \fBgetip\fR moves on to the next source. \fB\-t\fR sets the timeout in seconds for how long to give each source to complete. Real numbers are permitted. e.g. \fB-t\fR \fB0.75\fR, \fB-t\fR \fB1.5\fR, \fB-t\fR \fB2\fR. The allowed range of seconds is >=0.2 and <=15 but these are extremes. See: \fBDESCRIPTION\fR

The default timeouts are dependant on mode, they are:
.br
Get IP mode    -  1.5 seconds
.br
Check mode     -  10 seconds
.br
Test Url mode  -  10 seconds

The Check and Test Url mode timeouts are intentionally high. They are intended to give servers a good chance to succeed, in case a server is slower than usual to respond. Any server which is consistently slow will be removed from the web sources list file.

.TP
\fB\-p\fR \fBdownload_program\fR \fB[wget | curl]\fR
Sets the Download program; \fBgetip\fR can use either \fBwget\fR or \fBcurl\fR to download each web source and the web sources list file. Use either: \fB-p\fR \fBwget\fR or \fB-p\fR \fBcurl\fR. \fBwget\fR is slightly quicker on tested systems and so is used by default if both programs are installed. Note that if only \fBcurl\fR is installed then that will be used by default. See: \fBREQUIREMENTS\fR

.TP
\fB\-s\fR \fBweb_sources_list\fR
Sets a custom the web Sources list file; \fBgetip\fR can use a custom web sources list file or a local copy. Either a file path or http url to the file may be used. e.g. \fB-s\fR \fB/path/to/list\fR or \fB-s\fR \fBhttp://url.to/list\fR. See: \fBDESCRIPTION\fR

.TP
\fB\-v\fR
Verbose; print detailed information while \fBgetip\fR runs.

.TP
\fB\-b\fR
Bare output; prints only the IP address on success or \fBFAILED\fR if the script fails to get the IP address from the required number of sources. Error messages will still be printed, redirect \fBSTDERR\fR to \fB/dev/null\fR if they are not wanted. This option exists primarily so that \fBgetip\fR can be used easily from within other scripts.

.TP
\fB\-h\fR
Help; print usage and help summary (cheat sheet).

.TP
\fB\-\-help\fR
Help; display the full manual in the form of a built-in man page.

.TP
\fB\-\-version\fR
Version; print the version number.

.SH EXIT STATUS
\fBgetip\fR returns 0 on success, and non-zero on failure.
.LP
\fB0\fR   - Success
.br
\fB101\fR - Command line argument invalid.
.br
\fB102\fR - Command line option invalid.
.br
\fB103\fR - Command line argument missing.
.br
\fB104\fR - Download program(s) not installed.
.br
\fB105\fR - Temp file creation failed.
.br
\fB106\fR - Web source list file is not a file or url.
.br
\fB107\fR - Reading the web source list file failed.
.br
\fB108\fR - Not enough urls in the web source list file.
.br
\fB109\fR - Failed to get the IP address.
.br
\fB110\fR - Test Url Mode: No IP address.
.br
\fB111\fR - Test Url Mode: Multiple IP addresses.
.br
\fB112\fR - Test Url Mode: Server timed out.

.SH BUGS
No known bugs. Please submit bug reports on the homepage.

.SH AUTHOR
<mattst@i-dig.info>

.SH HOMEPAGE
https://github.com/mattst/getip

.SH CONTRIBUTE
If you have a web server and would like to allow it to be used as a source in the web sources list then please see the homepage for details. It needs only a 2 line PHP program and would generate very little bandwidth - if lots of people did so it would be very helpful to the project.
.LP
Please feel free to submit new IP address providing web pages to be added to the web sources list. You should first check they are not already in the latest web source list file and that they are suitable for inclusion by using the \fB-u\fR \fBurl\fR option.

.SH COPYRIGHT
Copyright <mattst@i-dig.info>, GNU General Public License v.3 or later. GPLv3+ http://gnu.org/licenses/gpl.html
EOF

    # Display the man page and then delete it.

    man "$getip_man_page_temp_file"

    if [ -f "$getip_man_page_temp_file" ]; then rm "$getip_man_page_temp_file"; fi
}


# Function to output error messages to STDERR and then exit the script.
Exit_With_Error_Message()
{
    local error_num="$1"

    # This is used for those errors which have a configurable piece of information, e.g. an option.
    local additional_info="$2"

    # Output the appropriate error message to STDERR.

    if [ "$error_num" = "$EXIT_ERR_ARG_INVALID" ]; then
        printf "\n%s\n" "The argument given to the -$additional_info option is invalid." >&2

    elif [ "$error_num" = "$EXIT_ERR_ARG_MISSING" ]; then
        printf "\n%s\n" "The argument is missing from the -$additional_info option." >&2

    elif [ "$error_num" = "$EXIT_ERR_OPTION_INVALID" ]; then
        printf "\n%s\n" "An invalid option has been entered." >&2

    elif [ "$error_num" = "$EXIT_ERR_TEMP_FILE_CREATION_FAILED" ]; then
        printf "\n%s\n" "Unable to create a temp file using 'mktemp'." >&2

    elif [ "$error_num" = "$EXIT_ERR_WEB_SOURCE_LIST_INVALID" ]; then
        printf "\n%s\n" "The web sources list file is neither a local file nor a http url." >&2

    elif [ "$error_num" = "$EXIT_ERR_READ_WEB_SOURCE_LIST_FAILED" ]; then
        printf "\n%s\n" "Unable to read the web sources list file located at:" >&2
        printf "%s\n" "$web_sources_list_location_display_text" >&2

    elif [ "$error_num" = "$EXIT_ERR_NOT_ENOUGH_SOURCE_URLS" ]; then
        printf "\n%s\n" "Not enough web sources in the web sources list file." >&2
        printf "%s\n" "The minimum number of urls that are required is $WEB_SOURCES_MIN." >&2
        printf "\n%s\n" "[Note: HTTP redirections can cause this error to" >&2
        printf "%s\n" "be displayed when the domain/url does not exist.]" >&2

    elif [ "$error_num" = "$EXIT_ERR_FAILED_TO_GET_IP_ADDRESS" ]; then
        printf "\n%s\n" "getip failed to get the IP address." >&2

    elif [ "$error_num" = "$EXIT_ERR_DOWNLOAD_PROGRAM_NOT_INSTALLED" ]; then
        if [ "$additional_info" = "wget" ]; then
            printf "\n%s\n" "wget is not installed on your system." >&2
        elif [ "$additional_info" = "curl" ]; then
            printf "\n%s\n" "curl is not installed on your system." >&2
        elif [ "$additional_info" = "wget+curl" ]; then
            printf "\n%s\n" "Neither wget nor curl are installed on your system." >&2
        fi
    fi

    printf "\n%s\n" "-h      :  brief help (cheat sheet)." >&2
    printf "%s\n"   "--help  :  full manual (a man page)." >&2

    exit "$error_num"
}


# Function to read the web sources list file and store the urls in the $web_sources array.
Read_Source_Url_List()
{
    # Constants to specify whether the web sources list is an url or local file.
    local WEB_SOURCES_TYPE_URL="1"
    local WEB_SOURCES_TYPE_FILE="2"

    # Variables to hold the type (url or local file) and address/location.
    local web_sources_type
    local web_sources_list_url
    local web_sources_list_file

    # Use the default url if the user has not set the web sources list.
    if [ "$web_sources_list_user_set" = "FALSE" ]; then

        web_sources_type="$WEB_SOURCES_TYPE_URL"
        web_sources_list_url="$WEB_SOURCES_LIST_DEFAULT"
        web_sources_list_location_display_text="$WEB_SOURCES_LIST_DEFAULT"

    # If the user has set the web sources list then check if $web_sources_list_user_set is a local
    # file, if it is not then it must be an url.
    else
        if [ -f "$web_sources_list_user_set" ]; then
            web_sources_type="$WEB_SOURCES_TYPE_FILE"
            web_sources_list_file="$web_sources_list_user_set"
            web_sources_list_location_display_text="$web_sources_list_user_set"
        else
            web_sources_type="$WEB_SOURCES_TYPE_URL"
            web_sources_list_url="$web_sources_list_user_set"
            web_sources_list_location_display_text="$web_sources_list_user_set"
        fi
    fi

    # If the web sources list is an url, download it into a temp file.
    if [ "$web_sources_type" = "$WEB_SOURCES_TYPE_URL" ]; then

        local web_sources_temp_file
        web_sources_temp_file=$(mktemp -q -t "getip.tmp.XXXXXX")
        local make_temp_file_ret_val=$?

        if [ "$make_temp_file_ret_val" -ne "0" ]; then
            Exit_With_Error_Message "$EXIT_ERR_TEMP_FILE_CREATION_FAILED"
        fi

        # Use wget or curl to download the web sources list.

        local download_web_sources_list_ret_val

        if [ "$download_program" = "$DOWNLOAD_PROGRAM_WGET" ]; then

            wget --quiet --tries=1 --user-agent="$USER_AGENT" --output-document=- \
                                        "$web_sources_list_url" > "$web_sources_temp_file"
            download_web_sources_list_ret_val=$?

        elif [ "$download_program" = "$DOWNLOAD_PROGRAM_CURL" ]; then

            curl --silent --fail --user-agent "$USER_AGENT" "$web_sources_list_url" \
                                                                > "$web_sources_temp_file"
            download_web_sources_list_ret_val=$?
        fi

        if [ "$download_web_sources_list_ret_val" -ne "0" ]; then
            Exit_With_Error_Message "$EXIT_ERR_READ_WEB_SOURCE_LIST_FAILED"
        fi

        # Set the array building loop filename to the temp filen.
        web_sources_list_file="$web_sources_temp_file"
    fi

    # Read the urls from $web_sources_list_file into the $web_sources array.

    # The $web_sources array must be INDEXED IN NUMERICAL SEQUENCE FROM 0.
    local index="0"

    while read -r line ; do

        # Store only the http url lines, ignore anything else.

        if [[ "$line" =~ $REGEX_HTTP_URL ]]; then
            web_sources[$index]="$line"
            let 'index = index + 1'
        fi

    done < "$web_sources_list_file"

    # Set the length of the web_sources array.
    web_sources_len="$index"

    # If necessary delete the temp file.
    if [ "$web_sources_type" = "$WEB_SOURCES_TYPE_URL" ]; then
        if [ -f "$web_sources_temp_file" ]; then rm "$web_sources_temp_file"; fi
    fi
}


# Function that randomizes the $web_sources array. Since this script is being publicly released it
# is sensible to randomize the sources so as not to place undue pressure on the servers at the top
# of the list. This is an implementation of the Knuth shuffle algorithm (aka Fisher-Yates shuffle),
# which is a very efficient randomizer, needing just one pass through an array.
# IMPORTANT: THE $web_sources ARRAY MUST BE INDEXED FROM 0 FOR THIS FUNCTION.
Randomize_Url_List()
{
    # Bash $RANDOM provides a random number in the range 0..32767 so 32768 possible values.
    local rand_max="32768"

    # Start with the number of items in the array.
    local index="$web_sources_len"

    # Loop down through the array randomly swapping the current element with another randomly
    # chosen element (possibly itself). The use of >1 needs a moment to understand... if instead
    # it was >0 then the final iteration would always swap index 0 with itself.

    while [ "$index" -gt "1" ]; do

        # Modulo bias avoidance as per Knuth shuffle algorithm description. Calculate the highest
        # usable random number so that the modulo by index calculation will not result in any modulo
        # bias. Note: This is not really necessary but the function only takes ~0.006 seconds with
        # 120 urls (on my desktop PC) so may as well do it right.

        # Integer arithmetic (the division will be rounded down).
        local rand_num_highest_usable
        let "rand_num_highest_usable = (($rand_max / $index) * $index) - 1"

        # Get a random number within the modulo bias avoidance range.
        local rand_num="$RANDOM"

        while [ "$rand_num" -gt "$rand_num_highest_usable" ]; do
            rand_num="$RANDOM"
        done

        # Get a random index in the range 0..index-1.
        local rand_index
        let "rand_index = rand_num % index"

        # Decrement index to reference the current 'last' element in the array. For the first loop
        # iteration index will be $web_sources_len - 1 (the final element of the array) and for the
        # last loop iteration index will be 1. By doing so the current index will be swapped with
        # a randomly chosen index ($rand_index) in the range 0..index, so elements will sometimes
        # be 'swapped' with themselves (if elements could not stay put it wouldn't be random).

        let "index = index - 1"
        local swap_val="${web_sources[$index]}"
        web_sources[$index]="${web_sources[$rand_index]}"
        web_sources[$rand_index]="$swap_val"

    done
}


# Function to extract valid IP addresses from the file given in the function arg.
Extract_IP_Addresses_From_File()
{
    local web_page_source_file="$1"

    # The following Awk program will search for and print all valid IP addresses in the file which
    # is given to it as input. It looks for and removes IP-like number sequences which it identifies
    # as version numbers or numerical sequences in urls, and all numbers longer than 3 digits.

    local awk_extract_ip_addresses='
    BEGIN {
        # Regex to match an IP address like sequence (even if too long to be an IP). This is
        # deliberately a loose match, the END section will check for IP address validity.
        ip_like_sequence = "[0-9]+[.][0-9]+[.][0-9]+[.][0-9]+[0-9.]*";

        # Regex to match number sequences longer than 3 digits.
        digit_sequence_too_long_not_ip = "[0-9][0-9][0-9][0-9]+";

        # Regex to match an IP address like sequence which is part of a version number.
        # Equivalent to "(version|ver|v)[ .:]*" in conjunction with: line = tolower($0);
        versioning_not_ip = "[Vv]([Ee][Rr]([Ss][Ii][Oo][Nn])?)?[ .:]*" ip_like_sequence;

        # Regexes to match IP address like sequences next to forward slashes, to avoid
        # numbers in urls: e.g. http://web.com/libs/1.2.3.4/file.js
        begins_with_fwd_slash_not_ip = "[/]" ip_like_sequence;
        ends_with_fwd_slash_not_ip = ip_like_sequence "[/]";
    }
    {
        # Set line to the current line (more efficient than using $0 below).
        line = $0;

        # Replace sequences on line which will interfere with extracting genuine IPs. Use a
        # replacement char and not the empty string to avoid accidentally creating a valid IP
        # address from digits on either side of the removed sections. Use "/" as the replacement
        # char for the 2 "fwd_slash" regexes so that multiple number dot slash sequences all get
        # removed, as using "x" could result in inadvertently leaving such a sequence in place.
        # e.g. "/lib1.2.3.4/5.6.7.8/9.10.11.12/file.js" would leave "/lib1.2.3.4xx/file.js".

        gsub(digit_sequence_too_long_not_ip, "x", line);
        gsub(versioning_not_ip, "x", line);
        gsub(begins_with_fwd_slash_not_ip, "/", line);
        gsub(ends_with_fwd_slash_not_ip, "/", line);

        # Loop through the current line matching IP address like sequences and storing them in
        # the INDEX of the array ip_unique_matches. By using ip_match as the array index duplicates
        # are avoided and the values can be easily retrieved by the for loop in the END section.
        # match() automatically sets the built in variables RSTART and RLENGTH.

        while (match(line, ip_like_sequence))
        {
            ip_match = substr(line, RSTART, RLENGTH);
            ip_unique_matches[ip_match];
            line = substr(line, RSTART + RLENGTH + 1);
        }
    }
    END {
        # Define some IP address related constants.
        ip_range_min = 0;
        ip_range_max = 255;
        ip_num_segments = 4;
        ip_delimiter = ".";

        # Loop through the ip_unique_matches array and print any valid IP addresses. The awk "for
        # each" type of loop is different from the norm. It provides the indexes of the array
        # and NOT the values of the array elements which is more usual in this type of loop.

        for (ip_match in ip_unique_matches)
        {
            num_segments = split(ip_match, ip_segments, ip_delimiter);
            if (num_segments == ip_num_segments &&
                ip_segments[1] >= ip_range_min && ip_segments[1] <= ip_range_max &&
                ip_segments[2] >= ip_range_min && ip_segments[2] <= ip_range_max &&
                ip_segments[3] >= ip_range_min && ip_segments[3] <= ip_range_max &&
                ip_segments[4] >= ip_range_min && ip_segments[4] <= ip_range_max)
            {
                print ip_match;
            }
        }
    }'

    # Extract valid IP addresses, each will be separated by a new line.
    local valid_ip_addresses=$(awk "$awk_extract_ip_addresses" < "$web_page_source_file")

    # Echo for capture in a variable at the point of function invocation.
    echo "$valid_ip_addresses"
}


# Function that retrieves the web source's content and returns all unique IP addresses in it. The
# function provides an accurate timeout facility; neither wget nor curl provide an accurate timeout
# in the fractions of a second. Since getip uses so many web sources a low timeout is used, if one
# source fails to return quickly then the next source is tried. With this methodology a reliable and
# accurate timeout is required, this function provides it.
Get_IP_Address_From_Web_Source()
{
    # The url to download and search for IP addresses.
    local web_source_url="$1"

    # Create a temp file to store the page source of $web_source_url.
    local web_source_temp_file
    web_source_temp_file=$(mktemp -q -t "getip.tmp.XXXXXX")
    local make_temp_file_ret_val=$?

    # Temp file creation errors must be handled differently here. The function is called using the
    # $() construct which means a subshell will be created for it and any exit call inside the
    # function will exit only the subshell and not the script. To get around this the return value
    # of Get_IP_Address_From_Web_Source() is checked to handle exiting the script if mktemp failed.
    if [ "$make_temp_file_ret_val" -ne "0" ]; then
        return "$GET_IP_TEMP_FILE_CREATION_FAILED"
    fi

    # Download the web page's source and store its contents in the temp file. The process is forked
    # (the trailing '&') to facilitate the timeout procedure.

    # Note: %s seconds, %N nanoseconds.
    local timeout_start=$(date +%s.%N)

    # Download the web page's source placing it in the temp file and store the process's id.

    # Use a fail safe timeout. Occasionally wget and curl will not terminate in a timely manner
    # after receiving the SIGTERM request from kill. I have been unable to establish exactly why.
    # The default timeout for wget is 15 mins, for curl it is 5 mins, these are too high for this
    # script, so a fail safe timeout has been added to make sure the download program's processes
    # do not persist for long when the download programs do not terminate quickly after the kill
    # command. Note that this is just a precaution for relatively rare circumstances.

    # wget's --timeout option and curl's --max-time option both take a timeout in seconds.
    local timeout_fail_safe="30"

    local download_process_id

    if [ "$download_program" = "$DOWNLOAD_PROGRAM_WGET" ]; then

        wget --quiet --tries=1 --timeout="$timeout_fail_safe" --user-agent="$USER_AGENT" \
                                --output-document=- "$web_source_url" > "$web_source_temp_file" &
        download_process_id=$!

    elif [ "$download_program" = "$DOWNLOAD_PROGRAM_CURL" ]; then

        curl --silent --max-time "$timeout_fail_safe" --user-agent "$USER_AGENT" \
                                                    "$web_source_url" > "$web_source_temp_file" &
        download_process_id=$!
    fi

    # Even if kill has both stdout and stderr redirected to /dev/null the death of the process is
    # still displayed. Using disown to remove the job from the shell's active jobs table stops this
    # from happening so that there are no unsightly terminal messages.
    disown "$download_process_id"

    # Handle the timeout facility for the web page source download.

    local is_download_done="FALSE"
    local is_download_timed_out="FALSE"

    while [ "$is_download_done" = "FALSE" -a "$is_download_timed_out" = "FALSE" ]; do

        # Store whether the wget/curl download process is still listed by ps.
        ps -A | grep --quiet "$download_process_id"
        local process_grep_ret_val=$?

        # If the url download process has finished (no grep match for process id).
        if [ "$process_grep_ret_val" -ne "0" ]; then
            is_download_done="TRUE"

        # If the url download process has not finished.
        else
            # Note: %s seconds, %N nanoseconds.
            local timeout_now=$(date +%s.%N)

            # Use bc to check if the timeout has been exceeded.
            local bc_exp_timeout="if (($timeout_now - $timeout_start) > $timeout) 1 else 0"
            local bc_exp_timeout_ret_val=$(echo "$bc_exp_timeout" | bc -l)

            # The timeout has been exceeded.
            if [ "$bc_exp_timeout_ret_val" -eq "1" ]; then
                is_download_timed_out="TRUE"
            fi
        fi
    done

    # If the download process completed successfully.
    if [ "$is_download_done" = "TRUE" ]; then

        # Extract IP addresses from the temp file. Each IP address will be separated by a new line
        # and if the source is working as expected then there will be exactly one IP address.
        local ip_address_list=$(Extract_IP_Addresses_From_File "$web_source_temp_file")

        # Delete the temp file.
        if [ -f "$web_source_temp_file" ]; then rm "$web_source_temp_file"; fi

        # Iterate through $ip_address_list counting the number of IP addresses.
        local ip_address_list_len="0"
        local ip

        for ip in $(echo "$ip_address_list" | tr "\n" " "); do
            let "ip_address_list_len = ip_address_list_len + 1"
        done

        # Success: if exactly 1, then that is considered to be the user's IP address, it is echoed
        # so that it can be placed in a variable at the point of function invocation.
        if [ "$ip_address_list_len" -eq "1" ]; then
            echo "$ip_address_list"
            return "$GET_IP_SINGLE_VALID_IP"

        # Failure: if less than 1, then no valid IP addresses were found.
        elif [ "$ip_address_list_len" -lt "1" ]; then
            return "$GET_IP_NO_VALID_IP"

        # Failure: if more than 1 there is no way to tell which of them is the user's IP address.
        # Echo the list so that the IP addresses can be displayed if being verbose.
        elif [ "$ip_address_list_len" -gt "1" ]; then
            echo "$ip_address_list"
            return "$GET_IP_MULTIPLE_VALID_IP"
        fi
    fi

    # If the url download process timed out, which it did or this section would not be reached,
    # kill the process using SIGTERM which will allow the program to clean up after itself.
    # DO NOT USE SIGKILL; wget/curl may leave sockets open, temp files, child processes etc.

    # Since the process being killed has been disowned (removed from the shell's active jobs table)
    # the redirect of stdout and stderr to /dev/null may be unnecessary to hide the kill messages,
    # it is not necessary on the Linux systems used to test the script, other OSes?

    if [ "$is_download_timed_out" = "TRUE" ]; then
        kill -SIGTERM "$download_process_id" > /dev/null 2>&1
    fi

    # Delete the temp file.
    if [ -f "$web_source_temp_file" ]; then rm "$web_source_temp_file"; fi

    # Failure: the download timed out.
    return "$GET_IP_SERVER_TIMED_OUT"
}


# Function to check that all the IP addresses passed to it are exactly the same.
Verify_IP_Address_List()
{
    # Assign the IP addresses list passed to this function to an array.
    local ip_addresses=("$@")

    # Iterate through the array checking if all the IP addresses are the same.

    local ip
    local ip_first_element="${ip_addresses[0]}"

    for ip in "${ip_addresses[@]}"; do

        # If the current ip address is not the same as the first one.
        if [ "$ip" != "$ip_first_element" ]; then
            return "$VERIFY_IP_ADDRESS_LIST_FAILURE"
        fi
    done

    # All the IP addresses must be the same.
    return "$VERIFY_IP_ADDRESS_LIST_SUCCESS"
}


# Function that adds information to the statistics variables (check mode only).
Add_To_Stats()
{
    local status="$1"
    local ip_address_for_stats="$2"
    local ip_retrieval_time_for_stats="$3"

    # The IP address is valid.
    if [ "$status" = "$GET_IP_SINGLE_VALID_IP" ]; then

        # Add the IP address and the time taken to the stats lists.
        # Keep the trailing spaces they are serving as delimiters.

        stats_ip_address_list+="$ip_address_for_stats "
        stats_ip_retrieval_times_list+="$ip_retrieval_time_for_stats "
        let "stats_ip_valid_total = stats_ip_valid_total + 1"

    # The IP address is not valid.
    else
        let "stats_ip_invalid_total = stats_ip_invalid_total + 1"
    fi
}


# Function to find the fastest time in $stats_ip_retrieval_times_list (check mode only).
Find_Fastest_Time()
{
    # Use awk to find the fastest time.

    local awk_fastest='{ if (NR == 1) fastest = $1; if ($1 < fastest) fastest = $1; } \
                   END { printf("%.3f", fastest) }'

    local fastest=$(echo "$stats_ip_retrieval_times_list" | tr " " "\n" | awk "$awk_fastest" )

    echo "$fastest"
}


# Function to find the slowest time in $stats_ip_retrieval_times_list (check mode only).
Find_Slowest_Time()
{
    # Use awk to find the slowest time.

    local awk_slowest='{ if (NR == 1) slowest = $1; if ($1 > slowest) slowest = $1; } \
                   END { printf("%.3f", slowest) }'

    local slowest=$(echo "$stats_ip_retrieval_times_list" | tr " " "\n" | awk "$awk_slowest" )

    echo "$slowest"
}


# Function to calculate the mean of the $stats_ip_retrieval_times_list (check mode only).
Find_Mean_Average_Time()
{
    # Use awk to find the mean average.

    local awk_mean='{ total += $1; num += 1; } \
                END { mean = total / num; printf("%.3f", mean) }'

    local mean=$(echo "$stats_ip_retrieval_times_list" | tr " " "\n" | awk "$awk_mean")

    echo "$mean"
}


# Function to calculate the median of the $stats_ip_retrieval_times_list (check mode only).
Find_Median_Average_Time()
{
    # Use awk to find the median average.

    # Establish if there are an even or odd number of items.
    local num_items_parity
    let 'num_items_parity = stats_ip_valid_total % 2'

    # If an even number of items.
    if [ "$num_items_parity" -eq "0" ]; then

        # Work out which are the 2 middle items.
        local mid_item_1
        local mid_item_2
        let "mid_item_1 = stats_ip_valid_total / 2"
        let "mid_item_2 = mid_item_1 + 1"

        # Awk program to print the median value (the mean of the 2 middle rows).
        local awk_median='{ if (NR == mid_row_1) med_1 = $1; if (NR == mid_row_2) med_2 = $1; } \
                      END { median = (med_1 + med_2) / 2; printf("%.3f", median) }'

        local median=$(echo "$stats_ip_retrieval_times_list" | tr " " "\n" | sort -n |
                       awk "$awk_median" mid_row_1="$mid_item_1" mid_row_2="$mid_item_2")

    # If an odd number of items.
    else

        # Work out which is the middle item (integer arithmetic, division will be rounded down).
        local mid_item
        let "mid_item = (stats_ip_valid_total / 2) + 1"

        # Awk program to print the median value (the middle row).
        local awk_median='{ if (NR == mid_row) printf("%.3f", $1) }'

        local median=$(echo "$stats_ip_retrieval_times_list" | tr " " "\n" | sort -n |
                       awk "$awk_median" mid_row="$mid_item")
    fi

    echo "$median"
}


# Function to display the statistical information (check mode only).
Display_Stats()
{
    # Trim whitespace from $stats_ip_retrieval_times_list and $stats_ip_address_list.
    local sed_exp_trim='s/^[ \t]*//g;s/[ \t]*$//g'
    stats_ip_retrieval_times_list=$(echo "$stats_ip_retrieval_times_list" | sed "$sed_exp_trim")
    stats_ip_address_list=$(echo "$stats_ip_address_list" | sed "$sed_exp_trim")

    # Find the fastest, slowest, mean, and median of the successful retrieval times.
    local fastest_time=$(Find_Fastest_Time)
    local slowest_time=$(Find_Slowest_Time)
    local mean_time=$(Find_Mean_Average_Time)
    local median_time=$(Find_Median_Average_Time)

    # Calculate the percentage of successful and failed retrievals.
    local bc_exp_success_percent="($stats_ip_valid_total / $web_sources_len) * 100"
    local success_percent=$(printf "%.2f" $(echo "$bc_exp_success_percent" | bc -l))
    local bc_exp_failed_percent="100 - $success_percent"
    local failed_percent=$(printf "%.2f" $(echo "$bc_exp_failed_percent" | bc -l))

    # Calculate the number of unique IP addresses held in $stats_ip_address_list.
    # Important: Use printf to avoid the new line that echo would add.
    local ip_num_unique=$(printf "%s" "$stats_ip_address_list" | tr " " "\n" | \
                                                            sort -n | uniq | wc -l)

    # Output the statistics.

    printf "%s\n" "------------------------------------------------------------------------------"

    printf "%-62s %s\n" "Web Sources Checked Total:" "$web_sources_len"
    printf "%-62s %-3s (%s%%)\n" "IP Addresses - Successfully Retrieved:" \
                                 "$stats_ip_valid_total" "$success_percent"
    printf "%-62s %-3s (%s%%)\n" "IP Addresses - Failed (Timed Out / Invalid):" \
                                 "$stats_ip_invalid_total" "$failed_percent"
    printf "%-62s %s\n" "Fastest (Successfully Retrieved):" "$fastest_time secs"
    printf "%-62s %s\n" "Slowest (Successfully Retrieved):" "$slowest_time secs"
    printf "%-62s %s\n" "Mean Average   (Successfully Retrieved):" "$mean_time secs"
    printf "%-62s %s\n" "Median Average (Successfully Retrieved):" "$median_time secs"

    # The number of unique IP addresses should be 1, if not issue a warning.
    if [ "$ip_num_unique" = "1" ]; then
        printf "%-62s %s\n" "Unique IP Addresses (Should Be 1):" "$ip_num_unique"
    else
        printf "%-62s %s *WARNING*\n" "Unique IP Addresses (Should Be 1):" "$ip_num_unique"
    fi

    printf "%s\n" "------------------------------------------------------------------------------"

    # Display a warning if there were no valid IP addresses.
    if [ "$stats_ip_valid_total" = "0" ]; then
        local msg1="WARNING: The web sources did not supply any valid IP addresses at all. The"
        local msg2="most likely cause is that the internet connection has been lost, the timeout"
        local msg3="was set far too low, or possibly the web sources list file used is extremely"
        local msg4="out-of-date or even corrupted."
        printf "\n%s\n%s\n%s\n%s\n\n" "$msg1" "$msg2" "$msg3" "$msg4"
    fi

    # Display a warning if the number of unique IP addresses is >1.
    if [ "$ip_num_unique" -gt "1" ]; then
        local msg1="WARNING: The number of unique IP addresses retrieved exceeds one. It should be"
        local msg2="exactly one, your IP address. One or more of the sources is no longer suitable"
        local msg3="for use by this script. If you have the time please contact the developer with"
        local msg4="the url of the problematic source, use the --help option for contact details."
        printf "\n%s\n%s\n%s\n%s\n\n" "$msg1" "$msg2" "$msg3" "$msg4"

        # List each unique IP address in a table along with its number of occurrences.

        # This awk code uses a useful aspect of awk where any number or string in awk may be used
        # as an array index. In this case the IP addresses piped to awk will serve as the array
        # indexes for the count_ip[] array; same as used in Extract_IP_Addresses_From_File().
        local awk_ip_count='{ count_ip[$1]++ } END { for (ip in count_ip) \
                              printf("%-25s %d\n", ip, count_ip[ip]); }'

        printf "%s\n\n" "Unique IP Addresses List:"
        printf "%s\n" "IP Address         Num Occurrences"
        printf "%s\n" "----------------------------------"
        echo "$stats_ip_address_list" | tr " " "\n" | awk "$awk_ip_count" | sort -n -k 2,2
        printf "%s\n" "----------------------------------"
    fi
}


# Function which controls IP address retrieval and verification.
Handle_Get_IP_Mode()
{
    if [ "$verbose" = "TRUE" ]; then
        printf "\n%-25s %s\n" "Script Version:" "$VERSION"
        printf "%-25s %s\n" "Operation Mode:" "Get IP Address"
        printf "%-25s %s\n" "Num Web Sources:" "$web_sources_verify_num (used for verification)"
        printf "%-25s %s\n" "Timeout (Seconds):" "$timeout"
        printf "%-25s %s\n" "Download Program:" "$download_program_display_text"
    fi

    # Read the web sources list into $web_sources.
    Read_Source_Url_List

    if [ "$verbose" = "TRUE" ]; then
        printf "%-25s %s\n" "Web Sources Location:" "$web_sources_list_location_display_text"
        printf "%-25s %s\n" "Load Web Sources:" "Succeeded ($web_sources_len urls)"
    fi

    # Check that the number of source urls is adequate.
    if [ "$web_sources_len" -lt "$WEB_SOURCES_MIN" ]; then
        Exit_With_Error_Message "$EXIT_ERR_NOT_ENOUGH_SOURCE_URLS"
    fi

    # Shuffle the web sources array.
    Randomize_Url_List

    if [ "$verbose" = "TRUE" ]; then
        printf "%-25s %s\n" "Randomize Web Sources:" "Done"
    fi

    # Array to hold the IP addresses retrieved.
    local ip_address_list=()

    # Counter to hold how many IP addresses have so far been retrieved.
    local ip_counter="0"

    # Test error handling with 'fake' IP address.
    # 1 = has 1 IP address, 2 = has 3 IP addresses, 3 = no IP addresses, 4/5 = same IP.
    # web_sources[1]="https://crius.feralhosting.com/gencon/fakeip1.html"
    # web_sources[1]="https://crius.feralhosting.com/gencon/fakeip2.html"
    # web_sources[1]="https://crius.feralhosting.com/gencon/fakeip3.html"
    # web_sources[3]="https://crius.feralhosting.com/gencon/fakeip4.html"
    # web_sources[1]="https://crius.feralhosting.com/gencon/fakeip5.html"
    # web_sources[1]="https://crius.feralhosting.com/gencon/fakeip10.html"

    # Loop through the urls in $web_sources. The script will exit from within this loop if the IP
    # address has been successfully retrieved and verified by the required number of sources.
    local url

    for url in "${web_sources[@]}"; do

        if [ "$verbose" = "TRUE" ]; then
            local time_start=$(date +%s.%N);
            printf "\n%-25s %s\n" "Trying Web Source:" "$url"
        fi

        # Store any IP addresses in the page source of $url.
        local ip_address
        ip_address=$(Get_IP_Address_From_Web_Source "$url")
        local get_ip_ret_val=$?

        # Failed due to a temp file creation error.
        if [ "$get_ip_ret_val" -eq "$GET_IP_TEMP_FILE_CREATION_FAILED" ]; then
            Exit_With_Error_Message "$EXIT_ERR_TEMP_FILE_CREATION_FAILED"
        fi

        if [ "$verbose" = "TRUE" ]; then

            local time_stop=$(date +%s.%N)
            local time_taken=$(printf "%.3F" $(echo "$time_stop - $time_start" | bc -l))

            if [ "$get_ip_ret_val" -eq "$GET_IP_SINGLE_VALID_IP" ]; then
                printf "%-25s %s\n" "Status:" "Success: A single valid IP address"
                printf "%-25s %s\n" "IP Address:" "$ip_address"

            elif [ "$get_ip_ret_val" -eq "$GET_IP_NO_VALID_IP" ]; then
                printf "%-25s %s\n" "Status:" "Failure: No IP address"

            elif [ "$get_ip_ret_val" -eq "$GET_IP_MULTIPLE_VALID_IP" ]; then
                printf "%-25s %s\n" "Status:" "Failure: More than one IP address"
                # Create neat text for displaying the list of IP addresses.
                local ip_list_csv=$(echo "$ip_address" | tr '\n' ',' | sed "s/,/, /g ; s/, $//g")
                printf "%-25s %s\n" "IP Addresses:" "$ip_list_csv"

            elif [ "$get_ip_ret_val" -eq "$GET_IP_SERVER_TIMED_OUT" ]; then
                printf "%-25s %s\n" "Status:" "Failure: Server timed out"
            fi

            printf "%-25s %s\n" "Duration (Seconds):" "$time_taken"
        fi

        # If a single valid IP addresses was retrieved.
        if [ "$get_ip_ret_val" -eq "$GET_IP_SINGLE_VALID_IP" ]; then

            # Add the IP address to the array, and increment the counter.
            ip_address_list[$ip_counter]="$ip_address"
            let "ip_counter = ip_counter + 1"

            if [ "$verbose" = "TRUE" ]; then
                printf "%-25s %s\n" "Add IP To List:" "$ip_counter of $web_sources_verify_num"
            fi

            # Check if all the IP addresses in the array are the same.
            Verify_IP_Address_List "${ip_address_list[@]}"
            local verify_ip_list_ret_val=$?

            # If the IP addresses in the array are all the same.
            if [ "$verify_ip_list_ret_val" = "$VERIFY_IP_ADDRESS_LIST_SUCCESS" ]; then

                # If the array now contains the required number of IP addresses to use for
                # verification then output the IP address and exit.

                if [ "$ip_counter" -eq "$web_sources_verify_num" ]; then

                    if [ "$verbose" = "TRUE" ]; then
                        printf "\n%-25s %s" "IP List Verification:" "Succeeded "
                        printf "%s\n" "[$ip_counter identical IP addresses]"
                        printf "\n%-25s %s\n" "IPv4 address:" "$ip_address"

                    elif [ "$bare_output" = "TRUE" ]; then
                        printf "%s\n" "$ip_address"

                    else
                        printf "%s\n" "IPv4 address: $ip_address"
                        printf "%s\n" "[Verified using $web_sources_verify_num web sources.]"
                    fi

                    # The successful exit point of the script in getip mode.
                    exit "$EXIT_SUCCESS"
                fi

            # If the IP addresses in the array are NOT all the same then re-start.
            elif [ "$verify_ip_list_ret_val" = "$VERIFY_IP_ADDRESS_LIST_FAILURE" ]; then

                if [ "$verbose" = "TRUE" ]; then
                    printf "\n%-25s %s\n" "IP List Verification:" "Failed [IP addresses differed]"
                    printf "%-25s %s\n" "Status:" "Restart using different sources"
                fi

                # Reset the array and counter variables.
                ip_address_list=()
                ip_counter="0"
            fi
        fi

    done  # End of loop through $web_sources loop.

    # The exit point of the script in getip mode if the script failed to supply a valid IP address
    # from the required number of sources.

    if [ "$bare_output" = "TRUE" ]; then
        printf "%s\n" "FAILED"
        exit "$EXIT_ERR_FAILED_TO_GET_IP_ADDRESS"
    fi

    Exit_With_Error_Message "$EXIT_ERR_FAILED_TO_GET_IP_ADDRESS"
}


# Function which controls testing a web page to see if suitable to add to the web sources list.
Handle_Test_Url_Mode()
{
    printf "\n%-22s %s\n" "Script Version:" "$VERSION"
    printf "%-22s %s\n" "Operation Mode:" "Test Url"
    printf "%-22s %s\n" "Test Url:" "$test_url_address"
    printf "%-22s %s\n" "Download Program:" "$download_program_display_text"
    printf "%-22s %s\n" "Timeout  (Seconds):" "$timeout"

    local time_start=$(date +%s.%N)

    # Store any IP addresses in the page source of $test_url_address.
    local ip_address
    ip_address=$(Get_IP_Address_From_Web_Source "$test_url_address")
    local get_ip_ret_val=$?

    # Failed due to temp file creation error.
    if [ "$get_ip_ret_val" -eq "$GET_IP_TEMP_FILE_CREATION_FAILED" ]; then
        Exit_With_Error_Message "$EXIT_ERR_TEMP_FILE_CREATION_FAILED"
    fi

    local time_stop=$(date +%s.%N)
    local time_taken=$(printf "%.3F" $(echo "$time_stop - $time_start" | bc -l))

    # Print the test results.

    printf "%-22s %s\n" "Duration (Seconds):" "$time_taken"

    if [ "$get_ip_ret_val" -eq "$GET_IP_SINGLE_VALID_IP" ]; then
        printf "%-22s %s\n" "Status:" "Success: A single valid IP address"
        printf "%-22s %s\n" "IP Address:" "$ip_address"
        printf "%-22s %s\n" "Conclusion:" "Url is suitable for using as a source."
        printf "%-22s %s\n" "" "[As long as the IP address is correct.]"
        exit "$EXIT_SUCCESS"

    elif [ "$get_ip_ret_val" -eq "$GET_IP_MULTIPLE_VALID_IP" ]; then
        printf "%-22s %s\n" "Status:" "Failure: More than one IP address"
        # Create neat text for displaying the list of IP addresses.
        local ip_list_csv=$(echo "$ip_address" | tr '\n' ',' | sed "s/,/, /g ; s/, $//g")
        printf "%-22s %s\n" "IP Addresses:" "$ip_list_csv"
        printf "%-22s %s\n" "Conclusion:" "Url is NOT suitable for using as a source."
        exit "$EXIT_ERR_TEST_URL_MULTIPLE_IP_ADDRESS"

    elif [ "$get_ip_ret_val" -eq "$GET_IP_NO_VALID_IP" ]; then
        printf "%-22s %s\n" "Status:" "Failure: No IP address"
        printf "%-22s %s\n" "Conclusion:" "Url is NOT suitable for using as a source."
        exit "$EXIT_ERR_TEST_URL_NO_IP_ADDRESS"

    elif [ "$get_ip_ret_val" -eq "$GET_IP_SERVER_TIMED_OUT" ]; then
        printf "%-22s %s\n" "Status:" "Failure: Server timed out"
        printf "%-22s %s\n" "Conclusion:" "Url is NOT suitable for using as a source unless the"
        printf "%-22s %s\n" "" "timeout was set very low or its timing out was unusual."
        exit "$EXIT_ERR_TEST_URL_TIMED_OUT"
    fi
}


# Function which controls checking if the web pages in the source list are working or not.
Handle_Check_Mode()
{
    printf "\n%-24s %s\n" "Script Version:" "$VERSION"
    printf "%-24s %s\n" "Operation Mode:" "Check Source Urls"
    printf "%-24s %s\n" "Timeout (Seconds):" "$timeout"
    printf "%-24s %s\n" "Download Program:" "$download_program_display_text"

    # Read the web sources list into $web_sources.
    Read_Source_Url_List

    printf "%-24s %s\n" "Web Sources Location:" "$web_sources_list_location_display_text"
    printf "%-24s %s\n\n" "Load Web Sources:" "Succeeded ($web_sources_len Urls)"

    # Table header.
    printf "%s\n" "------------------------------------------------------------------------------"
    printf "%-23s %-40s %s\n" "Seconds" "Url" "IP/Status"
    printf "%s\n" "------------------------------------------------------------------------------"

    # Loop through the urls in $web_sources providing information about the status of each one.
    local url

    for url in "${web_sources[@]}"; do

        local time_start=$(date +%s.%N);

        # Store any IP addresses in the page source of $url.
        local ip_address
        ip_address=$(Get_IP_Address_From_Web_Source "$url")
        local get_ip_ret_val=$?

        # Failed due to temp file creation error.
        if [ "$get_ip_ret_val" -eq "$GET_IP_TEMP_FILE_CREATION_FAILED" ]; then
            Exit_With_Error_Message "$EXIT_ERR_TEMP_FILE_CREATION_FAILED"
        fi

        local time_stop=$(date +%s.%N)
        local time_taken=$(printf "%.3F" $(echo "$time_stop - $time_start" | bc -l))

        # Add info. to the stored data for displaying at the end.
        Add_To_Stats "$get_ip_ret_val" "$ip_address" "$time_taken"

        # Neatly display the duration, source url, and the IP address or failure text.

        # If necessary set the failure display text.
        local web_source_failure_text

        if [ "$get_ip_ret_val" -eq "$GET_IP_NO_VALID_IP" ]; then
            web_source_failure_text="NO IP ADDRESS"

        elif [ "$get_ip_ret_val" -eq "$GET_IP_MULTIPLE_VALID_IP" ]; then
            web_source_failure_text="MULTIPLE IP"

        elif [ "$get_ip_ret_val" -eq "$GET_IP_SERVER_TIMED_OUT" ]; then
            web_source_failure_text="URL TIMED OUT"
        fi

        # Truncate $url if its length will muck up the table columns.
        local url_length=${#url}
        if [ "$url_length" -gt "52" ]; then
            local truncate_url=${url:0:48}
            url="$truncate_url..."
        fi

        if [ "$get_ip_ret_val" -eq "$GET_IP_SINGLE_VALID_IP" ]; then
            printf "%-8s %-53s %s\n" "$time_taken" "$url" "$ip_address"
        else
            printf "%-8s %-53s %s\n" "$time_taken" "$url" "$web_source_failure_text"
        fi

    done  # End of loop through $web_sources loop.

    # Display the stats info.
    Display_Stats

    # The exit point of the script in check mode.
    exit "$EXIT_SUCCESS"
}


#
# COMMAND LINE PROCESSING
#


# Handle the long options (loop through the args to check them all).

for arg in "$@"; do

    if [ "$arg" = "--help" -o "$arg" = "-help" ]; then
        Display_Help_Man_Page
        exit "$EXIT_SUCCESS"

    elif [ "$arg" = "--version" -o "$arg" = "-version" -o "$arg" = "-ver" ]; then
        printf "%s\n" "getip v. $VERSION"
        exit "$EXIT_SUCCESS"
    fi
done

# Use getopts to process the command line arguments.

while getopts ":bcghn:p:s:t:u:v" option; do

    case $option in

        g)
            # -g : set the operation mode.
            operation_mode="$GET_IP_MODE"
            ;;

        c)
            # -c : set the operation mode.
            operation_mode="$CHECK_MODE"
            ;;

        u)
            # -u url : set the operation mode and url to test.
            operation_mode="$TEST_URL_MODE"
            test_url_address_tmp="$OPTARG"

            # Check that a http url was entered.

            if [[ "$test_url_address_tmp" =~ $REGEX_HTTP_URL ]]; then
                test_url_address="$test_url_address_tmp"

            else
                Exit_With_Error_Message "$EXIT_ERR_ARG_INVALID" "u"
            fi

            ;;

        n)
            # -n num : set the number of web sources to use for verification.
            web_sources_verify_num_tmp="$OPTARG"

            # Check that a single digit in range was entered.

            # For the sake of readability use shorter variables.
            min="$WEB_SOURCES_VERIFY_NUM_MIN"
            max="$WEB_SOURCES_VERIFY_NUM_MAX"

            regex_single_digit_in_range="^[$min-$max]$"

            if [[ "$web_sources_verify_num_tmp" =~ $regex_single_digit_in_range ]]; then
                web_sources_verify_num="$web_sources_verify_num_tmp"

            else
                Exit_With_Error_Message "$EXIT_ERR_ARG_INVALID" "n"
            fi
            ;;

        t)
            # -t secs : set the user timeout.
            timeout_user_set_tmp="$OPTARG"

            # Use awk to check that an int or real number in the correct range was entered.

            # The "$1 + 0 == $1" bit is an awk hack to check for any kind of number (int or real).
            awk_is_num_in_range='{ if ($1 + 0 == $1 && $1 >= to_min && $1 <= to_max) \
                                                        print "TRUE"; else print "FALSE"; }'

            awk_is_num_in_range_ret_val=$(echo "$timeout_user_set_tmp" | \
                      awk "$awk_is_num_in_range" to_min="$TIMEOUT_MIN" to_max="$TIMEOUT_MAX")

            if [ "$awk_is_num_in_range_ret_val" = "TRUE" ]; then
                timeout_user_set="$timeout_user_set_tmp"

            else
                Exit_With_Error_Message "$EXIT_ERR_ARG_INVALID" "t"
            fi
            ;;

        p)
            # -p program : set which download program to use.
            user_set_download_program_tmp="$OPTARG"

            # Check that either 'wget' or 'curl' was entered.

            regex_wget="^[Ww][Gg][Ee][Tt]$"
            regex_curl="^[Cc][Uu][Rr][Ll]$"

            if [[ "$user_set_download_program_tmp" =~ $regex_wget ]]; then
                download_program_user_set="$DOWNLOAD_PROGRAM_WGET"

            elif [[ "$user_set_download_program_tmp" =~ $regex_curl ]]; then
                download_program_user_set="$DOWNLOAD_PROGRAM_CURL"

            else
                Exit_With_Error_Message "$EXIT_ERR_ARG_INVALID" "p"
            fi
            ;;

        s)
            # -s : set the location of the web sources list file.
            web_sources_list_user_set_tmp="$OPTARG"

            # Make sure that either a local file or a http url was entered.

            if [ -f "$web_sources_list_user_set_tmp" ]; then
                web_sources_list_user_set="$web_sources_list_user_set_tmp"

            elif [[ "$web_sources_list_user_set_tmp" =~ $REGEX_HTTP_URL ]]; then
                web_sources_list_user_set="$web_sources_list_user_set_tmp"

            else
                Exit_With_Error_Message "$EXIT_ERR_WEB_SOURCE_LIST_INVALID"
            fi
            ;;

        v)
            # -v : set verbose.
            verbose="TRUE"
            ;;

        b)
            # -b : set bare output.
            bare_output="TRUE"
            ;;

        h)
            # -h : display help.
            Display_Help_Summary
            exit "$EXIT_SUCCESS"
            ;;

        \?)
            # Invalid option : e.g. '-x'.
            Exit_With_Error_Message "$EXIT_ERR_OPTION_INVALID"
            ;;

        :)
            # Additional arg missing : e.g. -t <BLANK>.
            Exit_With_Error_Message "$EXIT_ERR_ARG_MISSING" "$OPTARG"
            ;;
    esac
done

# Check that there are no additional, and therefore invalid, options. Note that getopts stops
# processing if it encounters an arg without a '-' prefix (clearly not including the OPTARGs).

# $OPTIND is the index of the next arg to be processed by getopts.
let "num_args_processed_by_getopts = OPTIND - 1"
shift "$num_args_processed_by_getopts"
num_args_remaining="$#"

if [ "$num_args_remaining" -gt "0" ]; then
    Exit_With_Error_Message "$EXIT_ERR_OPTION_INVALID"
fi

# Set the appropriate timeout.

# If the user set the timeout use that, otherwise use the mode's default timeout.
if [ "$timeout_user_set" != "FALSE" ]; then
    timeout="$timeout_user_set"

else
    if [ "$operation_mode" = "$GET_IP_MODE" ]; then
        timeout="$TIMEOUT_DEFAULT_GET_IP_MODE"

    elif [ "$operation_mode" = "$CHECK_MODE" ]; then
        timeout="$TIMEOUT_DEFAULT_CHECK_MODE"

    elif [ "$operation_mode" = "$TEST_URL_MODE" ]; then
        timeout="$TIMEOUT_DEFAULT_TEST_URL_MODE"
    fi
fi

# If both verbose and bare output are set, then verbose trumps bare.
if [ "$verbose" = "TRUE" -a "$bare_output" = "TRUE" ]; then
    bare_output="FALSE"
fi

# Determine whether to use wget or curl as the download program.

# Check whether wget and curl are installed, at least one is required.
hash wget > /dev/null 2>&1
is_wget_installed=$?

hash curl > /dev/null 2>&1
is_curl_installed=$?

# If neither wget nor curl are installed, display error and exit.
if [ "$is_wget_installed" -ne "0" -a "$is_curl_installed" -ne "0" ]; then
    Exit_With_Error_Message "$EXIT_ERR_DOWNLOAD_PROGRAM_NOT_INSTALLED" "wget+curl"
fi

# If the user has not set which download program to use.
if [ "$download_program_user_set" = "FALSE" ]; then

    # If both wget and curl are installed, use the default.
    if [ "$is_wget_installed" -eq "0" -a "$is_curl_installed" -eq "0" ]; then
        download_program="$DOWNLOAD_PROGRAM_DEFAULT"

    # If only wget is installed, use that.
    elif [ "$is_wget_installed" -eq "0" -a "$is_curl_installed" -ne "0" ]; then
        download_program="$DOWNLOAD_PROGRAM_WGET"

    # If only curl is installed, use that.
    elif [ "$is_curl_installed" -eq "0" -a "$is_wget_installed" -ne "0" ]; then
        download_program="$DOWNLOAD_PROGRAM_CURL"
    fi

# If the user has set which download program to use.
else

    # If the user set wget, use that if it is installed.
    if [ "$download_program_user_set" = "$DOWNLOAD_PROGRAM_WGET" ]; then

        if [ "$is_wget_installed" -eq "0" ]; then
            download_program="$DOWNLOAD_PROGRAM_WGET"
        else
            Exit_With_Error_Message "$EXIT_ERR_DOWNLOAD_PROGRAM_NOT_INSTALLED" "wget"
        fi

    # If the user set curl, use that if it is installed.
    elif [ "$download_program_user_set" = "$DOWNLOAD_PROGRAM_CURL" ]; then

        if [ "$is_curl_installed" -eq "0" ]; then
            download_program="$DOWNLOAD_PROGRAM_CURL"
        else
            Exit_With_Error_Message "$EXIT_ERR_DOWNLOAD_PROGRAM_NOT_INSTALLED" "curl"
        fi
    fi
fi

# Set the download program display text (used if being verbose).
if [ "$download_program" = "$DOWNLOAD_PROGRAM_WGET" ]; then
    download_program_display_text="wget"
elif [ "$download_program" = "$DOWNLOAD_PROGRAM_CURL" ]; then
    download_program_display_text="curl"
fi

# Call the appropriate function for the operation mode.

if [ "$operation_mode" = "$GET_IP_MODE" ]; then
    Handle_Get_IP_Mode

elif [ "$operation_mode" = "$TEST_URL_MODE" ]; then
    Handle_Test_Url_Mode

elif [ "$operation_mode" = "$CHECK_MODE" ]; then
    Handle_Check_Mode
fi

#
# End of script.
#
