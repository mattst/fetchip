#!/bin/bash

#
# Name:             getip
# Description:      Script to retrieve a WAN IPv4 address from IP providing web sites.
# Requirements:     UNIX/Linux, Bash, either curl or wget.
# Exit Status:      0 on success, and non-zero on failure.
# Author:           mattst@i-dig.info
# Homepage:         https://github.com/mattst/getip
# License:          GNU General Public License v3 - http://www.gnu.org/copyleft/gpl.html
#
#
# The complete list of all utilities and Bash shell commands used by the getip script:
#
# awk, bc, cat, curl, date, disown, echo, exit, getopts, grep, hash, kill, man, mktemp, printf, ps,
# $RANDOM, read, rm, sed, shift, sort, tr, uniq, unset, wc, wget. [Either wget or curl required.]
#
# With the exception of wget and curl all of the above utilities and commands are expected to exist
# on all modern UNIX like operating systems. If neither wget nor curl are installed the script will
# advise the user to install one of them.
#
#
# Description:
#
# getip is a script used to retrieve a wide area network (WAN) IPv4 address from behind a router
# using web pages which provide the IP address in plain text or HTML. Note: IPv4 only, not IPv6.
#
# getip echoes the IP address so that it is displayed in the shell or so that it can be stored in a
# variable if getip is called from within another script.
#
# Command line options (all optional, in typical use none are necessary.):
#
# -g           GET; get the IP from multiple sources and verify they are all the same (default).
# -c           CHECK; display the IP address returned by all the web sources and show stats.
# -u url       TEST; test an URL for possible inclusion in the source urls and show info.
# -n number    Set the NUMBER of sources to use for verification. Range >=2 and <=5 (default 2).
# -t seconds   Set the TIMEOUT for each source. Real numbers are permitted (e.g. '1.5', '0.75').
# -b           BARE output; display only the IP address and not the "IPv4 address:" prefix.
# -v           VERBOSE; output information while the script is running.
# -h, --help   HELP; output program usage and help.
# --version    Display the VERSION of the script.
#
# GET is the default operation so -g is not needed on the command line, is uses the default number
# of sources which is 2 unless '-n number' is used to set the number of sources.
#
#
# The getip script uses a file containing a long list of web pages, each of which provide the IP
# address in either plain text or HTML, this is known as the sources list. Each web page url is on a
# separate line in the file. The sources list is stored online on the script's github page. Each
# time the script is run it downloads the sources list - by doing this the script always uses the
# most up-to-date version of the list, rather than a local copy which would soon contain sites that
# no longer exist, have become slow to respond, or which are otherwise no longer reliable.
#
# From time to time a web page used as a source will stop providing IP addresses, either temporarily
# or permanently, but an example IP address might remain on the page, or an IP-like software version
# number might be present on the page (embedded within the HTML) and which could be mistaken for a
# real IP address (although the script tries hard to spot and ignore these). In such circumstances
# an IP address fetching script could provide a user with an incorrect IP address. The getip script
# makes sure this does not happen.
#
# The getip script retrieves the IP address from multiple sources and checks that they have supplied
# the same IP address. The default number of sources used is 2 but this can be set on the command
# line using the -n option, the range of values is >=2 and <=5. The default is considered to be
# sufficient due to the unlikelihood of 2 sources both providing the same incorrect IP address. Very
# cautious users might want to use 3 sources, while 5 would indicate paranoia. Clearly the more
# sources used the longer it takes [guesstimate about 1 second per source on average].
#
#
# Description of Options:
# FAST                   - Command line switch: -f
#
# In fast mode getip will retrieve the IP address from just one source with no verification. It is
# generally faster, though not guaranteed to be. It is suggested that fast mode only be used when a
# user already knows their IP address and is seeking confirmation of it, for instance if checking
# whether a connection to a VPN has finished or succeeded. Fast mode should never be used when
# calling getip from within another script.
#
#
# CHECK                  - Command line switch: -c
#
# In check mode getip will display a table in the shell of all the web sources, the IP address which
# was retrieved from the source, or a message of failure, and the time taken to retrieve the IP
# address. In addition it displays some statistics at the end. It can be used by users to check that
# all the sources are functioning correctly. getip will continue to function perfectly adequately
# with even 25% of the sources down. If more than 15% are down you may wish to download the latest
# version of the script or manually delete sources which aren't working from the list below.
#
#
# TESTURL                - Command line switch: -u url
#
# In test url mode getip will test the url to see if it is suitable for adding to the source list.
# getip will display a small table showing the url's status; whether the Url successfully returned a
# single valid IP address, or failed by supplying more than one valid IP address, no IP address at
# all, or if the timeout was exceeded. Only experienced users should use this feature to add an url
# to the source list. If you do use this to find a new url that is suitable for inclusion please
# contact the developer with the url.
#
#
# The other command line options:
#
# NUMBER OF SOURCES      - Command line switch: -n num_sources [Default 2, range: >=2 and <=5.]
#
# Sets the number of sources to use in the multi operation mode.
#
#
# TIMEOUT, FOR EACH SOURCE   - Command line switch: -t seconds
#                            - Range >=0.2 and <=15. [Real numbers are permitted.]
#                            - Default timeout in multi mode:    1.0
#                            - Default timeout in fast mode:     0.75
#                            - Default timeout in check mode:    10.0
#                            - Default timeout in test url mode: 10.0
#
# Sets the timeout in seconds for each source, not an overall timeout for the script. When a source
# gives no response or is slow to respond, getip simply moves on to the next source in the sources
# list. Each of the operation modes has its own default timeout (shown above). The allowed range is
# large, >=0.2 and <=15, as large variations in speed of web access and the user's physical location
# both mean a flexible timeout is sensible, however values less than 0.75 or greater than 3 are not
# usually advisable in multi or fast mode. In check and test url modes a high timeout facilitates
# checking whether sources are functioning at all or not.
#
# There is no 'global' timeout feature for getip, however the timeout multiplied by the number of
# sources means there is a de facto 'global' timeout.
#
#
# BARE                   - Command line switch: -b
#
# Turn bare output on - the script will display only the IP address and not the "IPv4 address:"
# prefix.
#
#
# VERBOSE                - Command line switch: -v
#
# Turns verbose on - the script will output diagnostic information while it runs.
#
#
# HELP                   - Command line switch: -h or --help
#
# Outputs program usage and help then exits.
#
#
# Note on Bash: If 'local' is on the same line as a command then $? will contain local's exit status
# instead of the command's, effectively $? will always be zero. Define the local first, e.g.
#
# local example
# example=$(program etc.)
#
#

# http://stackoverflow.com/questions/4082966/what-are-the-alternatives-now-that-the-google-web-search-api-has-been-deprecated

##
## START OF SCRIPT.
##


#
# The Principal Variables.
#


# Holds the script version number.
version="1.0.1"


# Constants to hold the 3 operation modes.
GET_IP_MODE="1"
TEST_URL_MODE="2"
CHECK_MODE="3"

# Holds the operation mode, default is $GET_IP_MODE.
operation_mode="$GET_IP_MODE"

# Holds the default url of the list of web sources.
web_sources_list_default="https://crius.feralhosting.com/gencon/SourceUrlsList"

# Holds the url or local file if the user set the location of the web sources list (the -s option).
web_sources_list_user_set="FALSE"

# Array which holds the urls from the web sources list (once it's been read).
web_sources=()

# Holds the number of urls in $web_sources.
web_sources_len="0"

# Holds the minimum number of web sources that are required. Note: Consider removing this.
web_sources_min="10"

# Holds the default number of web sources to use for verification. Range: >=2 and <=5.
web_sources_verify_num="2"

# Holds the timeout value in seconds (of each web source, no overall script timeout).
timeout="0"

# Holds the GETIP mode default timeout. [Set it low so that if a source is offline or slow to
# respond the script can move on to the next source. Recommended: 0.75 to 3.0.]
timeout_get_ip_mode_default="1.5"

# Holds the CHECK mode default timeout. [Set it high to give the url a chance to succeed.]
timeout_check_mode_default="10"

# Holds the TESTURL mode default timeout. [Set it high to give the url a chance to succeed.]
timeout_test_url_mode_default="10"

# Holds the user set timeout (the -t option).
timeout_user_set="FALSE"

# Holds the url of the web source to test in test url mode.
test_url_address=""

# Constants to hold the 2 download program possibilities, either wget or curl can be used.
DOWNLOAD_PROGRAM_WGET="1"
DOWNLOAD_PROGRAM_CURL="2"

# Holds the download program.
download_program=""

# Holds the default download program (in tests wget seems to be a little quicker than curl).
download_program_default="$DOWNLOAD_PROGRAM_WGET"

# Holds the user set download program (the -d option).
download_program_user_set="FALSE"

# Holds if the user has turned on verbose reporting (the -v option).
verbose="FALSE"

# Holds if the user has turned on bare output (the -b option).
bare_output="FALSE"

# Holds the user agent to use for web page downloads. The script does not try to hide that it
# is a script. The user agent header is modelled on the one used by Google's 'googlebot':
# Googlebot 2.1: "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
user_agent="Mozilla/5.0 (compatible; getipscript/$version; +https://github.com/mattst/getip)"

# Variables used to hold statistical information in the check operation mode.
stats_ip_address_list=""
stats_ip_valid_total="0"
stats_ip_invalid_total="0"
stats_ip_retrieval_times_list=""

# Variables used only for neatly displaying information.
web_sources_list_location_display_text=""
download_program_display_text=""

# Constants to hold the values returned by the Get_IP_Address_From_Web_Source() function.
SINGLE_VALID_IP="0"
NO_VALID_IP="1"
MULTIPLE_VALID_IP="2"
SERVER_TIMED_OUT="3"
TEMP_FILE_CREATION_FAILED="4"

# Constants to hold the values returned by the Verify_IP_List() function.
IP_ADDRESS_LIST_VERIFICATION_SUCCESS="0"
IP_ADDRESS_LIST_VERIFICATION_FAILURE="1"

# Constant to hold the successful exit value.
EXIT_SUCCESS="0"

# Constants to hold the various error exit values, they are also used for error messages.
EXIT_ERR_ARG_INVALID="100"
EXIT_ERR_OPTION_INVALID="101"
EXIT_ERR_ARG_MISSING="102"
EXIT_ERR_MORE_THAN_ONE_MODE_SET="103"
EXIT_ERR_TEMP_FILE_CREATION_FAILED="104"
EXIT_ERR_WEB_SOURCE_LIST_INVALID="105"
EXIT_ERR_READ_WEB_SOURCE_LIST_FAILED="106"
EXIT_ERR_NOT_ENOUGH_SOURCE_URLS="107"
EXIT_ERR_FAILED_TO_GET_IP_ADDRESS="108"
EXIT_ERR_WGET_NOT_INSTALLED="109"
EXIT_ERR_CURL_NOT_INSTALLED="110"
EXIT_ERR_WGET_AND_CURL_NOT_INSTALLED="111"
EXIT_ERR_TEST_URL_NO_IP_ADDRESS="112"
EXIT_ERR_TEST_URL_MULTIPLE_IP_ADDRESS="113"
EXIT_ERR_TEST_URL_TIMED_OUT="114"


#
# The Functions.
#


# Function to display the help summary (cheat sheet). [-h]
Display_Help_Summary()
{
cat <<EOF
getip v. $version - Bash script usage and help summary (cheat sheet).

Display the full manual using --help, an inbuilt man page will be displayed.

getip is a script used to get the WAN IPv4 address (aka external/public IP).
The IP address will be retrieved from multiple web sources and verified.

getip [-g | -c | -u url] [-n num_sources] [-t timeout_secs] [-s sources_list]
cont.           [-d download_program] [-v | -b] [-h] [--help]

In typical usage no options are necessary.

-g      Retrieve the IP address from multiple sources and verify it. (Default)
-c      Diagnostic information about each source. Used for list maintenance.
-u url  Tests an url for use as a source. Used for list maintenance.
-n num  Set the num sources used to verify IP address. (Default: 2; >=2 <=5)
-t secs Set the timeout for each web source. (Default 1.5 secs; >=0.2 <=15)
-v      Verbose; display detailed information while getip runs.
-b      Bare; display only the IP address. Warning: no error messages.
-h      Help; display this usage and help summary (cheat sheet).
--help  Help; display the full manual in the form of an inbuilt man page.
--version  Version; display the version number.
-s file    Set a custom web sources list file; local file path or http url.
-d prog    Set the program to download each source. Use: 'wget' or 'curl'.
           (Default: Uses wget unless only curl is installed. wget is faster.)
EOF
}


# Function to display the manual in the form of an inbuilt man page. [--help]
Display_Help_Man_Page()
{
    # Create a temp file to store the man page here document.
    local getip_man_page_temp_file
    getip_man_page_temp_file=$(mktemp -q -t "getip.tmp.XXXXXX")
    local make_temp_file_ret_val=$?

    if [ "$make_temp_file_ret_val" -ne "0" ]; then
        Exit_With_Error_Message "$EXIT_ERR_TEMP_FILE_CREATION_FAILED"
    fi

# Output a here document of the getip man page to $getip_man_page_temp_file.
# Here documents don't work well with indentation, keep to the hard left.
cat >>$getip_man_page_temp_file <<EOF
.\" Manpage for getip.

.TH GETIP 1 "01 Nov 2015" "1.0.1" "GETIP VERSION 1.0.1"

.SH NAME

getip retrieves the wide area network (WAN) IP address; this is sometimes referred to as the external IP address or the public IP address. It gets IPv4 addresses only and not IPv6 addresses.

.SH SYNOPSIS

\fBgetip\fR [-g | -c | -u url] [-n num_sources] [-t timeout_secs] [-s sources_list] [-d download_program] [-v | -b] [-h] [--help]

In typical usage no options are necessary.


.SH DESCRIPTION

\fBgetip\fR is a \fBBASH\fR script used to retrieve the WAN IPv4 address from behind a router using web pages (sources) which provide the IP address of the 'visitor' in HTML or plain text. Once retrieved and confirmed, the IP address is printed to the terminal (\fBstdout\fR). \fBgetip\fR can use either \fBwget\fR or \fBcurl\fR as the download program to retrieve the web sources. See: \fBREQUIREMENTS\fR

From time to time a web page used as a source will stop providing IP addresses, either temporarily or permanently, but an example IP address might remain on the web page, or an IP-address-like software version number might be present, embedded in the HTML, and which could be mistaken for a real IP address (although the script tries hard to spot these so they can be ignored). In such cases, or in other similar ones, an IP address fetching script could provide an incorrect IP address.

To prevent an incorrect IP address being provided \fBgetip\fR always retrieves the IP address from multiple sources (2 by default) and compares them to make sure they are the same. In the unusual cases that they differ \fBgetip\fR will restart the retrieval procedure and use different web sources. The number of sources used for verification can be set on the command line using the \fB-n\fR option, the range of values is >=2 and <=5 but the default of 2 is considered to be sufficient due to the unlikelihood of 2 web sources both providing the same incorrect IP address. Very cautious users might want to use 3 sources, while 5 would indicate a state of paranoia. Clearly if more sources are used then the IP address retrieval will take longer (on average), 1 second per source on a low latency network is not an unreasonable guesstimate.

\fBgetip\fR uses a list of web sources held in a file on the script's \fBGithub\fR page. Each web source is a web page that would display the IP address if viewed in a web browser. \fBgetip\fR downloads the web sources list file each time the script is run so that users are always using the most up-to-date list, even if they are not using the most recent version of \fBgetip\fR. An alternative web sources list can be set on the command line using the \fB-s\fR option which can take a local file path or http url. The urls in a web sources list file must each be on a separate line and any line which does not begin with 'http' will be ignored ('https' is fine), so avoid leading whitespace. The minimum number of urls needed for a valid list is 10. See the description of the \fB-u\fR option (Test Url mode), if you wish to create a custom web sources list file.

The \fB-s\fR option can also be used to specify a copy of the web sources list file which has been downloaded and saved locally but clearly the most up-to-date list will not be used each time the script is run, and the quality of the list will degrade over time.

The web sources list is randomized every time the script is run. This is so that undue pressure is not placed on servers at the top of the list. The randomizing process is done very quickly and does not have a significant impact on the running time of the script.

\fBgetip\fR implements its own timeout facility; it does not rely on the timeout options of \fBwget\fR or \fBcurl\fR, neither of which accurately honour their timeout options nor can either accept timeouts in the fractions of a second. Since \fBgetip\fR uses many web sources a relatively low timeout is used for each web source, this can be set with the \fB-t\fR option. If one web source fails to respond quickly then \fBgetip\fR simply moves on to the next one. In Get IP mode the default timeout for each web source is 1.5 seconds. If modifying this a sensible range would be 0.75 to 3 seconds but tweaking the timeout for your internet connection's latency (speed of response) can result in improved performance (on average).

New users are advised to try the \fB-v\fR verbose option to see the current settings, the number of urls in the web sources list, etc., and (more importantly) to see a comprehensive representation what the script is doing while it runs. This will help new users gain confidence in \fBgetip's\fR reliability. Running it repeatedly with different timeout values can help users fine tune the best timeout for them to use. e.g. \fB-v\fR \fB-t\fR \fB0.75\fR ... \fB-v\fR \fB-t\fR \fB3\fR. Once a value has been decided on an alias can be created which uses that timeout so it is always used, more advanced users can change the value of the variable 'timeout_get_ip_mode_default' in the script itself if they prefer.

The \fBgetip\fR script retrieves \fBIPv4\fR addresses only and not \fBIPv6\fR addresses.

.SH REQUIREMENTS

\fBgetip\fR is a \fBBASH\fR script which requires either \fBwget\fR or \fBcurl\fR to be installed. Many modern \fBUNIX\fR like systems come with \fBwget\fR pre-installed. \fBwget\fR and \fBcurl\fR are both command line programs which can download web pages from the internet. \fBgetip\fR works very well with them both but, in lots of tests on \fBDebian\fR and \fBLinux\fR \fBMint\fR, \fBwget\fR was consistently just a little faster (approx. 200 milliseconds with the default options and using the same web sources). If both \fBwget\fR and \fBcurl\fR are installed \fBgetip\fR will use \fBwget\fR by default, if only one of them is installed then that one will be automatically used.

.SH OPTIONS
.BR
In typical usage no options are necessary.
.BR

.TP
\fB\-g\fR
Get IP mode (default); retrieve the IP address from multiple web sources and check that they are all the same. The default number of sources to use for IP address verification is 2. This is the default mode so \fB\-g\fR is optional. See: \fB\-n\fR \fBnum_sources\fR

.TP
\fB\-c\fR
Check mode; displays a table containing the url of each web source in the list, the IP address that it returned or a failure message, and the time taken to retrieve the IP address. Various statistics and, if necessary, warnings will be displayed on completion. This option is used to test all the urls in the web sources list during list maintenance and is not needed by the average user.

.TP
\fB\-u\fR \fBurl\fR
Test Url mode; tests an url for possible inclusion in the web sources list. It will display a table of helpful information, including the IP address and whether the url returned a single IP address (which is required for inclusion in the web sources list file), multiple IP addresses, or no IP addresses at all. The "http://" protocol part of the url is required. This option is not needed by the average user.

.TP
\fB\-n\fR \fBnum_web_sources\fR
Sets the Number of web sources to use for IP address verification in Get IP mode; the allowed range of values is >=2 and <=5. The default value is 2. See: \fBDESCRIPTION\fR

.TP
\fB\-t\fR \fBnum_seconds\fR
Sets the Timeout for each web source; when a web source gives no response or is slow to respond \fBgetip\fR moves on to the next source. \fB\-t\fR sets the timeout in seconds for how long to give each source to complete. Real numbers are permitted. e.g. \fB-t\fR \fB0.75\fR, \fB-t\fR \fB1.5\fR, \fB-t\fR \fB2\fR. The allowed range of seconds is >=0.2 and <=15 but these are extremes. See: \fBDESCRIPTION\fR

The default timeouts are dependant on mode, they are;
.br
Get IP mode    -  1.5 seconds
.br
Check mode     -  10 seconds
.br
Test Url mode  -  10 seconds

The Check and Test Url mode timeouts are intentionally high. They are intended to give servers a good chance to succeed, in case a server is slower than usual to respond. Any server which is consistently slow will be removed from the web sources list file.

.TP
\fB\-d\fR \fBdownload_program\fR \fB[wget | curl]\fR
Sets the Download program; \fBgetip\fR can use either \fBwget\fR or \fBcurl\fR to download each web source and the web sources list file. Use either: \fB-d\fR \fBwget\fR or \fB-d\fR \fBcurl\fR. \fBwget\fR is slightly quicker on tested systems and so is used by default if both programs are installed. Note that if only \fBcurl\fR is installed then that will be used by default. See: \fBREQUIREMENTS\fR

.TP
\fB\-s\fR \fBweb_sources_list\fR
Sets the web Sources list file; \fBgetip\fR can use a custom web sources list file or a local copy. Either a file path or http url to the file may be used. e.g. \fB-s\fR \fB/path/to/list\fR or \fB-s\fR \fBhttp://url.to/list\fR. See: \fBDESCRIPTION\fR

.TP
\fB\-b\fR
Bare output; prints only the IP address. Warning: with this option error messages are not displayed. The option exists primarily so that \fBgetip\fR can be used very easily from within other scripts which should check the exit status value of \fBgetip\fR.

.TP
\fB\-v\fR
Verbose; display detailed information while \fBgetip\fR runs.

.TP
\fB\-h\fR
Help; display usage and help summary (cheat sheet).

.TP
\fB\-\-help\fR
Help; display the full manual in the form of an inbuilt man page.


.TP
\fB\-\-version\fR
Version; displays the version number.

.SH EXIT STATUS
\fBgetip\fR returns 0 on success, and non-zero on failure.
.LP
\fB0\fR   - Success
.br
\fB100\fR - Command line argument invalid.
.br
\fB101\fR - Command line option invalid.
.br
\fB102\fR - Command line argument missing.
.br
\fB103\fR - Command line more than one mode set.
.br
\fB104\fR - Temp file creation failed.
.br
\fB105\fR - The web source list file is not a file or url.
.br
\fB106\fR - Reading the web source list file failed.
.br
\fB107\fR - Not enough urls in the web source list file.
.br
\fB108\fR - Failed to get the IP address.
.br
\fB109\fR - Download program wget is not installed.
.br
\fB110\fR - Download program curl is not installed.
.br
\fB111\fR - Download programs wget and curl are not installed.
.br
\fB112\fR - Test Url Mode: No IP address.
.br
\fB113\fR - Test Url Mode: Multiple IP addresses.
.br
\fB114\fR - Test Url Mode: Server timed out.

.SH BUGS
No known bugs. Please submit bug reports on the homepage.

.SH AUTHOR
<mattst@i-dig.info>

.SH HOMEPAGE
https://github.com/mattst/getip

.SH CONTRIBUTE
If you have a web server and would like to submit it to be used as a source in the web sources list then please see the homepage for details. It needs only a 2 line PHP program and would generate very little bandwidth - if lots of people did so it would be very helpful to the project.
.LP
Please feel free to submit new IP address providing web pages to be added to the web sources list. You should first check they are not already in the web source list file and that they are suitable for inclusion by using the \fB-u\fR \fBurl\fR option.

.SH COPYRIGHT
Copyright <mattst@i-dig.info>, GNU General Public License v.3 or later. GPLv3+ http://gnu.org/licenses/gpl.html
EOF

    # Display the man page.
    man "$getip_man_page_temp_file"

    # Delete the temp file.
    if [ -f "$getip_man_page_temp_file" ]; then rm "$getip_man_page_temp_file"; fi
}


# Function to send the various error messages to STDERR.
Exit_With_Error_Message()
{
    # Assign $err_num the value of the 1st function arg (NOT script arg).
    local err_num="$1"

    # Assign $additional_info the value of the 2nd function arg (NOT script arg).
    # Some errors display a configurable piece of information, e.g. an option.
    local additional_info="$2"

    # If bare output is set, just exit with no error message.
    if [ "$bare_output" = "TRUE" ]; then
        exit "$err_num"
    fi

    # Output the appropriate error message to STDERR.

    if [ "$err_num" = "$EXIT_ERR_ARG_INVALID" ]; then
        printf "\n%s\n" "The argument given to the $additional_info option is invalid." >&2

    elif [ "$err_num" = "$EXIT_ERR_OPTION_INVALID" ]; then
        printf "\n%s\n" "An invalid option has been entered." >&2

    elif [ "$err_num" = "$EXIT_ERR_ARG_MISSING" ]; then
        printf "\n%s\n" "The argument is missing for the option: -$additional_info" >&2

    elif [ "$err_num" = "$EXIT_ERR_MORE_THAN_ONE_MODE_SET" ]; then
        printf "\n%s\n" "Only one operation mode can be used at a time." >&2

    elif [ "$err_num" = "$EXIT_ERR_TEMP_FILE_CREATION_FAILED" ]; then
        printf "\n%s\n" "Unable to create a temp file using 'mktemp'." >&2

    elif [ "$err_num" = "$EXIT_ERR_WEB_SOURCE_LIST_INVALID" ]; then
        printf "\n%s\n" "The web sources list file is neither a local file nor a http url." >&2

    elif [ "$err_num" = "$EXIT_ERR_READ_WEB_SOURCE_LIST_FAILED" ]; then
        printf "\n%s\n" "Unable to read the web sources list file located at:" >&2
        printf "%s\n" "$web_sources_list_location_display_text" >&2

    elif [ "$err_num" = "$EXIT_ERR_NOT_ENOUGH_SOURCE_URLS" ]; then
        printf "\n%s\n" "Too few web sources in the web sources list file." >&2
        printf "%s\n" "The minimum number of urls that are required is $web_sources_min." >&2
        printf "%s\n" "[Note: HTTP redirections can cause this error to be" >&2
        printf "%s\n" "displayed when the entered url does not exist.]" >&2

    elif [ "$err_num" = "$EXIT_ERR_FAILED_TO_GET_IP_ADDRESS" ]; then
        printf "\n%s\n" "getip failed to get the IP address." >&2

    elif [ "$err_num" = "$EXIT_ERR_WGET_NOT_INSTALLED" ]; then
        printf "\n%s\n" "wget is not installed on your system." >&2

    elif [ "$err_num" = "$EXIT_ERR_CURL_NOT_INSTALLED" ]; then
        printf "\n%s\n" "curl is not installed on your system." >&2

    elif [ "$err_num" = "$EXIT_ERR_WGET_AND_CURL_NOT_INSTALLED" ]; then
        printf "\n%s\n" "Neither wget nor curl are installed on your system." >&2
    fi

    printf "\n%s\n" "-h      :  brief help (cheat sheet)." >&2
    printf "%s\n"   "--help  :  full manual (a man page)." >&2

    exit "$err_num"
}


# Function to store the urls from the appropriate web sources list in $web_sources.
Read_Source_Url_List()
{
    # Constants to specify whether the web sources list is an url or local file.
    local WEB_SOURCES_TYPE_URL="1"
    local WEB_SOURCES_TYPE_FILE="2"

    # Variables to hold the type (url or local file) and address/location.
    local web_sources_type
    local web_sources_list_url
    local web_sources_list_file

    # Use the default url if the user has not set the web sources list on the command line.
    if [ "$web_sources_list_user_set" = "FALSE" ]; then

        web_sources_type="$WEB_SOURCES_TYPE_URL"
        web_sources_list_url="$web_sources_list_default"
        web_sources_list_location_display_text="$web_sources_list_default"

    # If the user has set the web sources list on the command line then check if
    # $web_sources_list_user_set is a local file, if it's not then it's an url.
    else
        if [ -f "$web_sources_list_user_set" ]; then
            web_sources_type="$WEB_SOURCES_TYPE_FILE"
            web_sources_list_file="$web_sources_list_user_set"
            web_sources_list_location_display_text="$web_sources_list_user_set"
        else
            web_sources_type="$WEB_SOURCES_TYPE_URL"
            web_sources_list_url="$web_sources_list_user_set"
            web_sources_list_location_display_text="$web_sources_list_user_set"
        fi
    fi

    # If the web sources list is an url, download it into a temp file.
    if [ "$web_sources_type" = "$WEB_SOURCES_TYPE_URL" ]; then

        # Create a temp file to store the web sources list in.
        local web_sources_temp_file
        web_sources_temp_file=$(mktemp -q -t "getip.tmp.XXXXXX")
        local make_temp_file_ret_val=$?

        if [ "$make_temp_file_ret_val" -ne "0" ]; then
            Exit_With_Error_Message "$EXIT_ERR_TEMP_FILE_CREATION_FAILED"
        fi

        # Use wget to download the web sources list to the temp file.
        if [ "$download_program" = "$DOWNLOAD_PROGRAM_WGET" ]; then

            wget --quiet --tries=1 --user-agent="$user_agent" --output-document=- \
                                        "$web_sources_list_url" > "$web_sources_temp_file"
            local download_web_sources_list_ret_val=$?

            if [ "$download_web_sources_list_ret_val" -ne "0" ]; then
                Exit_With_Error_Message "$EXIT_ERR_READ_WEB_SOURCE_LIST_FAILED"
            fi

        # Use curl to download the web sources list to the temp file.
        elif [ "$download_program" = "$DOWNLOAD_PROGRAM_CURL" ]; then

            curl --silent --fail --user-agent "$user_agent" "$web_sources_list_url" \
                                                                > "$web_sources_temp_file"
            local download_web_sources_list_ret_val=$?

            if [ "$download_web_sources_list_ret_val" -ne "0" ]; then
                Exit_With_Error_Message "$EXIT_ERR_READ_WEB_SOURCE_LIST_FAILED"
            fi
        fi

        # Set the filename for the array reading to the temp filename.
        web_sources_list_file="$web_sources_temp_file"
    fi

    # Read the urls from $web_sources_list_file into the $web_sources array.

    # The $web_sources array must be INDEXED IN NUMERICAL SEQUENCE FROM 0.
    local index="0"

    while read -r line ; do

        # Store only lines which are urls by checking if the first 4 chars are 'http'.
        local line_first_4_chars=${line:0:4}
        local line_first_4_chars_lc=$(echo "$line_first_4_chars" | tr '[:upper:]' '[:lower:]')

        if [ "$line_first_4_chars_lc" = "http" ]; then
            web_sources[$index]="$line"
            let 'index = index + 1'
        fi

    done < "$web_sources_list_file"

    # Set the length of the web_sources array.
    web_sources_len="$index"

    # Delete the temp file.
    if [ "$web_sources_type" = "$WEB_SOURCES_TYPE_URL" ]; then
        if [ -f "$web_sources_temp_file" ]; then rm "$web_sources_temp_file"; fi
    fi
}


# Function that randomizes the $web_sources array. Since this script is being publicly released it
# is sensible to randomize the sources so as not to place undue pressure on the servers at the top
# of the list. This is an implementation of the Knuth shuffle algorithm (aka Fisher-Yates shuffle),
# which is a very efficient randomizer, needing just one pass through an array.
# IMPORTANT: THE $web_sources ARRAY MUST BE INDEXED FROM 0 FOR THIS FUNCTION.
Randomize_Url_List()
{
    # Bash $RANDOM provides a random number in the range 0..32767 so 32768 possible values.
    local rand_max="32768"

    # Start at the number of items in the array (i.e. final element's index + 1).
    local index="$web_sources_len"

    # Loop down through the array randomly swapping the current element with another randomly
    # chosen element (possibly itself). The use of >1 needs a moment to understand... if instead
    # it was >0 then the final iteration would always swap index 0 with itself.

    while [ "$index" -gt "1" ]; do

        # Modulo bias avoidance as per Knuth shuffle algorithm description. Calculate the highest
        # usable random number so that the modulo by index calculation will not result in any modulo
        # bias. Note: Not really necessary but the function only takes ~0.003 seconds with 100 urls
        # (on my PC) so why not do it right!? That's just ~0.15% of the approx. average run time.

        # Integer arithmetic, the division will be rounded down.
        local rand_num_highest_usable
        let "rand_num_highest_usable = (($rand_max / $index) * $index) - 1"

        # Get a random number within the modulo bias avoidance range.
        local rand_num="$RANDOM"

        while [ "$rand_num" -gt "$rand_num_highest_usable" ]; do
            rand_num="$RANDOM"
        done

        # Get a random index in the range 0..index-1.
        local rand_index
        let "rand_index = rand_num % index"

        # Decrement index to reference the current 'last' element in the array. For the first loop
        # iteration index will be $web_sources_len - 1 (the final element of the array) and for the
        # last loop iteration index will be 1. By doing so the current index will be swapped with
        # a randomly chosen index ($rand_index) in the range 0..index, so elements will sometimes
        # be 'swapped' with themselves (if elements could not stay put it wouldn't be random).

        let "index = index - 1"
        local swap_val=${web_sources[$index]}
        web_sources[$index]=${web_sources[$rand_index]}
        web_sources[$rand_index]=$swap_val

    done
}


# Function to extract valid IP addresses from the file given in the 1st function arg.
Extract_IP_Addresses_From_File()
{
    # Assign $web_page_source_file the value of the 1st function arg (NOT script arg).
    local web_page_source_file="$1"

    # The following Awk program will search for and print all valid IP addresses in the file
    # which is given to it as input.

    # The regex ip_like_sequence deliberately matches IP address like sequences which are too long
    # to be a valid IP so that the regex does not examine something like '11.22.33.44.55' and
    # match just the '11.22.33.44' section of it, and in so doing cause something that is
    # definitely not an IP address to get mistaken for a valid one. IP address like sequences
    # are checked in the END section to make sure they have exactly 4 segments, separated by
    # dots (".") and that each of the 4 numbers are within the 0..255 range of a valid IP.

    # The regex digit_sequence_too_long_not_ip will match sequences of numbers which are longer than
    # 3 digits. They are all removed as they can not possibly be part of an IP address.

    # The regex versioning_not_ip attempts to spot version numbers which happen to be in the same
    # format as an IP address (which they are often are). e.g. Version 1.2.0.12
    # Any ip_like_sequence which follows words like "Version", "Ver", or "V", will be removed.

    # Version numbers are also often embedded in an url like "1.2.3.4" in these examples:
    # e.g. <script language="Java_script" src="http://web.com/libs/1.2.3.4/file.js"></script>
    # e.g. <script language="Java_script" src="http://web.com/libs/v.1.2.3.4/file.js"></script>
    # The regexes begins_with_fwd_slash_not_ip and ends_with_fwd_slash_not_ip match an
    # ip_like_sequence which begins or ends with a forward slash so that those sequences can be
    # removed.

    local awk_extract_ip_addresses='
    BEGIN {
        # Regex to match an IP address like sequence (even if too long to be an IP). This is
        # deliberately a loose match, the END section will check for IP address validity.
        ip_like_sequence = "[0-9]+[.][0-9]+[.][0-9]+[.][0-9]+[0-9.]*";

        # Regex to match a number sequence longer than 3 digits.
        digit_sequence_too_long_not_ip = "[0-9][0-9][0-9][0-9]+";

        # Regex to match an IP address like sequence which is a version number.
        # Equivalent to "(version|ver|v)[ .:]*" in conjunction with: line = tolower($0);
        versioning_not_ip = "[Vv]([Ee][Rr]([Ss][Ii][Oo][Nn])?)?[ .:]*" ip_like_sequence;

        # Regexes to match IP address like sequences next to forward slashes, to avoid version
        # numbers in urls: e.g. http://web.com/libs/1.2.3.4/file.js
        begins_with_fwd_slash_not_ip = "[/]" ip_like_sequence;
        ends_with_fwd_slash_not_ip = ip_like_sequence "[/]";
    }
    {
        # Set line to the current line (more efficient than using $0 below).
        line = $0;

        # Replace sequences on line which will interfere with extracting genuine IPs. Use a
        # replacement char and not the empty string to avoid accidentally creating a valid IP
        # address from digits on either side of the removed sections. Use "/" as the replacement
        # char for the 2 "Fwd_slash" regexes so that multiple number dot slash sequences all get
        # removed, as using "x" could result in inadvertently leaving such a sequence in place.
        # e.g. "/lib1.2.3.4/5.6.7.8/9.10.11.12/file.js" would leave "/lib1.2.3.4xx/file.js".

        gsub(digit_sequence_too_long_not_ip, "x", line);
        gsub(versioning_not_ip, "x", line);
        gsub(begins_with_fwd_slash_not_ip, "/", line);
        gsub(ends_with_fwd_slash_not_ip, "/", line);

        # Loop through the current line matching IP address like sequences and storing them in
        # the INDEX of the array ip_unique_matches. By using ip_match as the array index duplicates
        # are avoided and the values can be easily retrieved by the for loop in the END section.
        # match() automatically sets the built in variables RSTART and RLENGTH.

        while (match(line, ip_like_sequence))
        {
            ip_match = substr(line, RSTART, RLENGTH);
            ip_unique_matches[ip_match];
            line = substr(line, RSTART + RLENGTH + 1);
        }
    }
    END {
        # Define some IP address related constants.
        ip_range_min = 0;
        ip_range_max = 255;
        ip_num_segments = 4;
        ip_delimiter = ".";

        # Loop through the ip_unique_matches array and print any valid IP addresses. The awk "for
        # each" type of loop is different from the norm. It provides the indexes of the array
        # and NOT the values of the array elements which is more usual in this type of loop.

        for (ip_match in ip_unique_matches)
        {
            num_segments = split(ip_match, ip_segments, ip_delimiter);
            if (num_segments == ip_num_segments &&
                ip_segments[1] >= ip_range_min && ip_segments[1] <= ip_range_max &&
                ip_segments[2] >= ip_range_min && ip_segments[2] <= ip_range_max &&
                ip_segments[3] >= ip_range_min && ip_segments[3] <= ip_range_max &&
                ip_segments[4] >= ip_range_min && ip_segments[4] <= ip_range_max)
            {
                print ip_match;
            }
        }
    }'

    # Extract valid IP addresses, each will be separated by a new line.
    local valid_ip_addresses=$(awk "$awk_extract_ip_addresses" < "$web_page_source_file")

    # Echo for capture in a variable at the point of function invocation.
    echo "$valid_ip_addresses"
}


# Function that retrieves the web source's content and returns all unique IP addresses in it. The
# function provides an accurate timeout facility; neither curl nor wget reliably honour any timeout
# value which has been set nor do they allow timeouts in fractions of a second. Since getip uses so
# many web sources a low timeout is used, if one source fails to return quickly then the next source
# is tried. With this methodology a reliable and accurate timeout is required, this function
# provides it.
Get_IP_Address_From_Web_Source()
{
    # Assign $web_source_url the value of the 1st function arg (NOT script arg).
    local web_source_url="$1"

    # Create a temp file to store the page source of $web_source_url.
    local web_source_temp_file
    web_source_temp_file=$(mktemp -q -t "getip.tmp.XXXXXX")
    local make_temp_file_ret_val=$?

    # Temp file creation errors must be handled differently here. The function is called using the
    # $() construct which means a subshell will be created for it and any exit call inside the
    # function will exit only the subshell and not the script. To get around this the return value
    # of Get_IP_Address_From_Web_Source() is checked to handle exiting the script if mktemp failed.
    if [ "$make_temp_file_ret_val" -ne "0" ]; then
        return "$TEMP_FILE_CREATION_FAILED"
    fi

    # Download the web page's source and store its contents in the temp file. The process is forked
    # (the trailing '&') to facilitate the timeout procedure.

    # Store the start time for the timeout. Note: %s seconds, %N nanoseconds.
    local timeout_start=$(date +%s.%N)

    # Download the web page's source placing it in the temp file and store the process's id.

    if [ "$download_program" = "$DOWNLOAD_PROGRAM_WGET" ]; then

        wget --quiet --tries=1 --user-agent="$user_agent" --output-document=- \
                                    "$web_source_url" > "$web_source_temp_file" &
        local download_process_id=$!

    elif [ "$download_program" = "$DOWNLOAD_PROGRAM_CURL" ]; then

        curl --silent --user-agent "$user_agent" "$web_source_url" > "$web_source_temp_file" &
        local download_process_id=$!
    fi

    # Even if kill has both stdout and stderr redirected to /dev/null the death of the process is
    # still displayed. Using disown to remove the job from the shell's active jobs table stops this
    # from happening so that there are no unsightly terminal messages.
    disown "$download_process_id"

    # Handle the timeout facility for the web page source download.

    local is_download_done="FALSE"
    local is_download_timed_out="FALSE"

    while [ "$is_download_done" = "FALSE" -a "$is_download_timed_out" = "FALSE" ]; do

        # Store whether the wget/curl download process is still listed by ps.
        ps -e | grep --quiet "$download_process_id"
        local process_grep_ret_val=$?

        # If the url download process has finished.
        if [ "$process_grep_ret_val" -ne "0" ]; then
            is_download_done="TRUE"

        # If the url download process has not finished.
        else
            # Store the current time for the timeout. Note: %s seconds, %N nanoseconds.
            local timeout_now=$(date +%s.%N)

            # Use bc to check if the timeout has been exceeded.
            local bc_exp_timeout="if (($timeout_now - $timeout_start) > $timeout) 1 else 0"
            local bc_exp_timeout_ret_val=$(echo "$bc_exp_timeout" | bc -l)

            # The timeout has been exceeded.
            if [ "$bc_exp_timeout_ret_val" -eq "1" ]; then
                is_download_timed_out="TRUE"
            fi
        fi
    done

    # If the download process completed successfully.
    if [ "$is_download_done" = "TRUE" ]; then

        # Extract IP addresses from the temp file. Each IP address will be separated by a new line
        # but if the source is working as expected then there will be exactly one IP address.
        local ip_address_list=$(Extract_IP_Addresses_From_File "$web_source_temp_file")

        # Delete the temp file.
        if [ -f "$web_source_temp_file" ]; then rm "$web_source_temp_file"; fi

        # Iterate through $ip_address_list counting the number of IP addresses.
        local ip_address_list_len="0"

        for valid_ip_address in $(echo "$ip_address_list" | tr "\n" " "); do
            let "ip_address_list_len = ip_address_list_len + 1"
        done

        # Success: if exactly 1, then that is considered to be the user's IP address, it is echoed
        # so that it can be placed in a variable at the point of function invocation.
        if [ "$ip_address_list_len" -eq "1" ]; then
            echo "$ip_address_list"
            return "$SINGLE_VALID_IP"

        # Failure: if less than 1, then no valid IP addresses were found.
        elif [ "$ip_address_list_len" -lt "1" ]; then
            return "$NO_VALID_IP"

        # Failure: if more than 1 there is no way to tell which of them is the user's IP address.
        # Echo the list so that the IP addresses can be displayed if being verbose.
        elif [ "$ip_address_list_len" -gt "1" ]; then
            echo "$ip_address_list"
            return "$MULTIPLE_VALID_IP"
        fi
    fi

    # If the url download process timed out (which it must have done or this section of the
    # function would not have been reached) then kill the process (and make doubly sure).

    # Since the process being killed has been disowned (removed from the shell's active jobs
    # table) the redirect of stdout and stderr to /dev/null may be unnecessary to hide the kill
    # messages, it is not necessary on the 3 Linux systems used to test the script, other OSes?

    if [ "$is_download_timed_out" = "TRUE" ]; then

        kill -INT "$download_process_id" > /dev/null 2>&1
        local kill_ret_val=$?

        if [ "$kill_ret_val" -ne "0" ]; then
            kill -KILL "$download_process_id" > /dev/null 2>&1
        fi
    fi

    # Delete the temp file.
    if [ -f "$web_source_temp_file" ]; then rm "$web_source_temp_file"; fi

    # Failure: the download timed out.
    return "$SERVER_TIMED_OUT"
}


# Function to check that all the IP addresses passed to it are exactly the same.
Verify_IP_List()
{
    # Assign the IP addresses passed to this function to an array.
    local ip_addresses=("$@")

    # Iterate through the array checking that all the IP addresses are the same.
    local ip
    for ip in "${ip_addresses[@]}"; do

        # If $ip is not the same as the first IP address in the array.
        if [ "$ip" != "${ip_addresses[0]}" ]; then
            return "$IP_ADDRESS_LIST_VERIFICATION_FAILURE"
        fi
    done

    # All the IP addresses in the array must be the same.
    return "$IP_ADDRESS_LIST_VERIFICATION_SUCCESS"
}


# Function that adds information to the statistics variables (check mode only).
Add_To_Stats()
{
    local status="$1"
    local ip_address_for_stats="$2"
    local ip_retrieval_time_for_stats="$3"

    # If the IP address is valid.
    if [ "$status" = "$SINGLE_VALID_IP" ]; then

        # Add the IP address to the list of valid IP addresses. [The trailing space is essential.]
        stats_ip_address_list+="$ip_address_for_stats "

        # Add the time taken to the list of times taken. [The trailing space is essential.]
        stats_ip_retrieval_times_list+="$ip_retrieval_time_for_stats "

        # Increment the valid IP address counter.
        let "stats_ip_valid_total = stats_ip_valid_total + 1"

    # If the IP address is not valid increment the invalid IP address counter.
    else
        let "stats_ip_invalid_total = stats_ip_invalid_total + 1"
    fi
}


# Function to find the fastest time in $stats_ip_retrieval_times_list (check mode only).
Find_Fastest_Time()
{
    # Awk expression to find the fastest time.
    local awk_fastest='{ if (NR == 1) fastest = $1; if ($1 < fastest) fastest = $1; } \
                   END { printf("%.3f", fastest) }'

    # Pipe the $stats_ip_retrieval_times_list to awk to get the fastest time.
    local fastest=$(echo "$stats_ip_retrieval_times_list" | tr " " "\n" | awk "$awk_fastest" )

    # Echo $fastest for capture at point of function invocation.
    echo "$fastest"
}


# Function to find the slowest time in $stats_ip_retrieval_times_list (check mode only).
Find_Slowest_Time()
{
    # Awk expression to find the slowest time.
    local awk_slowest='{ if (NR == 1) slowest = $1; if ($1 > slowest) slowest = $1; } \
                   END { printf("%.3f", slowest) }'

    # Pipe the $stats_ip_retrieval_times_list to awk to get the slowest time.
    local slowest=$(echo "$stats_ip_retrieval_times_list" | tr " " "\n" | awk "$awk_slowest" )

    # Echo $slowest for capture at point of function invocation.
    echo "$slowest"
}


# Function to calculate the mean of the $stats_ip_retrieval_times_list (check mode only).
Find_Mean_Average_Time()
{
    # Awk expression to calculate the mean average.
    local awk_mean='{ total += $1; num += 1; } \
                END { mean = total / num; printf("%.3f", mean) }'

    # Pipe the $stats_ip_retrieval_times_list to awk to get the mean.
    local mean=$(echo "$stats_ip_retrieval_times_list" | tr " " "\n" | awk "$awk_mean")

    # Echo $mean for capture at point of function invocation.
    echo "$mean"
}


# Function to calculate the median of the $stats_ip_retrieval_times_list (check mode only).
Find_Median_Average_Time()
{
    # Establish if there are an even or odd number of items.
    local num_items_parity
    let 'num_items_parity = stats_ip_valid_total % 2'

    # If an even number of items.
    if [ "$num_items_parity" -eq "0" ]; then

        # Work out which are the 2 middle items.
        local mid_item_1
        local mid_item_2
        let "mid_item_1 = stats_ip_valid_total / 2"
        let "mid_item_2 = mid_item_1 + 1"

        # Awk expression to print the median value (the mean of the 2 middle rows).
        local awk_median='{ if (NR == mid_row_1) med_1 = $1; if (NR == mid_row_2) med_2 = $1; } \
                      END { median = (med_1 + med_2) / 2; printf("%.3f", median) }'

        # Pipe the $stats_ip_retrieval_times_list to awk as a sorted list to get the median.
        local median=$(echo "$stats_ip_retrieval_times_list" | tr " " "\n" | sort -n |
                       awk "$awk_median" mid_row_1="$mid_item_1" mid_row_2="$mid_item_2")

    # If an odd number of items.
    else

        # Work out which is the middle item (integer arithmetic so division will be rounded down).
        local mid_item
        let "mid_item = (stats_ip_valid_total / 2) + 1"

        # Awk expression to print the median value (the middle row).
        local awk_median='{ if (NR == mid_row) printf("%.3f", $1) }'

        # Pipe the $stats_ip_retrieval_times_list to awk as a sorted list to get the median.
        local median=$(echo "$stats_ip_retrieval_times_list" | tr " " "\n" | sort -n |
                       awk "$awk_median" mid_row="$mid_item")
    fi

    # Echo $median for capture at point of function invocation.
    echo "$median"
}


# Function to display the statistical information (check mode only).
Display_Stats()
{
    # Trim whitespace from $stats_ip_retrieval_times_list and $stats_ip_address_list.
    local sed_exp_trim='s/^[ \t]*//g;s/[ \t]*$//g'
    stats_ip_retrieval_times_list=$(echo "$stats_ip_retrieval_times_list" | sed "$sed_exp_trim")
    stats_ip_address_list=$(echo "$stats_ip_address_list" | sed "$sed_exp_trim")

    # Find the fastest, slowest, mean, and median of the successful retrieval times.
    local fastest_time=$(Find_Fastest_Time)
    local slowest_time=$(Find_Slowest_Time)
    local mean_time=$(Find_Mean_Average_Time)
    local median_time=$(Find_Median_Average_Time)

    # Calculate the percentage of successful and failed retrievals.
    local bc_exp_success_percent="($stats_ip_valid_total / $web_sources_len) * 100"
    local success_percent=$(printf "%.2f" $(echo "$bc_exp_success_percent" | bc -l))
    local bc_exp_failed_percent="100 - $success_percent"
    local failed_percent=$(printf "%.2f" $(echo "$bc_exp_failed_percent" | bc -l))

    # Calculate the number of unique IP addresses held in $stats_ip_address_list.
    # Important: Use printf to avoid the new line that echo would add.
    local ip_num_unique=$(printf "%s" "$stats_ip_address_list" | tr " " "\n" | \
                                                            sort -n | uniq | wc -l)

    # Output the statistics.

    printf "%s\n" "------------------------------------------------------------------------------"

    printf "%-62s %s\n" "Web Sources Checked Total:" "$web_sources_len"
    printf "%-62s %-3s (%s%%)\n" "IP Addresses - Successfully Retrieved:" \
                                 "$stats_ip_valid_total" "$success_percent"
    printf "%-62s %-3s (%s%%)\n" "IP Addresses - Failed (Timed Out / Invalid):" \
                                 "$stats_ip_invalid_total" "$failed_percent"
    printf "%-62s %s\n" "Fastest (Successfully Retrieved):" "$fastest_time secs"
    printf "%-62s %s\n" "Slowest (Successfully Retrieved):" "$slowest_time secs"
    printf "%-62s %s\n" "Mean Average   (Successfully Retrieved):" "$mean_time secs"
    printf "%-62s %s\n" "Median Average (Successfully Retrieved):" "$median_time secs"

    # The number of unique IP addresses should be 1, if not issue a warning.
    if [ "$ip_num_unique" = "1" ]; then
        printf "%-62s %s\n" "Unique IP Addresses (Should Be 1):" "$ip_num_unique"
    else
        printf "%-62s %s *WARNING*\n" "Unique IP Addresses (Should Be 1):" "$ip_num_unique"
    fi

    printf "%s\n" "------------------------------------------------------------------------------"

    # Display a warning if there were no valid IP addresses.
    if [ "$stats_ip_valid_total" = "0" ]; then
        local msg1="WARNING: The web sources did not supply any valid IP addresses at all. The"
        local msg2="most likely cause is that the internet connection has been lost, the timeout"
        local msg3="was set far too low, or possibly the web sources list file used is extremely"
        local msg4="out-of-date or even corrupted."
        printf "\n%s\n%s\n%s\n%s\n\n" "$msg1" "$msg2" "$msg3" "$msg4"
    fi

    # Display a warning if the number of unique IP addresses is > 1, and list them.
    if [ "$ip_num_unique" -gt "1" ]; then
        local msg1="WARNING: The number of unique IP addresses retrieved exceeds one. It should be"
        local msg2="exactly one, your IP address. One or more of the sources is no longer suitable"
        local msg3="for use by this script. If you have the time please contact the developer with"
        local msg4="the url of the problematic source, use the --help option for contact details."
        printf "\n%s\n%s\n%s\n%s\n\n" "$msg1" "$msg2" "$msg3" "$msg4"

        # List each unique IP address in a table along with its number of occurrences.

        # This awk code uses a useful aspect of awk where any number or string in awk may be used
        # as an array index. In this case the IP addresses piped to awk will serve as the array
        # indexes for the count_ip[] array.
        local awk_ip_count='{ count_ip[$1]++ } END { for (ip in count_ip) \
                              printf("%-25s %d\n", ip, count_ip[ip]); }'

        printf "%s\n\n" "Unique IP Addresses List:"
        printf "%s\n" "IP Address         Num Occurrences"
        printf "%s\n" "----------------------------------"
        echo "$stats_ip_address_list" | tr " " "\n" | awk "$awk_ip_count" | sort -n -k 2,2
        printf "%s\n" "----------------------------------"
    fi
}


# Function which controls IP address retrieval and verification.
Handle_Get_IP_Mode()
{
    if [ "$verbose" = "TRUE" ]; then
        printf "\n%-25s %s\n" "Script Version:" "$version"
        printf "%-25s %s\n" "Operation Mode:" "Get IP Address"
        printf "%-25s %s\n" "Num Web Sources:" "$web_sources_verify_num (used for verification)"
        printf "%-25s %s\n" "Timeout (Seconds):" "$timeout"
        printf "%-25s %s\n" "Download Program:" "$download_program_display_text"
    fi

    # Read the web sources list into $web_sources.
    Read_Source_Url_List

    if [ "$verbose" = "TRUE" ]; then
        printf "%-25s %s\n" "Web Sources Location:" "$web_sources_list_location_display_text"
        printf "%-25s %s\n" "Load Web Sources:" "Succeeded ($web_sources_len urls)"
    fi

    # Check if the number of source urls is fewer than the required minimum.
    if [ "$web_sources_len" -lt "$web_sources_min" ]; then
        Exit_With_Error_Message "$EXIT_ERR_NOT_ENOUGH_SOURCE_URLS"
    fi

    # Shuffle the web sources array.
    Randomize_Url_List

    if [ "$verbose" = "TRUE" ]; then
        printf "%-25s %s\n" "Randomize Web Sources:" "Done"
    fi

    # Array to hold the IP addresses retrieved.
    local ip_list=()

    # Counter to hold how many IP addresses have so far been retrieved.
    local ip_counter="0"

    # Test error handling with 'fake' IP address.
    # 1 = has 1 IP address, 2 = has 2 IP addresses, 3 = no IP addresses, 4 = does not exist.
    # web_sources[1]="https://crius.feralhosting.com/gencon/fakeip1.html"
    # web_sources[1]="https://crius.feralhosting.com/gencon/fakeip2.html"
    # web_sources[1]="https://crius.feralhosting.com/gencon/fakeip3.html"
    # web_sources[1]="https://crius.feralhosting.com/gencon/fakeip4.html"

    # Loop through the urls in $web_sources. The script will exit from within this loop if the IP
    # address has been successfully retrieved from the required number of sources and verified.
    local url

    for url in "${web_sources[@]}"; do

        if [ "$verbose" = "TRUE" ]; then
            local time_start=$(date +%s.%N);
            printf "\n%-25s %s\n" "Trying Web Source:" "$url"
        fi

        # Store any IP addresses in the page source of $url in $ip_address.
        local ip_address
        ip_address=$(Get_IP_Address_From_Web_Source "$url")
        local get_ip_ret_val=$?

        # Failed due to a temp file creation error.
        if [ "$get_ip_ret_val" -eq "$TEMP_FILE_CREATION_FAILED" ]; then
            Exit_With_Error_Message "$EXIT_ERR_TEMP_FILE_CREATION_FAILED"
        fi

        if [ "$verbose" = "TRUE" ]; then

            # Calculate the time taken (reduce to 3 decimal places).
            local time_stop=$(date +%s.%N)
            local time_taken=$(printf "%.3F" $(echo "$time_stop - $time_start" | bc -l))

            if [ "$get_ip_ret_val" -eq "$SINGLE_VALID_IP" ]; then
                printf "%-25s %s\n" "Status:" "Success: A single valid IP address"
                printf "%-25s %s\n" "IP Address:" "$ip_address"

            elif [ "$get_ip_ret_val" -eq "$NO_VALID_IP" ]; then
                printf "%-25s %s\n" "Status:" "Failure: No IP address"

            elif [ "$get_ip_ret_val" -eq "$MULTIPLE_VALID_IP" ]; then
                printf "%-25s %s\n" "Status:" "Failure: More than one IP address"
                # Create neat text for displaying the list of IP addresses.
                local ip_address_ssv
                ip_address_ssv=$(echo "$ip_address" | tr '\n' ' ' | sed "s/ /, /g" | sed "s/, $//g")
                printf "%-25s %s\n" "IP Addresses:" "$ip_address_ssv"

            elif [ "$get_ip_ret_val" -eq "$SERVER_TIMED_OUT" ]; then
                printf "%-25s %s\n" "Status:" "Failure: Server timed out"
            fi

            printf "%-25s %s\n" "Duration (Seconds):" "$time_taken"
        fi

        # If a single valid IP addresses was retrieved.
        if [ "$get_ip_ret_val" -eq "$SINGLE_VALID_IP" ]; then

            # Add the IP address to the array, and increment the counter.
            ip_list[$ip_counter]="$ip_address"
            let "ip_counter = ip_counter + 1"

            if [ "$verbose" = "TRUE" ]; then
                printf "%-25s %s\n" "Add IP To List:" "$ip_counter of $web_sources_verify_num"
            fi

            # Check if all the IP addresses in the array are the same.
            Verify_IP_List "${ip_list[@]}"
            local verify_ip_list_ret_val=$?

            # If the IP addresses in the array are NOT all the same then re-start.
            if [ "$verify_ip_list_ret_val" = "$IP_ADDRESS_LIST_VERIFICATION_FAILURE" ]; then

                if [ "$verbose" = "TRUE" ]; then
                    printf "\n%-25s %s\n" "IP List Verification:" "Failed [IP addresses differed]"
                    printf "%-25s %s\n" "Status:" "Restart using different sources"
                fi

                # Reset the array and counter variables.
                unset ip_list
                ip_counter="0"
            fi

            # If the array now contains the required number of IP addresses to use for verification
            # then output the IP address and exit. Note: The if statement below will only evaluate
            # to true if the IP addresses in the array are all the same, if they weren't then
            # $ip_counter would have been set to 0 in the conditional above.
            if [ "$ip_counter" -eq "$web_sources_verify_num" ]; then

                if [ "$verbose" = "TRUE" ]; then
                    printf "\n%-25s %s" "IP List Verification:" "Succeeded "
                    printf "%s\n" "[$ip_counter identical IP addresses]"
                    printf "\n%-25s %s\n" "IPv4 address:" "$ip_address"
                else
                    if [ "$bare_output" = "TRUE" ]; then
                        printf "%s\n" "$ip_address"
                    else
                        printf "%s\n" "IPv4 address: $ip_address"
                        printf "%s\n" "[Verified using $web_sources_verify_num web sources.]"
                    fi
                fi

                # The exit point of the script in getip mode.
                exit "$EXIT_SUCCESS"
            fi
        fi

    done  # End of loop through $web_sources loop.

    # The exit point of the script in getip mode if the script failed to supply a valid IP address
    # from the required number of sources.
    Exit_With_Error_Message "$EXIT_ERR_FAILED_TO_GET_IP_ADDRESS"
}


# Function which controls testing a web page to see if suitable for inclusion in the source list.
Handle_Test_Url_Mode()
{
    local time_start=$(date +%s.%N)

    # Store any IP addresses in the page source of $test_url_address in $ip_address.
    local ip_address
    ip_address=$(Get_IP_Address_From_Web_Source "$test_url_address")
    local get_ip_ret_val=$?

    # Failed due to temp file creation error.
    if [ "$get_ip_ret_val" -eq "$TEMP_FILE_CREATION_FAILED" ]; then
        Exit_With_Error_Message "$EXIT_ERR_TEMP_FILE_CREATION_FAILED"
    fi

    local time_stop=$(date +%s.%N)
    local time_taken=$(printf "%.3F" $(echo "$time_stop - $time_start" | bc -l))

    # 'Pretty print' the test results.

    printf "\n%-22s %s\n" "Script Version:" "$version"
    printf "%-22s %s\n" "Operation Mode:" "Test Url"
    printf "%-22s %s\n" "Test Url:" "$test_url_address"
    printf "%-22s %s\n" "Download Program:" "$download_program_display_text"
    printf "%-22s %s\n" "Timeout  (Seconds):" "$timeout"
    printf "%-22s %s\n" "Duration (Seconds):" "$time_taken"

    if [ "$get_ip_ret_val" -eq "$SINGLE_VALID_IP" ]; then
        printf "%-22s %s\n" "Status:" "Success: A single valid IP address"
        printf "%-22s %s\n" "IP Address:" "$ip_address"
        printf "%-22s %s\n" "Conclusion:" "Url is suitable for using as a source."
        printf "%-22s %s\n" "" "[As long as the IP address is correct.]"
        exit "$EXIT_SUCCESS"

    elif [ "$get_ip_ret_val" -eq "$MULTIPLE_VALID_IP" ]; then
        printf "%-22s %s\n" "Status:" "Failure: More than one IP address"
        # Create neat text for displaying the list of IP addresses.
        local ip_address_ssv=$(echo "$ip_address" | tr '\n' ' ' | sed "s/ /, /g" | sed "s/, $//g")
        printf "%-22s %s\n" "IP Addresses:" "$ip_address_ssv"
        printf "%-22s %s\n" "Conclusion:" "Url is NOT suitable for using as a source."
        exit "$EXIT_ERR_TEST_URL_MULTIPLE_IP_ADDRESS"

    elif [ "$get_ip_ret_val" -eq "$NO_VALID_IP" ]; then
        printf "%-22s %s\n" "Status:" "Failure: No IP address"
        printf "%-22s %s\n" "Conclusion:" "Url is NOT suitable for using as a source."
        exit "$EXIT_ERR_TEST_URL_NO_IP_ADDRESS"

    elif [ "$get_ip_ret_val" -eq "$SERVER_TIMED_OUT" ]; then
        printf "%-22s %s\n" "Status:" "Failure: Server timed out"
        printf "%-22s %s\n" "Conclusion:" "Url is NOT suitable for using as a source unless the"
        printf "%-22s %s\n" "" "timeout was set very low or its timing out was unusual."
        exit "$EXIT_ERR_TEST_URL_TIMED_OUT"
    fi
}


# Function which controls checking if the web pages in the source list are working or not.
Handle_Check_Mode()
{
    printf "\n%-24s %s\n" "Script Version:" "$version"
    printf "%-24s %s\n" "Operation Mode:" "Check Source Urls"
    printf "%-24s %s\n" "Timeout (Seconds):" "$timeout"
    printf "%-24s %s\n" "Download Program:" "$download_program_display_text"

    # Read the web sources list into $web_sources.
    Read_Source_Url_List

    printf "%-24s %s\n" "Web Sources Location:" "$web_sources_list_location_display_text"
    printf "%-24s %s\n\n" "Load Web Sources:" "Succeeded ($web_sources_len Urls)"

    # Table header.
    printf "%s\n" "------------------------------------------------------------------------------"
    printf "%-23s %-40s %s\n" "Seconds" "Url" "IP/Status"
    printf "%s\n" "------------------------------------------------------------------------------"

    # Loop through the urls in $web_sources providing information about the status of each one.
    local url

    for url in "${web_sources[@]}"; do

        local time_start=$(date +%s.%N);

        # Store any IP addresses in the page source of $url in $ip_address.
        local ip_address
        ip_address=$(Get_IP_Address_From_Web_Source "$url")
        local get_ip_ret_val=$?

        # Failed due to temp file creation error.
        if [ "$get_ip_ret_val" -eq "$TEMP_FILE_CREATION_FAILED" ]; then
            Exit_With_Error_Message "$EXIT_ERR_TEMP_FILE_CREATION_FAILED"
        fi

        # Calculate the time taken (reduce to 3 decimal places).
        local time_stop=$(date +%s.%N)
        local time_taken=$(printf "%.3F" $(echo "$time_stop - $time_start" | bc -l))

        # Add info. to the stored statistics for displaying at the end.
        Add_To_Stats "$get_ip_ret_val" "$ip_address" "$time_taken"

        # Set the failure text for display.
        local web_source_failure_text

        if [ "$get_ip_ret_val" -eq "$NO_VALID_IP" ]; then
            web_source_failure_text="NO IP ADDRESS"

        elif [ "$get_ip_ret_val" -eq "$MULTIPLE_VALID_IP" ]; then
            web_source_failure_text="MULTIPLE IP"

        elif [ "$get_ip_ret_val" -eq "$SERVER_TIMED_OUT" ]; then
            web_source_failure_text="URL TIMED OUT"
        fi

        # Neatly display the duration, source url, and the IP address or failure text.

        # Truncate the $url if its length will muck up the table columns.
        local url_length=${#url}
        if [ "$url_length" -gt "52" ]; then
            local truncate_url=${url:0:48}
            url="$truncate_url..."
        fi

        if [ "$get_ip_ret_val" -eq "$SINGLE_VALID_IP" ]; then
            printf "%-8s %-53s %s\n" "$time_taken" "$url" "$ip_address"
        else
            printf "%-8s %-53s %s\n" "$time_taken" "$url" "$web_source_failure_text"
        fi

    done  # End of loop through $web_sources loop.

    # Display the stats info.
    Display_Stats

    # The exit point of the script in check mode.
    exit "$EXIT_SUCCESS"
}


#
# Process The Command Line.
#


# Before processing the arguments with getopts, check for long options:

# Check for "--help", if so display the man page version of help (-h is handled by getopts).
if [ "$1" = "--help" -o "$1" = "-help" ]; then
    Display_Help_Man_Page
    exit "$EXIT_SUCCESS"

# Check if the first arg is "--version", etc., if so display version number and exit.
elif [ "$1" = "--version" -o "$1" = "-version" -o "$1" = "-ver" ]; then
    printf "%s\n" "getip v. $version"
    exit "$EXIT_SUCCESS"
fi

# Holds how many modes the user set on the command line. Only 1 mode can be used at a time.
modes_used_counter="0"

# Use getopts to process the command line arguments.
while getopts ":gcu:bvhn:t:d:s:" option; do

    case $option in

        g)
            # -g has been used, set the operation mode.
            operation_mode="$GET_IP_MODE"
            let "modes_used_counter = modes_used_counter + 1"
            ;;

        c)
            # -c has been used, set the operation mode.
            operation_mode="$CHECK_MODE"
            let "modes_used_counter = modes_used_counter + 1"
            ;;

        u)
            # -u url has been used, set the operation mode and set the test url address.
            operation_mode="$TEST_URL_MODE"
            test_url_address="$OPTARG"
            let "modes_used_counter = modes_used_counter + 1"
            ;;

        b)
            # -b has been used, turn on bare output.
            bare_output="TRUE"
            ;;

        v)
            # -v has been used, turn on verbose.
            verbose="TRUE"
            ;;

        h)
            # -h has been used, display help and then exit.
            Display_Help_Summary
            exit "$EXIT_SUCCESS"
            ;;

        n)
            # -n num has been used, set the number of sources to use.
            web_sources_verify_num="$OPTARG"

            # Use sed to check that $web_sources_verify_num is a single digit >=2 and <=5.
            num_min="2"
            num_max="5"

            sed_exp_is_num_in_range="s/^[$num_min-$num_max]\{1\}$/SINGLE_DIGIT_IN_RANGE/g"
            is_num_in_range=$(echo "$web_sources_verify_num" | sed "$sed_exp_is_num_in_range")

            # A single digit in range has not been entered.
            if [ "$is_num_in_range" != "SINGLE_DIGIT_IN_RANGE" ]; then
                Exit_With_Error_Message "$EXIT_ERR_ARG_INVALID" "-n"
            fi
            ;;

        t)
            # -t seconds has been used, set the user timeout variable.
            timeout_user_set="$OPTARG"

            # Check that $timeout_user_set is a number, int or real. This is a Bash hack to use
            # awk to check whether a variable is any kind of number (int or real).
            awk_is_num='{ if ($1 + 0 == $1) print "Num"; else print "Not_num" }'
            awk_is_num_ret_val=$(echo "$timeout_user_set" | awk "$awk_is_num")

            if [ "$awk_is_num_ret_val" = "Not_num" ]; then
                Exit_With_Error_Message "$EXIT_ERR_ARG_INVALID" "-t"
            fi

            # Use bc to check that $timeout_user_set is a number in range, >=0.2 and <=15.
            # The awk check has to be done first as bc doesn't like being given a non-number.

            timeout_min="0.2"
            timeout_max="15"

            bc_exp_is_num_in_range="if ($timeout_user_set < $timeout_min || \
                                        $timeout_user_set > $timeout_max) 1 else 0"
            bc_exp_is_num_in_range_ret_val=$(echo "$bc_exp_is_num_in_range" | bc -l)

            # A positive number in range has not been entered.
            if [ "$bc_exp_is_num_in_range_ret_val" = "1" ]; then
                Exit_With_Error_Message "$EXIT_ERR_ARG_INVALID" "-t"
            fi
            ;;

        d)
            # -d has been used to set the download program, set which program to use.
            download_program_user_set_lc=$(echo "$OPTARG" | tr '[:upper:]' '[:lower:]')

            if [ "$download_program_user_set_lc" = "wget" ]; then
                download_program_user_set="$DOWNLOAD_PROGRAM_WGET"

            elif [ "$download_program_user_set_lc" = "curl" ]; then
                download_program_user_set="$DOWNLOAD_PROGRAM_CURL"

            else
                Exit_With_Error_Message "$EXIT_ERR_ARG_INVALID" "-d"
            fi
            ;;

        s)
            # -s has been used to set the location of the source urls list.
            user_set_list_tmp="$OPTARG"

            # Make sure that it is either an existing local file or it is an http url, 
            # just a rudimentary check by seeing if it begins with 'http'.

            if [ -f "$user_set_list_tmp" ]; then
                web_sources_list_user_set="$OPTARG"
            
            else
                first_4_chars=${user_set_list_tmp:0:4}
                first_4_chars_lc=$(echo "$first_4_chars" | tr '[:upper:]' '[:lower:]')

                if [ "$first_4_chars_lc" = "http" ]; then
                    web_sources_list_user_set="$OPTARG"

                else
                    Exit_With_Error_Message "$EXIT_ERR_WEB_SOURCE_LIST_INVALID"
                fi
            fi
            ;;

        \?)
            # User entered an invalid option, e.g. '-x'. Note: Invalid args with no '-' prefix,
            # are handled immediately after this while loop.
            Exit_With_Error_Message "$EXIT_ERR_OPTION_INVALID"
            ;;

        :)
            # User omitted the required additional arg of an option (e.g. -t <BLANK>).
            Exit_With_Error_Message "$EXIT_ERR_ARG_MISSING" "$OPTARG"
            ;;
    esac
done


# getopts stops processing args if it encounters an arg without a '-' prefix (unless it is an
# $OPTARG in which case processing continues). Check if any invalid args have been entered by
# working out how many args getopts has actually processed, shifting them off, and if more
# than 0 args remain then an invalid arg must have been entered.

let "num_args_processed_by_getopts = OPTIND - 1"
shift "$num_args_processed_by_getopts"
num_args_remaining="$#"

if [ "$num_args_remaining" -gt "0" ]; then
    Exit_With_Error_Message "$EXIT_ERR_OPTION_INVALID"
fi

# If the user set more than 1 mode on the command line, display error and exit.
if [ "$modes_used_counter" -gt "1" ]; then
    Exit_With_Error_Message "$EXIT_ERR_MORE_THAN_ONE_MODE_SET"
fi

# Set the appropriate timeout.

# If the user set the timeout use that otherwise use the default.
if [ "$timeout_user_set" != "FALSE" ]; then
    timeout="$timeout_user_set"
else
    if [ "$operation_mode" = "$GET_IP_MODE" ]; then
        timeout="$timeout_get_ip_mode_default"

    elif [ "$operation_mode" = "$CHECK_MODE" ]; then
        timeout="$timeout_check_mode_default"

    elif [ "$operation_mode" = "$TEST_URL_MODE" ]; then
        timeout="$timeout_test_url_mode_default"
    fi
fi

# If both verbose and bare output are set, then verbose trumps bare.
if [ "$verbose" = "TRUE" -a "$bare_output" = "TRUE" ]; then
    bare_output="FALSE"
fi

# Determine whether to use wget or curl as the download program.

# Check whether wget and curl are installed, at least one is required.
hash wget > /dev/null 2>&1
is_wget_installed=$?

hash curl > /dev/null 2>&1
is_curl_installed=$?

# If the user has not set which one to use on the command line.
if [ "$download_program_user_set" = "FALSE" ]; then

    # If both wget and curl are installed, use the default.
    if [ "$is_wget_installed" -eq "0" -a "$is_curl_installed" -eq "0" ]; then
        download_program="$download_program_default"

    # If only wget is installed, use that.
    elif [ "$is_wget_installed" -eq "0" -a "$is_curl_installed" -ne "0" ]; then
        download_program="$DOWNLOAD_PROGRAM_WGET"

    # If only curl is installed, use that.
    elif [ "$is_curl_installed" -eq "0" -a "$is_wget_installed" -ne "0" ]; then
        download_program="$DOWNLOAD_PROGRAM_CURL"

    # If neither wget nor curl are installed, display error and exit.
    else
        Exit_With_Error_Message "$EXIT_ERR_WGET_AND_CURL_NOT_INSTALLED"
    fi

# If the user has set which one to use on the command line.
else

    # If the user set wget, use that if it is installed.
    if [ "$download_program_user_set" = "$DOWNLOAD_PROGRAM_WGET" ]; then

        if [ "$is_wget_installed" -eq "0" ]; then
            download_program="$DOWNLOAD_PROGRAM_WGET"
        else
            Exit_With_Error_Message "$EXIT_ERR_WGET_NOT_INSTALLED"
        fi

    # If the user set curl, use that if it is installed.
    elif [ "$download_program_user_set" = "$DOWNLOAD_PROGRAM_CURL" ]; then

        if [ "$is_curl_installed" -eq "0" ]; then
            download_program="$DOWNLOAD_PROGRAM_CURL"
        else
            Exit_With_Error_Message "$EXIT_ERR_CURL_NOT_INSTALLED"
        fi
    fi
fi

# Set the url download program display text (used if being verbose).
if [ "$download_program" = "$DOWNLOAD_PROGRAM_WGET" ]; then
    download_program_display_text="wget"
elif [ "$download_program" = "$DOWNLOAD_PROGRAM_CURL" ]; then
    download_program_display_text="curl"
fi

# Call the appropriate function for the operation mode, the script will exit from within these
# functions so the conditional below will never complete.

if [ "$operation_mode" = "$GET_IP_MODE" ]; then
    Handle_Get_IP_Mode

elif [ "$operation_mode" = "$TEST_URL_MODE" ]; then
    Handle_Test_Url_Mode

elif [ "$operation_mode" = "$CHECK_MODE" ]; then
    Handle_Check_Mode
fi


##
## END OF SCRIPT.
##
