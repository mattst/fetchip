#!/bin/bash


#
# fetchip script Copyright (C) 2015 mattst <mattst@i-dig.info>
#
# This program is free software: you can redistribute it and/or modify it under the terms of the
# GNU General Public License as published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without
# even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License along with this program.
# If not see: http://www.gnu.org/licenses/gpl-3.0.en.html
#


#
# Name:            fetchip
# Description:     Script to retrieve a WAN IPv4 address from IP providing web sites.
# Requirements:    UNIX/Linux, Bash, either wget or curl (wget is a little faster).
# Exit Status:     0 on success, and non-zero on failure.
# Author:          mattst <mattst@i-dig.info>
# Homepage:        https://github.com/mattst/fetchip
#
#
# fetchip retrieves the wide area network (WAN) IP address; this is sometimes referred to as the
# external IP address or the public IP address. It gets IPv4 addresses only and not IPv6 addresses.
# fetchip always gets the IP address from multiple sources so that it can be verified. See the
# documentation for detailed information; a man page is built-in to this script and can be viewed
# using the --help option, while -h will display a help summary (cheat sheet).
#
#
# Developer's Notes:
#
# 1. A list of all utilities and Bash shell commands used by the fetchip script. With the exception
#    of wget and curl all of the utilities below are expected to exist on all modern UNIX like
#    operating systems (wget is fairly standard these days). If neither wget nor curl are installed
#    the script will inform the user that one of them needs to be installed.
#
#    awk, bc, case (etc.), cat, curl, date, disown, echo, exit, for (etc.), getopts, grep, hash,
#    if (etc.), kill, let, local, man, mktemp, printf, ps, $RANDOM, read, return, rm, sed, shift,
#    sort, tr, uniq, unset, wc, wget, while (etc.). [Either wget or curl is required.]
#
# 2. In Bash if 'local' is on the same line as a variable is used to capture the output of a command
#    with $() and $? is used immediately afterwards, then $? will return local's exit status and not
#    the command's, effectively $? will always return 0. To avoid this define the local first. e.g.
#
# Wrong:
#    local example=$(program ...)
#    local program_ret_val=$?
#
# Right:
#    local example
#    example=$(program ...)
#    local program_ret_val=$?
#
# 3. If a function is called using the $() construct then a subshell will be created for it. This
#    means that any exit call from inside the function will exit only the subshell and not the
#    script. Watch out for this if the intention is to exit the script. This is seen in the function
#    Fetch_IP_Address_From_Web_Source() which returns an error constant if its call to mktemp fails,
#    rather than exiting the script which is what happens if mktemp were to fail elsewhere.
#
# 4. A regex pattern used with the =~ regular expression matching operator is handled differently by
#    different versions of Bash. The safe way of formatting these is to always place the pattern in
#    a variable first and then to avoid quoting the pattern in the conditional. e.g.
#
#    regex_pattern="^regex$"
#    if [[ "$variable" =~ $regex_pattern ]]; then
#


#
# CONSTANTS
#


VERSION="1.0.2"

# The 3 operation modes.
FETCH_IP_MODE="1"
TEST_URL_MODE="2"
CHECK_MODE="3"

# The default url to the list of web sources.
WEB_SOURCES_LIST_DEFAULT="https://raw.githubusercontent.com/mattst/fetchip/master/source_urls_list"

# The default, min, and max values of the number of web sources to use for verification.
WEB_SOURCES_VERIFY_NUM_DEFAULT="2"
WEB_SOURCES_VERIFY_NUM_MIN="2"
WEB_SOURCES_VERIFY_NUM_MAX="5"

# Various timeouts - all are in seconds.

# The default, min, and max timeouts for web source downloads.
TIMEOUT_MIN="0.2"
TIMEOUT_MAX="15"
TIMEOUT_DEFAULT_FETCH_IP_MODE="1.5"
TIMEOUT_DEFAULT_CHECK_MODE="10"
TIMEOUT_DEFAULT_TEST_URL_MODE="10"

# The timeout for downloading the web sources list file. [This must be an integer.]
TIMEOUT_WEB_SOURCES_LIST="10"

# The minimum number of web sources that are required (in a web sources list).
WEB_SOURCES_MIN="10"

# The 2 download program possibilities and the default (wget is slightly faster).
DOWNLOAD_PROGRAM_WGET="1"
DOWNLOAD_PROGRAM_CURL="2"
DOWNLOAD_PROGRAM_DEFAULT="$DOWNLOAD_PROGRAM_WGET"

# The user agent to use for web page downloads. It is modelled on the one used by googlebot:
# Googlebot 2.1: "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
USER_AGENT="Mozilla/5.0 (compatible; fetchipscript/$VERSION; +https://github.com/mattst/fetchip)"

# Holds a regex to match a http url (just a rudimentary case insensitive match).
HTTP_URL_REGEX="^[Hh][Tt][Tt][Pp][Ss]?://.+\..+$"

# Holds the template to use when creating temp files.
MKTEMP_TEMPLATE="fetchip.tmp.XXXXXX"

# The values returned by the Fetch_IP_Address_From_Web_Source() function.
FETCH_IP_SINGLE_VALID_IP="0"
FETCH_IP_NO_VALID_IP="1"
FETCH_IP_MULTIPLE_VALID_IP="2"
FETCH_IP_SERVER_TIMED_OUT="3"
FETCH_IP_TEMP_FILE_CREATION_FAILED="4"

# The values returned by the Verify_IP_Address_List() function.
VERIFY_IP_ADDRESS_LIST_SUCCESS="0"
VERIFY_IP_ADDRESS_LIST_FAILURE="1"

# The various exit values.
EXIT_SUCCESS="0"
EXIT_ERR_OPTION_UNKNOWN="101"
EXIT_ERR_ARG_INVALID="102"
EXIT_ERR_ARG_MISSING="103"
EXIT_ERR_DOWNLOAD_PROGRAM_NOT_INSTALLED="104"
EXIT_ERR_TEMP_FILE_CREATION_FAILED="105"
EXIT_ERR_WEB_SOURCE_LIST_INVALID="106"
EXIT_ERR_READ_WEB_SOURCE_LIST_FAILED="107"
EXIT_ERR_NOT_ENOUGH_SOURCE_URLS="108"
EXIT_ERR_FAILED_TO_FETCH_IP_ADDRESS="109"
EXIT_ERR_TEST_URL_NO_IP_ADDRESS="110"
EXIT_ERR_TEST_URL_MULTIPLE_IP_ADDRESS="111"
EXIT_ERR_TEST_URL_SERVER_TIMED_OUT="112"


#
# VARIABLES
#


# The operation mode, default is $FETCH_IP_MODE.
operation_mode="$FETCH_IP_MODE"

# Array to store the urls from the web sources list file.
web_sources=()

# The number of urls in $web_sources.
web_sources_len="0"

# The number of web sources to use for verification.
web_sources_verify_num="$WEB_SOURCES_VERIFY_NUM_DEFAULT"

# The url or local file of a user set web sources list file.
web_sources_list_user_set="FALSE"

# The timeout value in seconds for each web source (no overall script timeout).
timeout=""

# A user set timeout.
timeout_user_set="FALSE"

# A fail safe timeout is used when downloading web sources.
# Set_Timeouts() explains the purposes of these variables.
timeout_fail_safe=""
timeout_fail_safe_offset="5"

# Holds which download program to use.
download_program="$DOWNLOAD_PROGRAM_DEFAULT"

# A user set download program.
download_program_user_set="FALSE"

# Whether to use verbose output.
verbose="FALSE"

# Whether to use bare output.
bare_output="FALSE"

# A user set url of a web source to test in test url mode.
test_url_address=""

# Variables used to hold statistical data in check mode.
stats_ip_address_list=""
stats_ip_valid_total="0"
stats_ip_invalid_total="0"
stats_ip_retrieval_times_list=""

# Variables used to hold display text if being verbose.
download_program_display_text=""
web_sources_list_location_display_text=""


#
# FUNCTIONS
#


# Function to display the help summary (cheat sheet). [-h]
Display_Help_Summary()
{
cat <<EOF
fetchip v. $VERSION - script usage and help summary (cheat sheet).

--help displays the full manual, a built-in man page will be shown.

A Bash script used to get the WAN IPv4 address (aka external/public IP).
The IP address will be retrieved from multiple web sources and verified.

fetchip [OPTION...]

In typical usage no options are needed; -f is the default.

-f         Fetch the IP address from multiple sources and verify it. [Default]
-n num     Set the num sources used to verify IP address. [Default: 2; >=2 <=5]
-t secs    Set the timeout for each web source. [Default 1.5 secs; >=0.2 <=15]
-s file    Set a custom web sources list file. [Use: file path or http url.]
-p prog    Set the program to download each source. [Use: 'wget' or 'curl'.]
           [Default: wget; unless only curl is installed. wget is faster.]
-v         Verbose; print detailed information while the script runs.
-b         Bare; print only the IP address.
-h         Help; print this usage and help summary (cheat sheet).
--help     Help; display the full manual in the form of a built-in man page.
--version  Print the version number.
-c         Diagnostic information about all sources.  [List maintenance only.]
-u url     Tests an url for possible use as a source. [List maintenance only.]
EOF
}


# Function to display the manual in the form of an inbuilt man page. [--help]
Display_Help_Man_Page()
{
    # Output a here document of the fetchip man page to a temp file.

    local man_page_temp_file
    man_page_temp_file=$(mktemp -q -t "$MKTEMP_TEMPLATE")
    local make_temp_file_ret_val=$?

    if [ "$make_temp_file_ret_val" -ne "0" ]; then
        Exit_With_Error_Message "$EXIT_ERR_TEMP_FILE_CREATION_FAILED"
    fi

# Here documents don't like indentation, keep to the hard left.

cat >>$man_page_temp_file <<EOF
.\" Man page for fetchip.

.TH FETCHIP 1 "13 Nov 2015" "1.0.2" "FETCHIP VERSION 1.0.2"

.SH NAME

fetchip retrieves the wide area network (WAN) IP address; this is sometimes referred to as the external IP address or the public IP address. It gets IPv4 addresses only and not IPv6 addresses.

.SH SYNOPSIS
\fBfetchip\fR [OPTION...]

.SH DESCRIPTION

\fBfetchip\fR is a BASH script used to retrieve the WAN IPv4 address from behind a router using web pages (sources) which provide the IP address of the 'visitor' in HTML or plain text. Once retrieved and confirmed (minimum 2 sources), the IP address is printed to the terminal (\fBstdout\fR). \fBfetchip\fR can use either \fBwget\fR or \fBcurl\fR as the download program to retrieve the web sources. See: \fBREQUIREMENTS\fR

From time to time a web page used as a source will stop providing IP addresses, either temporarily or permanently, but an example IP address might remain on the web page, or an IP-address-like software version number might be present, embedded in the HTML, and which could be mistaken for a real IP address (although the script tries hard to spot these so they can be ignored). In such cases, or in other similar ones, an IP address fetching script could provide an incorrect IP address.

To prevent an incorrect IP address being provided \fBfetchip\fR always retrieves the IP address from multiple web sources (2 by default) and compares them to make sure they are the same. In the unusual cases that they differ \fBfetchip\fR will restart the retrieval procedure and use different web sources. The number of sources used for verification can be set on the command line using the \fB-n\fR option, the range of values is >=2 and <=5 but the default of 2 is considered to be sufficient due to the unlikelihood of 2 web sources both providing the same incorrect IP address. Very cautious users might want to use 3 sources, while 5 would indicate a state of paranoia. Clearly if more sources are used then the IP address retrieval will take longer (on average), 1 second per source on a low latency network is not an unreasonable guesstimate.

\fBfetchip\fR uses a list of web sources held in a file on the script's \fBGithub\fR page. Each web source is a web page that would display the IP address if viewed in a web browser. \fBfetchip\fR downloads the web sources list file each time the script is run so that users are always using the most up-to-date list, even if they are not using the most recent version of \fBfetchip\fR. An alternative web sources list file can be set on the command line using the \fB-s\fR option which can take a local file path or http url. The http urls in a web sources list file must each be on a separate line with no additional whitespace, any line which does not begin with a http url will be ignored. The minimum number of urls needed for a valid list is 10. See the description of the \fB-u\fR option (Test Url mode), if you wish to create a custom web sources list file.

The \fB-s\fR option can also be used to specify a copy of the web sources list file which has been downloaded and saved locally, but clearly the most up-to-date list will not be used each time the script is run, and the reliability of such a list will degrade over time.

The web sources list is randomized every time the script is run. This is so that undue pressure is not placed on servers at the top of the list. The randomizing process is done very quickly and does not have a significant impact on the running time of the script.

\fBfetchip\fR implements its own timeout facility; it does not rely on the timeout options of \fBwget\fR or \fBcurl\fR, neither of which can accept timeouts in the fractions of a second. Since \fBfetchip\fR uses many web sources a relatively low timeout is used for each web source, this can be set with the \fB-t\fR option. If one web source fails to respond quickly then \fBfetchip\fR simply moves on to the next one. The default timeout for each web source is 1.5 seconds. If modifying this a sensible range would be 0.75 to 3 seconds but tweaking the timeout for your internet connection's latency (speed of response) can result in improved performance (on average).

New users are advised to try the \fB-v\fR verbose option to see the settings, the number of urls in the web sources list file, etc., and (more importantly) to see a comprehensive representation of what the script is doing while it runs. This will help new users gain confidence in \fBfetchip's\fR reliability. Running it repeatedly with different timeout values can help users fine tune the best timeout for them to use. e.g. \fB-v\fR \fB-t\fR \fB0.75\fR ... \fB-v\fR \fB-t\fR \fB3\fR. Once a value has been decided on an alias can be created which always uses that timeout.

The \fBfetchip\fR script retrieves IPv4 addresses only and not IPv6 addresses.

All output is printed to \fBstdout\fR except for error messages which are printed to \fBstderr\fR.

.SH REQUIREMENTS

\fBfetchip\fR is a \fBBASH\fR script which requires either \fBwget\fR or \fBcurl\fR to be installed. Many modern UNIX-like operating systems come with wget pre-installed. wget and curl are both command line programs which can download web pages from the internet. \fBfetchip\fR works very well with them both but, in lots of tests on Debian and Linux Mint operating systems, wget was consistently just a little faster. If both wget and curl are installed \fBfetchip\fR will use wget by default, if only one of them is installed then that one will be automatically used. See: \fB\-p\fR \fBdownload_program\fR

.SH OPTIONS
.BR
In typical usage no options are needed; \fB\-f\fR is the default.
.BR

.TP
\fB\-f\fR
Fetch IP mode (default); print the WAN IP address. This will retrieve the IP address from multiple web sources and check that they are all the same. The default number of sources to use for IP address verification is 2. This is the default mode so \fB\-f\fR is optional. See: \fB\-n\fR \fBnum_sources\fR

.TP
\fB\-c\fR
Check mode; print a table containing the http url of each of the sources in the web sources list file, the IP address that it returned or a failure message, and the time taken to retrieve the IP address. Various statistics and, if necessary, warnings will be printed on completion. [This option is used during the maintenance of the web sources list file and is not needed by the average user.]

.TP
\fB\-u\fR \fBurl\fR
Test Url mode; tests a http url for possible inclusion in the web sources list file. It will print a table of helpful information, including the IP address and whether the url returned a single IP address (which is required for inclusion in the web sources list file), multiple IP addresses, or no IP addresses at all. The "http://" protocol part of the url is required. [This option is used during the maintenance of the web sources list file and is not needed by the average user.]

.TP
\fB\-n\fR \fBnum_web_sources\fR
Sets the number of web sources to use for IP address verification. The allowed range of values is >=2 and <=5. The default value is 2. See: \fBDESCRIPTION\fR

.TP
\fB\-t\fR \fBnum_seconds\fR
Sets the timeout for each web source; when a web source gives no response or is slow to respond \fBfetchip\fR moves on to the next source. \fB\-t\fR sets the timeout in seconds for how long to give each source to complete. Real numbers are permitted. The allowed range of seconds is >=0.2 and <=15 but these are extremes. e.g. \fB-t\fR \fB0.75\fR, \fB-t\fR \fB1.5\fR, \fB-t\fR \fB3\fR. See: \fBDESCRIPTION\fR

The default timeouts are dependant on mode, they are:
.br
Fetch IP mode  -  1.5 seconds
.br
Check mode     -  10 seconds
.br
Test Url mode  -  10 seconds

The Check and Test Url mode timeouts are intentionally high. They are intended to give servers a good chance to succeed, in case a server is slower than usual to respond. Any url which is consistently slow will be removed from the current web sources list file.

.TP
\fB\-p\fR \fBdownload_program\fR \fB[wget | curl]\fR
Sets the download program; either wget or curl can be used to download each web source and the web sources list file. Use either: \fB-p\fR \fBwget\fR or \fB-p\fR \fBcurl\fR. wget is slightly quicker on tested systems and so is used by default if both programs are installed. Note that if only curl is installed then that will be used by default. See: \fBREQUIREMENTS\fR

.TP
\fB\-s\fR \fBweb_sources_list\fR
Sets a custom web sources list file; \fBfetchip\fR can use a custom web sources list file or a local copy. Either a file path or a http url to the file may be used. e.g. \fB-s\fR \fB/path/to/list\fR or \fB-s\fR \fBhttp://url.to/list\fR. See: \fBDESCRIPTION\fR

.TP
\fB\-v\fR
Verbose; print detailed information while the script runs.

.TP
\fB\-b\fR
Bare output; prints only the IP address on success or \fBFAILED\fR if the script fails to get the IP address from the required number of sources. Error messages will still be printed, redirect \fBstderr\fR to \fB/dev/null\fR if they are not wanted. This option exists so that \fBfetchip\fR can be used easily from within other scripts. 

.TP
\fB\-h\fR
Help; print usage and help summary (cheat sheet).

.TP
\fB\-\-help\fR
Help; display the full manual in the form of a built-in man page.

.TP
\fB\-\-version\fR
Version; print the version number.

.SH EXIT STATUS
\fBfetchip\fR returns 0 on success, and non-zero on failure.
.LP
\fB0\fR   - Success
.br
\fB101\fR - Command line option unknown.
.br
\fB102\fR - Command line argument invalid.
.br
\fB103\fR - Command line argument missing.
.br
\fB104\fR - Download program(s) not installed.
.br
\fB105\fR - Temp file creation failed.
.br
\fB106\fR - Web sources list file is not a file or url.
.br
\fB107\fR - Reading the web sources list file failed.
.br
\fB108\fR - Not enough urls in the web sources list file.
.br
\fB109\fR - Failed to get the IP address.
.br
\fB110\fR - Test Url Mode: No IP address.
.br
\fB111\fR - Test Url Mode: Multiple IP addresses.
.br
\fB112\fR - Test Url Mode: Server timed out.

.SH BUGS
No known bugs. Please submit bug reports on the homepage.

.SH AUTHOR
mattst <mattst@i-dig.info>

.SH HOMEPAGE
https://github.com/mattst/fetchip

.SH CONTRIBUTE
If you have a web server and would like to allow it to be used as a source in the web sources list file then please see the homepage for details. It needs only a 2 line PHP program and would generate very little bandwidth - if lots of people did so it would be helpful to the project.
.LP
Please feel free to submit new IP address providing web pages to be added to the web sources list file. You should first check they are not already in the latest web sources list file and that they are suitable for inclusion by using the \fB-u\fR \fBurl\fR option.

.SH COPYRIGHT
Copyright mattst <mattst@i-dig.info>
.br
GNU General Public License v.3
.br
http://www.gnu.org/licenses/gpl-3.0.en.html
EOF

    # Display the man page and then delete it.

    man "$man_page_temp_file"

    if [ -f "$man_page_temp_file" ]; then rm "$man_page_temp_file"; fi
}


# Function to display error messages and then exit the script.
Exit_With_Error_Message()
{
    local error_num="$1"

    # Some errors have a configurable piece of information, e.g. an option letter.
    local additional_info="$2"

    # Output the appropriate error message to STDERR.

    if [ "$error_num" = "$EXIT_ERR_OPTION_UNKNOWN" ]; then
        printf "\n%s\n" "An unknown option has been entered." >&2

    elif [ "$error_num" = "$EXIT_ERR_ARG_INVALID" ]; then
        printf "\n%s\n" "The argument given to the -$additional_info option is invalid." >&2

    elif [ "$error_num" = "$EXIT_ERR_ARG_MISSING" ]; then
        printf "\n%s\n" "The argument is missing from the -$additional_info option." >&2

    elif [ "$error_num" = "$EXIT_ERR_DOWNLOAD_PROGRAM_NOT_INSTALLED" ]; then
        if [ "$additional_info" = "wget+curl" ]; then
            printf "\n%s\n" "Neither wget nor curl are installed on your system." >&2
        else
            printf "\n%s\n" "$additional_info is not installed on your system." >&2
        fi

    elif [ "$error_num" = "$EXIT_ERR_TEMP_FILE_CREATION_FAILED" ]; then
        printf "\n%s\n" "Unable to create a temp file using mktemp." >&2

    elif [ "$error_num" = "$EXIT_ERR_WEB_SOURCE_LIST_INVALID" ]; then
        printf "\n%s\n" "The web sources list file is neither a local file nor a http url." >&2

    elif [ "$error_num" = "$EXIT_ERR_READ_WEB_SOURCE_LIST_FAILED" ]; then
        printf "\n%s\n" "Unable to read the web sources list file located at:" >&2
        printf "%s\n" "$web_sources_list_location_display_text" >&2

    elif [ "$error_num" = "$EXIT_ERR_NOT_ENOUGH_SOURCE_URLS" ]; then
        printf "\n%s\n" "Not enough web sources in the web sources list file." >&2
        printf "%s\n" "The minimum number of urls that are required is $WEB_SOURCES_MIN." >&2
        printf "\n%s\n" "[Note: HTTP redirections can cause this error to" >&2
        printf "%s\n" "be displayed when the domain/url does not exist.]" >&2

    elif [ "$error_num" = "$EXIT_ERR_FAILED_TO_FETCH_IP_ADDRESS" ]; then
        printf "\n%s\n" "Unable to get the IP address." >&2

    fi

    printf "\n%s\n" "-h      :  brief help (cheat sheet)." >&2
    printf "%s\n"   "--help  :  full manual (a man page)." >&2

    exit "$error_num"
}


# Function to read the web sources list file and store the urls in the $web_sources array.
Read_Source_Url_List()
{
    # Constants to specify whether the web sources list is an url or local file.
    local WEB_SOURCES_TYPE_URL="1"
    local WEB_SOURCES_TYPE_FILE="2"

    # Variables to hold the type (url or local file) and address/location.
    local web_sources_type
    local web_sources_list_url
    local web_sources_list_file

    # Use the default url if the user has not set the web sources list.
    if [ "$web_sources_list_user_set" = "FALSE" ]; then

        web_sources_type="$WEB_SOURCES_TYPE_URL"
        web_sources_list_url="$WEB_SOURCES_LIST_DEFAULT"
        web_sources_list_location_display_text="$WEB_SOURCES_LIST_DEFAULT"

    # If the user has set the web sources list then check if $web_sources_list_user_set is a local
    # file, if it is not then it must be an url.
    else
        if [ -f "$web_sources_list_user_set" ]; then
            web_sources_type="$WEB_SOURCES_TYPE_FILE"
            web_sources_list_file="$web_sources_list_user_set"
            web_sources_list_location_display_text="$web_sources_list_user_set"

        else
            web_sources_type="$WEB_SOURCES_TYPE_URL"
            web_sources_list_url="$web_sources_list_user_set"
            web_sources_list_location_display_text="$web_sources_list_user_set"
        fi
    fi

    # If the web sources list is an url, download it into a temp file.
    if [ "$web_sources_type" = "$WEB_SOURCES_TYPE_URL" ]; then

        local web_sources_temp_file
        web_sources_temp_file=$(mktemp -q -t "$MKTEMP_TEMPLATE")
        local make_temp_file_ret_val=$?

        if [ "$make_temp_file_ret_val" -ne "0" ]; then
            Exit_With_Error_Message "$EXIT_ERR_TEMP_FILE_CREATION_FAILED"
        fi

        # Use wget or curl to download the web sources list.

        local download_web_sources_list_ret_val

        if [ "$download_program" = "$DOWNLOAD_PROGRAM_WGET" ]; then

            wget --quiet --tries=1 --timeout="$TIMEOUT_WEB_SOURCES_LIST" --output-document=- \
                    --user-agent="$USER_AGENT" "$web_sources_list_url" > "$web_sources_temp_file"

            download_web_sources_list_ret_val=$?

        elif [ "$download_program" = "$DOWNLOAD_PROGRAM_CURL" ]; then

            curl --silent --max-time "$TIMEOUT_WEB_SOURCES_LIST" --fail \
                    --user-agent "$USER_AGENT" "$web_sources_list_url" > "$web_sources_temp_file"

            download_web_sources_list_ret_val=$?
        fi

        if [ "$download_web_sources_list_ret_val" -ne "0" ]; then
            Exit_With_Error_Message "$EXIT_ERR_READ_WEB_SOURCE_LIST_FAILED"
        fi

        # Set the array building loop filename to the temp filename.
        web_sources_list_file="$web_sources_temp_file"
    fi

    # Read the urls from $web_sources_list_file into the $web_sources array.

    # The $web_sources array must be INDEXED IN NUMERICAL SEQUENCE FROM 0.
    local index="0"

    while read -r line ; do

        # Store only the http url lines, ignore anything else.

        if [[ "$line" =~ $HTTP_URL_REGEX ]]; then
            web_sources[$index]="$line"
            let 'index = index + 1'
        fi

    done < "$web_sources_list_file"

    # Set the length of the web_sources array.
    web_sources_len="$index"

    # If necessary delete the temp file.
    if [ "$web_sources_type" = "$WEB_SOURCES_TYPE_URL" ]; then
        if [ -f "$web_sources_temp_file" ]; then rm "$web_sources_temp_file"; fi
    fi
}


# Function that randomizes the $web_sources array. Since this script is being publicly released it
# is sensible to randomize the sources so as not to place undue pressure on the servers at the top
# of the list. This is an implementation of the Knuth shuffle algorithm (aka Fisher-Yates shuffle),
# which is a very efficient randomizer, needing just one pass through an array.
# IMPORTANT: THE $web_sources ARRAY MUST BE INDEXED FROM 0 FOR THIS FUNCTION.
Randomize_Url_List()
{
    # Bash $RANDOM provides a random number in the range 0..32767 so 32768 possible values.
    local rand_max="32768"

    # Set the number of items in the array.
    local index="$web_sources_len"

    # Loop down through the array randomly swapping the current element with another randomly
    # chosen element (possibly itself). The use of >1 needs a moment to understand... if instead
    # it was >0 then the final iteration would always swap index 0 with itself.

    local rand_num_highest_usable
    local rand_num
    local rand_index
    local swap_val

    while [ "$index" -gt "1" ]; do

        # Modulo bias avoidance as per Knuth shuffle algorithm description. Calculate the highest
        # usable random number so that the modulo by index calculation will not result in any modulo
        # bias. Note: This is not really necessary but the whole function only takes ~0.005 seconds
        # with 124 urls (on my desktop PC) so may as well do it right.

        # Integer arithmetic (the division will be rounded down).
        let "rand_num_highest_usable = (($rand_max / $index) * $index) - 1"

        # Get a random number within the modulo bias avoidance range.
        rand_num="$RANDOM"

        while [ "$rand_num" -gt "$rand_num_highest_usable" ]; do
            rand_num="$RANDOM"
        done

        # Get a random index in the range 0..index-1.
        let "rand_index = rand_num % index"

        # Decrement index to reference the current array element. For the first loop iteration index
        # will be the final element of the array and for the last loop iteration index will be 1. By
        # doing so the current index will be swapped with the randomly chosen index in the range
        # 0..index, so elements will sometimes be swapped with themselves (if elements could not
        # stay put it wouldn't be random).

        let "index = index - 1"
        swap_val="${web_sources[$index]}"
        web_sources[$index]="${web_sources[$rand_index]}"
        web_sources[$rand_index]="$swap_val"

    done
}


# Function to extract valid IP addresses from the file given in the function's arg.
Extract_IP_Addresses_From_File()
{
    local web_page_source_file="$1"

    # The following Awk program will search for and print all valid IP addresses in the file which
    # is given to it as input. It looks for and removes IP-like number sequences which it identifies
    # as version numbers or numerical sequences in urls, and all numbers longer than 3 digits.

    local awk_extract_ip_addresses='
    BEGIN {
        # Regex to match an IP address like sequence (even if too long to be an IP). This is
        # deliberately a loose match, the END section will check for IP address validity.
        ip_like_sequence = "[0-9]+[.][0-9]+[.][0-9]+[.][0-9]+[0-9.]*";

        # Regex to match number sequences longer than 3 digits.
        digit_sequence_too_long_not_ip = "[0-9][0-9][0-9][0-9]+";

        # Regex to match an IP address like sequence which is part of a version number.
        # Equivalent to "(version|ver|v)[ .:]*" in conjunction with: line = tolower($0);
        versioning_not_ip = "[Vv]([Ee][Rr]([Ss][Ii][Oo][Nn])?)?[ .:]*" ip_like_sequence;

        # Regexes to match IP address like sequences next to forward slashes, to avoid
        # numbers in urls: e.g. http://web.com/libs/1.2.3.4/file.js
        begins_with_fwd_slash_not_ip = "[/]" ip_like_sequence;
        ends_with_fwd_slash_not_ip = ip_like_sequence "[/]";
    }
    {
        # Set line to the current line (more efficient than using $0 below).
        line = $0;

        # Replace sequences on line which will interfere with extracting genuine IPs. Use a
        # replacement char and not the empty string to avoid accidentally creating a valid IP
        # address from digits on either side of the removed sections. Use "/" as the replacement
        # char for the 2 "fwd_slash" regexes so that multiple number dot slash sequences all get
        # removed, as using "x" could result in inadvertently leaving such a sequence in place.
        # e.g. "/lib1.2.3.4/5.6.7.8/9.10.11.12/file.js" would leave "/lib1.2.3.4xx/file.js".

        gsub(digit_sequence_too_long_not_ip, "x", line);
        gsub(versioning_not_ip, "x", line);
        gsub(begins_with_fwd_slash_not_ip, "/", line);
        gsub(ends_with_fwd_slash_not_ip, "/", line);

        # Loop through the current line matching IP address like sequences and storing them in
        # the INDEX of the array ip_unique_matches. By using ip_match as the array index duplicates
        # are avoided and the values can be easily retrieved by the for loop in the END section.
        # match() automatically sets the built in variables RSTART and RLENGTH.

        while (match(line, ip_like_sequence))
        {
            ip_match = substr(line, RSTART, RLENGTH);
            ip_unique_matches[ip_match];
            line = substr(line, RSTART + RLENGTH + 1);
        }
    }
    END {
        # IP address constants.
        ip_range_min = 0;
        ip_range_max = 255;
        ip_num_segments = 4;
        ip_delimiter = ".";

        # Loop through the ip_unique_matches array and print any valid IP addresses. The awk "for
        # each" type of loop is different from the norm. It provides the indexes of the array
        # and NOT the values of the array elements which is more usual in this type of loop.

        for (ip_match in ip_unique_matches)
        {
            num_segments = split(ip_match, ip_segments, ip_delimiter);
            if (num_segments == ip_num_segments &&
                ip_segments[1] >= ip_range_min && ip_segments[1] <= ip_range_max &&
                ip_segments[2] >= ip_range_min && ip_segments[2] <= ip_range_max &&
                ip_segments[3] >= ip_range_min && ip_segments[3] <= ip_range_max &&
                ip_segments[4] >= ip_range_min && ip_segments[4] <= ip_range_max)
            {
                print ip_match;
            }
        }
    }'

    # Extract valid IP addresses, each will be separated by a new line.
    local valid_ip_addresses=$(awk "$awk_extract_ip_addresses" < "$web_page_source_file")

    echo "$valid_ip_addresses"
}


# Function that retrieves the web source's content and returns all unique IP addresses in it. The
# function provides an accurate timeout facility; neither wget nor curl provide an accurate timeout
# in the fractions of a second. Since fetchip uses many web sources a low timeout is used, if one
# source fails to return quickly then the next source is tried. With this methodology a reliable and
# accurate timeout is required, this function provides it.
Fetch_IP_Address_From_Web_Source()
{
    # The url to download and search for IP addresses.
    local web_source_url="$1"

    # Temp file to store the page source of $web_source_url.
    local web_source_temp_file
    web_source_temp_file=$(mktemp -q -t "$MKTEMP_TEMPLATE")
    local make_temp_file_ret_val=$?

    # Temp file creation errors must be handled differently here. The function is called using the
    # $() construct which means a subshell will be created for it and any exit call inside the
    # function will exit only the subshell and not the script. To get around this the return value
    # of this function is checked to handle exiting the script if mktemp failed.
    if [ "$make_temp_file_ret_val" -ne "0" ]; then
        return "$FETCH_IP_TEMP_FILE_CREATION_FAILED"
    fi

    # Download the web page's source placing it in the temp file and storing the process's id.
    # The process is forked (the trailing '&') to help facilitate the timeout procedure.

    # Note: %s seconds, %N nanoseconds.
    local time_start=$(date +%s.%N)

    local download_process_id

    if [ "$download_program" = "$DOWNLOAD_PROGRAM_WGET" ]; then

        wget --quiet --tries=1 --timeout="$timeout_fail_safe" --user-agent="$USER_AGENT" \
                                --output-document=- "$web_source_url" > "$web_source_temp_file" &
        download_process_id=$!

    elif [ "$download_program" = "$DOWNLOAD_PROGRAM_CURL" ]; then

        curl --silent --max-time "$timeout_fail_safe" --user-agent "$USER_AGENT" \
                                                    "$web_source_url" > "$web_source_temp_file" &
        download_process_id=$!
    fi

    # Even if kill has both stdout and stderr redirected to /dev/null the death of the process is
    # still displayed in the shell. Using disown to remove the job from the shell's active jobs
    # table stops this from happening so that there are no unsightly terminal messages.
    disown "$download_process_id"

    # Handle the timeout facility for the web page source download.

    local is_download_done="FALSE"
    local is_download_timed_out="FALSE"

    local process_grep_ret_val
    local time_now
    local bc_exp_timeout
    local bc_exp_timeout_val

    while [ "$is_download_done" = "FALSE" -a "$is_download_timed_out" = "FALSE" ]; do

        # Store whether the wget/curl download process is still listed by ps.
        ps -A | grep --quiet "$download_process_id"
        process_grep_ret_val=$?

        # If the url download process has finished grep will not match the process id.
        if [ "$process_grep_ret_val" -ne "0" ]; then
            is_download_done="TRUE"

        # If the url download process has not finished (grep did match the process id).
        else
            time_now=$(date +%s.%N)

            # Use bc to check if the timeout has been exceeded.

            # Do not use 'else' in the bc expression, it is not in POSIX bc (only in GNU bc).
            bc_exp_timeout="if (($time_now - $time_start) > $timeout) 1"
            bc_exp_timeout_val=$(echo "$bc_exp_timeout" | bc -l)

            # The timeout has been exceeded (do not use -eq, most of the time it will be "").
            if [ "$bc_exp_timeout_val" = "1" ]; then
                is_download_timed_out="TRUE"
            fi
        fi
    done

    # If the download process completed successfully.
    if [ "$is_download_done" = "TRUE" ]; then

        # Extract IP addresses from the temp file. Each IP address will be separated by a new line
        # and if the source is working as expected then there will be exactly one IP address.
        local ip_address_list=$(Extract_IP_Addresses_From_File "$web_source_temp_file")

        # Delete the temp file.
        if [ -f "$web_source_temp_file" ]; then rm "$web_source_temp_file"; fi

        # Iterate through $ip_address_list counting the number of IP addresses.
        local ip_address_list_len="0"
        local ip

        for ip in $(echo "$ip_address_list" | tr "\n" " "); do
            let "ip_address_list_len = ip_address_list_len + 1"
        done

        # Success: if exactly 1, then that is considered to be the user's IP address.
        if [ "$ip_address_list_len" -eq "1" ]; then
            echo "$ip_address_list"
            return "$FETCH_IP_SINGLE_VALID_IP"

        # Failure: if less than 1, then no valid IP addresses were found.
        elif [ "$ip_address_list_len" -lt "1" ]; then
            return "$FETCH_IP_NO_VALID_IP"

        # Failure: if more than 1 there is no way to tell which of them is the user's IP address.
        elif [ "$ip_address_list_len" -gt "1" ]; then
            echo "$ip_address_list"
            return "$FETCH_IP_MULTIPLE_VALID_IP"
        fi
    fi

    # The url download process must have timed out. No need to check for it, the code below can not
    # be reached unless it did.

    # Kill the process using SIGTERM which will allow the program to clean up after itself.
    # DO NOT USE SIGKILL; wget/curl might leave sockets open, child processes, temp files.

    kill -SIGTERM "$download_process_id" > /dev/null 2>&1

    # Delete the temp file.
    if [ -f "$web_source_temp_file" ]; then rm "$web_source_temp_file"; fi

    # Failure: the download timed out.
    return "$FETCH_IP_SERVER_TIMED_OUT"
}


# Function to check that all the IP addresses passed to it are exactly the same.
Verify_IP_Address_List()
{
    # Assign the IP addresses list passed to this function to an array.
    local ip_addresses=("$@")

    # Iterate through the array checking if all the IP addresses are the same.

    local ip
    local ip_first_element="${ip_addresses[0]}"

    for ip in "${ip_addresses[@]}"; do

        if [ "$ip" != "$ip_first_element" ]; then
            return "$VERIFY_IP_ADDRESS_LIST_FAILURE"
        fi
    done

    # All the IP addresses must be the same.
    return "$VERIFY_IP_ADDRESS_LIST_SUCCESS"
}


# Function that adds data to the statistics variables (check mode only).
Add_To_Stats()
{
    local status="$1"
    local ip_address_for_stats="$2"
    local ip_retrieval_time_for_stats="$3"

    if [ "$status" = "$FETCH_IP_SINGLE_VALID_IP" ]; then

        # Add the IP address and the time taken to the stats lists.
        # Keep the trailing spaces, they are serving as delimiters.

        stats_ip_address_list+="$ip_address_for_stats "
        stats_ip_retrieval_times_list+="$ip_retrieval_time_for_stats "
        let "stats_ip_valid_total = stats_ip_valid_total + 1"

    else
        let "stats_ip_invalid_total = stats_ip_invalid_total + 1"
    fi
}


# Function to find the fastest time in $stats_ip_retrieval_times_list (check mode only).
Find_Fastest_Time()
{
    # Use awk to find the fastest time.

    local awk_fastest='{ if (NR == 1) fastest = $1; if ($1 < fastest) fastest = $1; }
                   END { printf("%.3f", fastest) }'

    local fastest=$(echo "$stats_ip_retrieval_times_list" | tr " " "\n" | awk "$awk_fastest" )

    echo "$fastest"
}


# Function to find the slowest time in $stats_ip_retrieval_times_list (check mode only).
Find_Slowest_Time()
{
    # Use awk to find the slowest time.

    local awk_slowest='{ if (NR == 1) slowest = $1; if ($1 > slowest) slowest = $1; }
                   END { printf("%.3f", slowest) }'

    local slowest=$(echo "$stats_ip_retrieval_times_list" | tr " " "\n" | awk "$awk_slowest" )

    echo "$slowest"
}


# Function to calculate the mean of the $stats_ip_retrieval_times_list (check mode only).
Find_Mean_Average_Time()
{
    # Use awk to find the mean average.

    local awk_mean='{ total += $1; num += 1; }
                END { mean = total / num; printf("%.3f", mean) }'

    local mean=$(echo "$stats_ip_retrieval_times_list" | tr " " "\n" | awk "$awk_mean")

    echo "$mean"
}


# Function to calculate the median of the $stats_ip_retrieval_times_list (check mode only).
Find_Median_Average_Time()
{
    # Use awk to find the median average.

    # Establish if there are an even or odd number of items.
    local num_items_parity
    let 'num_items_parity = stats_ip_valid_total % 2'

    # If an even number of items.
    if [ "$num_items_parity" -eq "0" ]; then

        # Work out which are the 2 middle items.
        local mid_item_1
        local mid_item_2
        let "mid_item_1 = stats_ip_valid_total / 2"
        let "mid_item_2 = mid_item_1 + 1"

        # Awk program to print the median value (the mean of the 2 middle rows).
        local awk_median='{ if (NR == mid_row_1) med_1 = $1; if (NR == mid_row_2) med_2 = $1; }
                      END { median = (med_1 + med_2) / 2; printf("%.3f", median) }'

        local median=$(echo "$stats_ip_retrieval_times_list" | tr " " "\n" | sort -n |
                       awk "$awk_median" mid_row_1="$mid_item_1" mid_row_2="$mid_item_2")

    # If an odd number of items.
    else

        # Work out which is the middle item (integer arithmetic, division will be rounded down).
        local mid_item
        let "mid_item = (stats_ip_valid_total / 2) + 1"

        # Awk program to print the median value (the middle row).
        local awk_median='{ if (NR == mid_row) printf("%.3f", $1) }'

        local median=$(echo "$stats_ip_retrieval_times_list" | tr " " "\n" | sort -n |
                       awk "$awk_median" mid_row="$mid_item")
    fi

    echo "$median"
}


# Function to display the statistical information (check mode only).
Display_Stats()
{
    # Trim whitespace from $stats_ip_retrieval_times_list and $stats_ip_address_list.
    local sed_exp_trim='s/^[ \t]*//g;s/[ \t]*$//g'
    stats_ip_retrieval_times_list=$(echo "$stats_ip_retrieval_times_list" | sed "$sed_exp_trim")
    stats_ip_address_list=$(echo "$stats_ip_address_list" | sed "$sed_exp_trim")

    # Find the fastest, slowest, mean, and median of the successful retrieval times.
    local fastest_time=$(Find_Fastest_Time)
    local slowest_time=$(Find_Slowest_Time)
    local mean_time=$(Find_Mean_Average_Time)
    local median_time=$(Find_Median_Average_Time)

    # Calculate the percentage of successful and failed retrievals.
    local bc_exp_success_percent="($stats_ip_valid_total / $web_sources_len) * 100"
    local success_percent=$(printf "%.2f" $(echo "$bc_exp_success_percent" | bc -l))
    local bc_exp_failed_percent="100 - $success_percent"
    local failed_percent=$(printf "%.2f" $(echo "$bc_exp_failed_percent" | bc -l))

    # Calculate the number of unique IP addresses held in $stats_ip_address_list.
    # Important: Use printf to avoid the new line that echo would add.
    local ip_num_unique=$(printf "%s" "$stats_ip_address_list" | tr " " "\n" | \
                                                            sort -n | uniq | wc -l)

    # Output the statistics.

    printf "%s\n" "------------------------------------------------------------------------------"

    printf "%-62s %s\n" "Web Sources Checked Total:" "$web_sources_len"
    printf "%-62s %-3s (%s%%)\n" "IP Addresses - Successfully Retrieved:" \
                                          "$stats_ip_valid_total" "$success_percent"
    printf "%-62s %-3s (%s%%)\n" "IP Addresses - Failed (Timed Out / Invalid):" \
                                         "$stats_ip_invalid_total" "$failed_percent"
    printf "%-62s %s\n" "Fastest (Successfully Retrieved):" "$fastest_time secs"
    printf "%-62s %s\n" "Slowest (Successfully Retrieved):" "$slowest_time secs"
    printf "%-62s %s\n" "Mean Average   (Successfully Retrieved):" "$mean_time secs"
    printf "%-62s %s\n" "Median Average (Successfully Retrieved):" "$median_time secs"

    # The number of unique IP addresses should be 1, if not issue a warning.
    if [ "$ip_num_unique" = "1" ]; then
        printf "%-62s %s\n" "Unique IP Addresses (Should Be 1):" "$ip_num_unique"
    else
        printf "%-62s %s *WARNING*\n" "Unique IP Addresses (Should Be 1):" "$ip_num_unique"
    fi

    printf "%s\n" "------------------------------------------------------------------------------"

    # Display a warning if there were no valid IP addresses.
    if [ "$stats_ip_valid_total" = "0" ]; then
        local msg1="WARNING: The web sources did not supply any valid IP addresses at all. The"
        local msg2="most likely cause is that the internet connection has been lost, the timeout"
        local msg3="was set far too low, or possibly the web sources list file used is extremely"
        local msg4="out-of-date or even corrupted."
        printf "\n%s\n%s\n%s\n%s\n\n" "$msg1" "$msg2" "$msg3" "$msg4"
    fi

    # Display a warning if the number of unique IP addresses is >1.
    if [ "$ip_num_unique" -gt "1" ]; then
        local msg1="WARNING: The number of unique IP addresses retrieved exceeds one. It should be"
        local msg2="exactly one, your IP address. One or more of the sources is no longer suitable"
        local msg3="for use by this script. If you have the time please contact the developer with"
        local msg4="the url of the problematic source, use the --help option for contact details."
        printf "\n%s\n%s\n%s\n%s\n\n" "$msg1" "$msg2" "$msg3" "$msg4"

        # List each unique IP address in a table along with its number of occurrences.

        # This awk code uses a useful aspect of awk where any number or string in awk may be used
        # as an array index. In this case the IP addresses piped to awk will serve as the array
        # indexes for the count_ip[] array; same as used in Extract_IP_Addresses_From_File().
        local awk_ip_count='{ count_ip[$1]++ } END { for (ip in count_ip)
                                                        printf("%-25s %d\n", ip, count_ip[ip]); }'

        printf "%s\n\n" "Unique IP Addresses List:"
        printf "%-18s %s\n" "IP Address" "Num Occurrences"
        printf "%s\n" "----------------------------------"
        # Use sort -k to sort by the number of occurrences (column number 2), low to high.
        echo "$stats_ip_address_list" | tr " " "\n" | awk "$awk_ip_count" | sort -n -k 2,2
        printf "%s\n" "----------------------------------"
    fi
}


# Function which controls setting the timeouts appropriately.
Set_Timeouts()
{
    # If the user set the timeout use that, otherwise use the mode's default timeout.

    if [ "$timeout_user_set" != "FALSE" ]; then
        timeout="$timeout_user_set"

    else
        if [ "$operation_mode" = "$FETCH_IP_MODE" ]; then
            timeout="$TIMEOUT_DEFAULT_FETCH_IP_MODE"

        elif [ "$operation_mode" = "$CHECK_MODE" ]; then
            timeout="$TIMEOUT_DEFAULT_CHECK_MODE"

        elif [ "$operation_mode" = "$TEST_URL_MODE" ]; then
            timeout="$TIMEOUT_DEFAULT_TEST_URL_MODE"
        fi
    fi

    # A fail safe timeout is used for web sources in addition to the timeout handling implemented
    # by the function Fetch_IP_Address_From_Web_Source().

    # If a download fails to complete within the time specified by $timeout its process will be
    # sent the SIGTERM signal by kill. Occasionally the download program will not terminate in a
    # timely manner after receiving the SIGTERM request. The default timeout for wget is 15 mins,
    # for curl it is 5 mins, so processes the script created and no longer wants could continue
    # for quite some time. The fail safe timeout is given to wget or curl to ensure that processes
    # do not persist for long if they do not terminate quickly after receiving the SIGTERM signal.
    # This is just a precautionary measure to handle relatively infrequent occurrences.

    # Use awk to ROUND UP $timeout (if it's a float) and then add the fail safe timeout offset.
    # This makes sure that the fail safe timeout is always at least $timeout_fail_safe_offset
    # seconds longer than $timeout.

    awk_timeout_fail_safe='
    {
        timeout = $1;
        timeout_offset = $2;
        timeout_floor = int(timeout);

        if (timeout == timeout_floor)
            timeout_fail_safe = timeout + timeout_offset;
        else
            timeout_fail_safe = timeout_floor + 1 + timeout_offset;

        printf("%d", timeout_fail_safe);
    }'

    timeout_fail_safe=$(echo "$timeout $timeout_fail_safe_offset" | awk "$awk_timeout_fail_safe")
}


# Function which controls setting which download program should be used.
Set_Download_Program()
{
    # Check whether wget and curl are installed, at least one is required.
    hash wget > /dev/null 2>&1
    is_wget_installed=$?

    hash curl > /dev/null 2>&1
    is_curl_installed=$?

    # If neither wget nor curl are installed, display error and exit.
    if [ "$is_wget_installed" -ne "0" -a "$is_curl_installed" -ne "0" ]; then
        Exit_With_Error_Message "$EXIT_ERR_DOWNLOAD_PROGRAM_NOT_INSTALLED" "wget+curl"
    fi

    # If the user has NOT set which download program to use.
    if [ "$download_program_user_set" = "FALSE" ]; then

        # If both wget and curl are installed, use the default.
        if [ "$is_wget_installed" -eq "0" -a "$is_curl_installed" -eq "0" ]; then
            download_program="$DOWNLOAD_PROGRAM_DEFAULT"

        # If only wget is installed, use that.
        elif [ "$is_wget_installed" -eq "0" -a "$is_curl_installed" -ne "0" ]; then
            download_program="$DOWNLOAD_PROGRAM_WGET"

        # If only curl is installed, use that.
        elif [ "$is_curl_installed" -eq "0" -a "$is_wget_installed" -ne "0" ]; then
            download_program="$DOWNLOAD_PROGRAM_CURL"
        fi

    # If the user has set which download program to use.
    else

        # If the user set wget, use that if it is installed.
        if [ "$download_program_user_set" = "$DOWNLOAD_PROGRAM_WGET" ]; then

            if [ "$is_wget_installed" -eq "0" ]; then
                download_program="$DOWNLOAD_PROGRAM_WGET"
            else
                Exit_With_Error_Message "$EXIT_ERR_DOWNLOAD_PROGRAM_NOT_INSTALLED" "wget"
            fi

        # If the user set curl, use that if it is installed.
        elif [ "$download_program_user_set" = "$DOWNLOAD_PROGRAM_CURL" ]; then

            if [ "$is_curl_installed" -eq "0" ]; then
                download_program="$DOWNLOAD_PROGRAM_CURL"
            else
                Exit_With_Error_Message "$EXIT_ERR_DOWNLOAD_PROGRAM_NOT_INSTALLED" "curl"
            fi
        fi
    fi

    # Set the download program display text (used if being verbose).
    if [ "$download_program" = "$DOWNLOAD_PROGRAM_WGET" ]; then
        download_program_display_text="wget"

    elif [ "$download_program" = "$DOWNLOAD_PROGRAM_CURL" ]; then
        download_program_display_text="curl"
    fi
}


# Function which controls IP address retrieval and verification in the default mode.
Handle_Fetch_IP_Mode()
{
    if [ "$verbose" = "TRUE" ]; then
        printf "\n%-25s %s\n" "Script Version:" "$VERSION"
        printf "%-25s %s\n" "Operation Mode:" "Fetch IP Address"
        printf "%-25s %s\n" "Num Web Sources:" "$web_sources_verify_num (used for verification)"
        printf "%-25s %s\n" "Timeout (Seconds):" "$timeout"
        printf "%-25s %s\n" "Download Program:" "$download_program_display_text"
    fi

    # Read the web sources list file into $web_sources.
    Read_Source_Url_List

    if [ "$verbose" = "TRUE" ]; then
        printf "%-25s %s\n" "Web Sources Location:" "$web_sources_list_location_display_text"
        printf "%-25s %s\n" "Load Web Sources:" "Succeeded ($web_sources_len urls)"
    fi

    # Check that the number of source urls is adequate.
    if [ "$web_sources_len" -lt "$WEB_SOURCES_MIN" ]; then
        Exit_With_Error_Message "$EXIT_ERR_NOT_ENOUGH_SOURCE_URLS"
    fi

    # Shuffle the web sources array.
    Randomize_Url_List

    if [ "$verbose" = "TRUE" ]; then
        printf "%-25s %s\n" "Randomize Web Sources:" "Done"
    fi

    # Loop through the urls in $web_sources. The script will exit from within this loop when the
    # IP address has been successfully retrieved and verified by the required number of sources.

    # Array to hold the IP addresses retrieved.
    local ip_address_list=()

    # Counter to hold how many IP addresses have so far been retrieved.
    local ip_counter="0"

    local url

    for url in "${web_sources[@]}"; do

        if [ "$verbose" = "TRUE" ]; then
            local time_start=$(date +%s.%N)
            printf "\n%-25s %s\n" "Trying Web Source:" "$url"
        fi

        # Get any IP addresses that are in the page source of $url.
        local ip_address
        ip_address=$(Fetch_IP_Address_From_Web_Source "$url")
        local fetch_ip_ret_val=$?

        # Failed due to a temp file creation error.
        if [ "$fetch_ip_ret_val" -eq "$FETCH_IP_TEMP_FILE_CREATION_FAILED" ]; then
            Exit_With_Error_Message "$EXIT_ERR_TEMP_FILE_CREATION_FAILED"
        fi

        if [ "$verbose" = "TRUE" ]; then

            local time_stop=$(date +%s.%N)
            local time_taken=$(printf "%.3F" $(echo "$time_stop - $time_start" | bc -l))

            if [ "$fetch_ip_ret_val" -eq "$FETCH_IP_SINGLE_VALID_IP" ]; then
                printf "%-25s %s\n" "Status:" "Success: A single valid IP address"
                printf "%-25s %s\n" "IP Address:" "$ip_address"

            elif [ "$fetch_ip_ret_val" -eq "$FETCH_IP_NO_VALID_IP" ]; then
                printf "%-25s %s\n" "Status:" "Failure: No IP address"

            elif [ "$fetch_ip_ret_val" -eq "$FETCH_IP_MULTIPLE_VALID_IP" ]; then
                printf "%-25s %s\n" "Status:" "Failure: More than one IP address"
                # Create neat text for displaying the list of IP addresses.
                local ip_list_csv=$(echo "$ip_address" | tr '\n' ',' | sed "s/,/, /g ; s/, $//g")
                printf "%-25s %s\n" "IP Addresses:" "$ip_list_csv"

            elif [ "$fetch_ip_ret_val" -eq "$FETCH_IP_SERVER_TIMED_OUT" ]; then
                printf "%-25s %s\n" "Status:" "Failure: Server timed out"
            fi

            printf "%-25s %s\n" "Duration (Seconds):" "$time_taken"
        fi

        # If a single valid IP addresses was retrieved.
        if [ "$fetch_ip_ret_val" -eq "$FETCH_IP_SINGLE_VALID_IP" ]; then

            # Add the IP address to the array, and increment the counter.
            ip_address_list[$ip_counter]="$ip_address"
            let "ip_counter = ip_counter + 1"

            if [ "$verbose" = "TRUE" ]; then
                printf "%-25s %s\n" "Add IP To List:" "$ip_counter of $web_sources_verify_num"
            fi

            # Check if all the IP addresses in the array are the same.
            Verify_IP_Address_List "${ip_address_list[@]}"
            local verify_ip_list_ret_val=$?

            # If the IP addresses in the array are all the same.
            if [ "$verify_ip_list_ret_val" = "$VERIFY_IP_ADDRESS_LIST_SUCCESS" ]; then

                # If the array now contains the required number of IP addresses to use for
                # verification then output the IP address and exit.

                if [ "$ip_counter" -eq "$web_sources_verify_num" ]; then

                    if [ "$verbose" = "TRUE" ]; then
                        printf "\n%-25s %s" "IP List Verification:" "Succeeded "
                        printf "%s\n" "[$ip_counter identical IP addresses]"
                        printf "\n%-25s %s\n" "IPv4 address:" "$ip_address"

                    elif [ "$bare_output" = "TRUE" ]; then
                        printf "%s\n" "$ip_address"

                    else
                        printf "%s\n" "IPv4 address: $ip_address"
                        printf "%s\n" "[Verified using $web_sources_verify_num web sources.]"
                    fi

                    exit "$EXIT_SUCCESS"
                fi

            # If the IP addresses in the array are NOT all the same.
            elif [ "$verify_ip_list_ret_val" = "$VERIFY_IP_ADDRESS_LIST_FAILURE" ]; then

                if [ "$verbose" = "TRUE" ]; then
                    printf "\n%-25s %s\n" "IP List Verification:" "Failed [IP addresses differed]"
                    printf "%-25s %s\n" "Status:" "Restart using different sources"
                fi

                # Reset the array and counter variables.
                ip_address_list=()
                ip_counter="0"
            fi
        fi

    done  # End of loop through $web_sources loop.

    # The exit point of the script if it failed.

    if [ "$bare_output" = "TRUE" ]; then
        printf "%s\n" "FAILED"
        exit "$EXIT_ERR_FAILED_TO_FETCH_IP_ADDRESS"
    fi

    Exit_With_Error_Message "$EXIT_ERR_FAILED_TO_FETCH_IP_ADDRESS"
}


# Function which controls testing a web page to see if suitable to add to the web sources list.
Handle_Test_Url_Mode()
{
    printf "\n%-22s %s\n" "Script Version:" "$VERSION"
    printf "%-22s %s\n" "Operation Mode:" "Test Url"
    printf "%-22s %s\n" "Test Url:" "$test_url_address"
    printf "%-22s %s\n" "Download Program:" "$download_program_display_text"
    printf "%-22s %s\n" "Timeout  (Seconds):" "$timeout"

    local time_start=$(date +%s.%N)

    # Store any IP addresses in the page source of $test_url_address.
    local ip_address
    ip_address=$(Fetch_IP_Address_From_Web_Source "$test_url_address")
    local fetch_ip_ret_val=$?

    # Failed due to temp file creation error.
    if [ "$fetch_ip_ret_val" -eq "$FETCH_IP_TEMP_FILE_CREATION_FAILED" ]; then
        Exit_With_Error_Message "$EXIT_ERR_TEMP_FILE_CREATION_FAILED"
    fi

    local time_stop=$(date +%s.%N)
    local time_taken=$(printf "%.3F" $(echo "$time_stop - $time_start" | bc -l))

    # Print the test results.

    printf "%-22s %s\n" "Duration (Seconds):" "$time_taken"

    if [ "$fetch_ip_ret_val" -eq "$FETCH_IP_SINGLE_VALID_IP" ]; then
        printf "%-22s %s\n" "Status:" "Success: A single valid IP address"
        printf "%-22s %s\n" "IP Address:" "$ip_address"
        printf "%-22s %s\n" "Conclusion:" "Url is suitable for using as a source."
        printf "%-22s %s\n" "" "[As long as the IP address is correct.]"
        exit "$EXIT_SUCCESS"

    elif [ "$fetch_ip_ret_val" -eq "$FETCH_IP_MULTIPLE_VALID_IP" ]; then
        printf "%-22s %s\n" "Status:" "Failure: More than one IP address"
        # Create neat text for displaying the list of IP addresses.
        local ip_list_csv=$(echo "$ip_address" | tr '\n' ',' | sed "s/,/, /g ; s/, $//g")
        printf "%-22s %s\n" "IP Addresses:" "$ip_list_csv"
        printf "%-22s %s\n" "Conclusion:" "Url is NOT suitable for using as a source."
        exit "$EXIT_ERR_TEST_URL_MULTIPLE_IP_ADDRESS"

    elif [ "$fetch_ip_ret_val" -eq "$FETCH_IP_NO_VALID_IP" ]; then
        printf "%-22s %s\n" "Status:" "Failure: No IP address"
        printf "%-22s %s\n" "Conclusion:" "Url is NOT suitable for using as a source."
        exit "$EXIT_ERR_TEST_URL_NO_IP_ADDRESS"

    elif [ "$fetch_ip_ret_val" -eq "$FETCH_IP_SERVER_TIMED_OUT" ]; then
        printf "%-22s %s\n" "Status:" "Failure: Server timed out"
        printf "%-22s %s\n" "Conclusion:" "Url is NOT suitable for using as a source. [Unless the"
        printf "%-22s %s\n" "" "timeout was set very low or its timing out was unusual.]"
        exit "$EXIT_ERR_TEST_URL_SERVER_TIMED_OUT"
    fi
}


# Function which controls checking if all the web pages in the web sources list file are working.
Handle_Check_Mode()
{
    printf "\n%-24s %s\n" "Script Version:" "$VERSION"
    printf "%-24s %s\n" "Operation Mode:" "Check Source Urls"
    printf "%-24s %s\n" "Timeout (Seconds):" "$timeout"
    printf "%-24s %s\n" "Download Program:" "$download_program_display_text"

    # Read the web sources list into $web_sources.
    Read_Source_Url_List

    printf "%-24s %s\n" "Web Sources Location:" "$web_sources_list_location_display_text"
    printf "%-24s %s\n\n" "Load Web Sources:" "Succeeded ($web_sources_len Urls)"

    # Table header.
    printf "%s\n" "------------------------------------------------------------------------------"
    printf "%-23s %-40s %s\n" "Seconds" "Url" "IP/Status"
    printf "%s\n" "------------------------------------------------------------------------------"

    # Loop through the urls in $web_sources providing information about the status of each one.

    local url

    for url in "${web_sources[@]}"; do

        local time_start=$(date +%s.%N)

        # Store any IP addresses in the page source of $url.
        local ip_address
        ip_address=$(Fetch_IP_Address_From_Web_Source "$url")
        local fetch_ip_ret_val=$?

        # Failed due to temp file creation error.
        if [ "$fetch_ip_ret_val" -eq "$FETCH_IP_TEMP_FILE_CREATION_FAILED" ]; then
            Exit_With_Error_Message "$EXIT_ERR_TEMP_FILE_CREATION_FAILED"
        fi

        local time_stop=$(date +%s.%N)
        local time_taken=$(printf "%.3F" $(echo "$time_stop - $time_start" | bc -l))

        # Add info. to the stored data for displaying at the end.
        Add_To_Stats "$fetch_ip_ret_val" "$ip_address" "$time_taken"

        # Neatly display the duration, source url, and the IP address or failure text.

        # If necessary set the failure display text.
        local web_source_failure_text

        if [ "$fetch_ip_ret_val" -eq "$FETCH_IP_NO_VALID_IP" ]; then
            web_source_failure_text="NO IP ADDRESS"

        elif [ "$fetch_ip_ret_val" -eq "$FETCH_IP_MULTIPLE_VALID_IP" ]; then
            web_source_failure_text="MULTIPLE IP"

        elif [ "$fetch_ip_ret_val" -eq "$FETCH_IP_SERVER_TIMED_OUT" ]; then
            web_source_failure_text="URL TIMED OUT"
        fi

        # Truncate $url if its length will mess up the table columns.
        local url_length=${#url}
        if [ "$url_length" -gt "52" ]; then
            local truncate_url=${url:0:48}
            url="$truncate_url..."
        fi

        if [ "$fetch_ip_ret_val" -eq "$FETCH_IP_SINGLE_VALID_IP" ]; then
            printf "%-8s %-53s %s\n" "$time_taken" "$url" "$ip_address"
        else
            printf "%-8s %-53s %s\n" "$time_taken" "$url" "$web_source_failure_text"
        fi

    done  # End of loop through $web_sources loop.

    # Display the stats info.
    Display_Stats

    # The exit point of the script in check mode.
    exit "$EXIT_SUCCESS"
}


#
# COMMAND LINE PROCESSING
#


# Handle any long options by looping through the args to check them all.

for arg in "$@"; do

    if [ "$arg" = "--help" -o "$arg" = "-help" ]; then
        Display_Help_Man_Page
        exit "$EXIT_SUCCESS"

    elif [ "$arg" = "--version" -o "$arg" = "-version" -o "$arg" = "-ver" ]; then
        printf "%s\n" "fetchip v. $VERSION"
        exit "$EXIT_SUCCESS"
    fi
done

# Process the command line arguments using getopts.

while getopts ":bcfhn:p:s:t:u:v" option; do

    case $option in

        f)
            # -f : set the operation mode.
            operation_mode="$FETCH_IP_MODE"
            ;;

        c)
            # -c : set the operation mode.
            operation_mode="$CHECK_MODE"
            ;;

        u)
            # -u url : set the operation mode and url to test.
            operation_mode="$TEST_URL_MODE"
            test_url_address_tmp="$OPTARG"

            # Check that a http url was entered.

            if [[ "$test_url_address_tmp" =~ $HTTP_URL_REGEX ]]; then
                test_url_address="$test_url_address_tmp"

            else
                Exit_With_Error_Message "$EXIT_ERR_ARG_INVALID" "u"
            fi
            ;;

        n)
            # -n num : set the number of web sources to use for verification.
            web_sources_verify_num_tmp="$OPTARG"

            # Check that a single digit in range was entered.

            # For the sake of readability use shorter variables.
            min="$WEB_SOURCES_VERIFY_NUM_MIN"
            max="$WEB_SOURCES_VERIFY_NUM_MAX"

            regex_single_digit_in_range="^[$min-$max]$"

            if [[ "$web_sources_verify_num_tmp" =~ $regex_single_digit_in_range ]]; then
                web_sources_verify_num="$web_sources_verify_num_tmp"

            else
                Exit_With_Error_Message "$EXIT_ERR_ARG_INVALID" "n"
            fi
            ;;

        t)
            # -t secs : set the user timeout.
            timeout_user_set_tmp="$OPTARG"

            # Use awk to check that an int or real number in the correct range was entered.

            # The "$1 + 0 == $1" bit is an awk hack to check for any kind of number (int or real).
            awk_is_num_in_range='{ if ($1 + 0 == $1 && $1 >= to_min && $1 <= to_max)
                                                        print "TRUE"; else print "FALSE"; }'

            awk_is_num_in_range_ret_val=$(echo "$timeout_user_set_tmp" | \
                      awk "$awk_is_num_in_range" to_min="$TIMEOUT_MIN" to_max="$TIMEOUT_MAX")

            if [ "$awk_is_num_in_range_ret_val" = "TRUE" ]; then
                timeout_user_set="$timeout_user_set_tmp"

            else
                Exit_With_Error_Message "$EXIT_ERR_ARG_INVALID" "t"
            fi
            ;;

        p)
            # -p program : set which download program to use.
            user_set_download_program_tmp="$OPTARG"

            # Check that either 'wget' or 'curl' was entered.

            regex_wget="^[Ww][Gg][Ee][Tt]$"
            regex_curl="^[Cc][Uu][Rr][Ll]$"

            if [[ "$user_set_download_program_tmp" =~ $regex_wget ]]; then
                download_program_user_set="$DOWNLOAD_PROGRAM_WGET"

            elif [[ "$user_set_download_program_tmp" =~ $regex_curl ]]; then
                download_program_user_set="$DOWNLOAD_PROGRAM_CURL"

            else
                Exit_With_Error_Message "$EXIT_ERR_ARG_INVALID" "p"
            fi
            ;;

        s)
            # -s : set the location of the web sources list file.
            web_sources_list_user_set_tmp="$OPTARG"

            # Make sure that either a local file or a http url was entered.

            if [ -f "$web_sources_list_user_set_tmp" ]; then
                web_sources_list_user_set="$web_sources_list_user_set_tmp"

            elif [[ "$web_sources_list_user_set_tmp" =~ $HTTP_URL_REGEX ]]; then
                web_sources_list_user_set="$web_sources_list_user_set_tmp"

            else
                Exit_With_Error_Message "$EXIT_ERR_WEB_SOURCE_LIST_INVALID"
            fi
            ;;

        v)
            # -v : set verbose.
            verbose="TRUE"
            ;;

        b)
            # -b : set bare output.
            bare_output="TRUE"
            ;;

        h)
            # -h : display help.
            Display_Help_Summary
            exit "$EXIT_SUCCESS"
            ;;

        \?)
            # Invalid option : e.g. -x
            Exit_With_Error_Message "$EXIT_ERR_OPTION_UNKNOWN"
            ;;

        :)
            # Additional arg missing : e.g. -t <BLANK>.
            Exit_With_Error_Message "$EXIT_ERR_ARG_MISSING" "$OPTARG"
            ;;
    esac
done

# Check that there are no additional, and therefore invalid, options. Note that getopts stops
# processing if it encounters an arg without a '-' prefix (clearly not including the OPTARGs).

# $OPTIND is the index of the next arg to be processed by getopts.
let "num_args_processed_by_getopts = OPTIND - 1"
shift "$num_args_processed_by_getopts"
num_args_remaining="$#"

if [ "$num_args_remaining" -gt "0" ]; then
    Exit_With_Error_Message "$EXIT_ERR_OPTION_UNKNOWN"
fi

# If both verbose and bare output are set then verbose trumps bare.
if [ "$verbose" = "TRUE" -a "$bare_output" = "TRUE" ]; then
    bare_output="FALSE"
fi

# Set the timeouts appropriately.
Set_Timeouts

# Set whether to use wget or curl as the download program.
Set_Download_Program

# Call the appropriate function for the operation mode.

if [ "$operation_mode" = "$FETCH_IP_MODE" ]; then
    Handle_Fetch_IP_Mode

elif [ "$operation_mode" = "$TEST_URL_MODE" ]; then
    Handle_Test_Url_Mode

elif [ "$operation_mode" = "$CHECK_MODE" ]; then
    Handle_Check_Mode
fi

#
# End of script.
#
